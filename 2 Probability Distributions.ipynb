{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One role for the distributions is to model the probability distribution $p\\left(\\mathbf{x}\\right)$ of a random variable $\\mathbf{x}$, given a finite set $\\mathbf{x}_1, \\dots, \\mathbf{x}_N$ of obserations. This problem is known as density estimation. We shall assume that the data points are independent and identically distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be emphasized that the problem of density estimation is fundamentally ill-posed, because there are infinitely many probability distributions that could have given rise to the obserbed finite data set. Indeed, any distribution $p\\left(\\mathbf{x}\\right)$ that is nonzero at each of the data points $\\mathbf{x}_1,\\dots, \\mathbf{x}_N$ is a potentioal candidate. The issue of choosing an approriate distribution relates to the problem of model selection and that is a central issue in pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by considering the binomial and multinomial distributions for discrete random variables and the Gaussian distribution for continuous random variables. These are specific examples of parametric distributions, so-called because they are governed by a small number of adaptive parameters, such as the mean and variance in the the case of a Gaussian for example. To apply such models to the problem of density estimation, we need a procedure for determining suitable values for the parameters, given an observed data set. In a frequentist treatment, we choose specific values for the parameters by optimizing some criterion, such as the likelihood function. By contrast, in a Bayesian treatment we introduce prior distributions over the parameters and then use Bayes' theorem to compute the corresponding posterior distribution given the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall see that an important role is played by conjugate priors, that lead to posterior distributions having the same functional form as the prior, and that therefore lead to greatly simplified Bayesian analysis. For example, the conjugate prior for the parameters of the multinomial distribution is called the Dirichlet distribution, while the conjugate prior for the mean of a Gaussian is another Gaussian. All of these distributions are examples of the exponential family of distributions, which possess a number of important properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of the parametric approach is that it assumes a specific functional form for the distribution, which may turn out to inappropriate for a particular application. An alternamtive approach is given by nonparametric density estimation methods in which the form of the distribution typically depends on the size of the data set. Such models still contain parameters, but these control the model complexity rather than the from of the distribution. We end this chapter by considering three nonparametric methods based respectively on histograms, nearest-neighbours, and kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Binary Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single binary random variable $x\\in\\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of $x=1$ will be denoted by the parameter $\\mu$ so that  \n",
    "$$ p\\left(x=1|\\mu\\right)=\\mu$$\n",
    "where $0\\leqslant\\mu\\leqslant1$, from which it follows that $p\\left(x=0|\\mu\\right)=1-\\mu$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That probability distribution over $x$ can therefore be written in the form\n",
    "$$Bern\\left(x|\\mu\\right)=\\mu^x\\left(1-\\mu\\right)^{1-x}$$\n",
    "which is known as the Bernoulli distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli distribution is normalized and it has mean and variance given by\n",
    "$$\\mathbb{E}\\left[x\\right]=\\mu \\\\\n",
    "var\\left[x\\right]=\\mu\\left(1-\\mu\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we have a data set $\\mathcal{D}=\\{x_1,\\dots,x_N\\}$ of observed values of $x$. We can construct the likelihood function, which is a function of $\\mu$, on the assumption that the observations are drawn independently from $p\\left(x|\\mu\\right)$, so that\n",
    "$$p\\left(\\mathcal{D}|\\mu\\right)=\\prod_{n=1}^N p\\left(x_n|\\mu\\right)=\\prod_{n=1}^N\\mu^{x_n}\\left(1-\\mu\\right)^{1-x_n}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a frequentist setting, we can estimate a value for $\\mu$ by maximizing the likelihood function, or equivalently by maximizing the logarithm of the likelihood. In the case of the Bernoulli distribution, the log likelihood function is given by \n",
    "$$\\ln p\\left(\\mathcal{D}|\\mu\\right)=\\sum_{n=1}^N \\ln p\\left(x_n|\\mu\\right)=\\sum_{n=1}^N\\{x_n\\ln\\mu+\\left(1-x_n\\right)\\ln\\left(1-\\mu\\right)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is worth noting that the log likelihood function depends on the $N$ observations $x_n$ only throught their sum $\\sum_n x_n$. This sum provides an example of a sufficient statistic for the data under this distribution. If we set the derivative of $\\ln p\\left(\\mathcal{D}|\\mu\\right)$ with respect to $\\mu$ equal to zero, we obtain the maximum likelihood estimator\n",
    "$$\\mu_{ML}=\\frac{1}{N}\\sum_{n=1}^N x_n \\quad\\quad\\quad\\left(2.7\\right)$$\n",
    "which is also known as the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote the number of observations of $x=1$ within this data set by $m$, then we can write (2.7) in the form\n",
    "$$\\mu_{ML}=\\frac{m}{N}$$\n",
    "so that the probability of landing heads is given, in this maximum likelihood framework, by the fraction of observations of $x=1$ in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also work out the distributin of the number $m$ of observations of $x=1$, given that the data set has size $N$. This is called the binomial distribution, and we see that it is proportional to $\\mu^m\\left(1-\\mu\\right)^{N-m}$. The binomial distribution can be written \n",
    "$$Bin\\left(m|N,\\mu\\right)=\\left(\\begin{array} {cccc} \n",
    "    N \\\\   \n",
    "    m\\\\    \n",
    "\\end{array}\\right)   \n",
    "\\mu^m\\left(1-\\mu\\right)^{N-m}$$\n",
    "where\n",
    "$$\\left(\\begin{array} {cccc} \n",
    "    N \\\\   \n",
    "    m\\\\    \n",
    "\\end{array}\\right)\\equiv \\frac{N!}{\\left(N-m\\right)!m!}  $$\n",
    "is the number of ways of choosing $m$ objects out of a total of $N$ identical objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and variance of the binomial distributin shows that for inependent events the mean of the sum is the sum of the means, and the variance of the sum is the sum of the variances. Because $m=x_1+x_2+\\dots+x_N$, we have \n",
    "$$\\mathbb{E}\\left[m\\right]\\equiv\\sum_{m=0}^N mBin\\left(m|N,\\mu\\right)=N\\mu \\\\\n",
    "var\\left[m\\right]\\equiv\\sum_{m=0}^N\\left(m-\\mathbb{E}\\left[m\\right]\\right)^2Bin\\left(m|N,\\mu\\right)=N\\mu\\left(1-mu\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 The beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior. This property is called conjugacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beta distributin, given by\n",
    "$$Beta\\left(\\mu|a,b\\right)=\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma\\left(a\\right)\\Gamma\\left(b\\right)}\\mu^{a-1}\\left(1-\\mu\\right)^{b-1}$$\n",
    "where $\\Gamma\\left(x\\right)$ is the gamma function defined by\n",
    "$$\\Gamma\\left(x\\right)\\equiv\\int_0^\\infty u^{x-1}e^{-u}\\mathrm{d}u.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient ensures that the beta distribution is normalized, so that\n",
    "$$\\int_0^1 Beta\\left(\\mu|a,b\\right)\\mathrm{d}\\mu=1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and variance of the beta distribution are given by\n",
    "$$\\mathbb{E}\\left[\\mu\\right]=\\frac{a}{a+b} \\\\\n",
    "var\\left[\\mu\\right]=\\frac{ab}{\\left(a+b\\right)^2\\left(a+b+1\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $a$ and $b$ are often called hyperparameters because they control the distribution of the parameter $\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution of $\\mu$ is now obtained by multiplying the beta prior by the binomial likelihood function and normailizing. Keeping only the factors that depend on $\\mu$, we see that this posterior distribution has the form\n",
    "$$p\\left(\\mu|m,l,a,b\\right)\\propto\\mu^{m+a-1}\\left(1-\\mu\\right)^{l+b-1}$$\n",
    "where $l=N-m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posterior distribution has the same functional dependence on $\\mu$ as the prior distribution, reflecting the conjugacy properties of the prior with respect to the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution is simply another beta distribution, given by\n",
    "$$p\\left(\\mu|m,l,a,b\\right)=\\frac{\\Gamma\\left(m+a+l+b\\right)}{\\Gamma\\left(m+a\\right)\\Gamma\\left(l+b\\right)}\\mu^{m+a-1}\\left(1-\\mu\\right)^{l+b-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the effect of observing a data set of $m$ observations of $x=1$ and $l$ observations of $x=0$ has been to incrase the value of $a$ by $m$, and the value of $b$ by $l$, in going from the prior distribution to the posterior distribution. This allows us to provide a simple interpretation of the hyperparameters $a$ and $b$ in the prior as an effective number of observations of $x=1$ and $x=0$, respectively. Note that $a$ and $b$ need not be integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the posterior distribution can act as the prior if we subsequently observe additioinal data. To see this, we can imagine taking observations one at a time and after each observation updating the current posterior distribution by multiplying by the likelihood function for the new observation and then normalizing to obtain the new, revised posterior distribution. At each stage, the posterior is beta distribution with some total number of (prior and actual) observed values for $x=1$ and $x=0$ giben by the parameters $a$ and $b$. Incorporation of an additional observation of $x=1$simply corresponds to incrementing the value of $a$ by $1$, whereas for an observation of $x=0$ we increment $b$ by $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this sequential approach to learning arises naturally when we adopt a Bayesian viewpoint. It is independent of the choice of prior and of the likelihood function and depends only on the assumption of i.i.d. data. Sequential methods make use of observations one at a time, or in small batches, and then discard them before the next observations are used. They can be used, for example, in real-time learning scenarios where a steady stream of data is arriving, and predictions must be made before all of the data is seen. Because they do not require the whole data set to be stored or loaded into memory, sequential methods are also useful for large data sets. Maximum likelihood methods can also be cast into a sequential framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our goal is to predict, as best we can, the outcome of the next trial, then we must evaluate the predictive distribution of $x$, given the observed data set $\\mathcal{D}$. From the sum and product rules of probability, this takes the from \n",
    "$$p\\left(x=1|\\mathcal{D}\\right)=\\int_0^1 p\\left(x=1|\\mu\\right)p\\left(\\mu|\\mathcal{D}\\right)\\mathrm{d}\\mu=\\int_0^1 \\mu p\\left(\\mu|\\mathcal{D}\\right)\\mathrm{d}\\mu=\\mathbb{E}\\left[\\mu|\\mathcal{D}\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the posterior distribution $p\\left(\\mu|\\mathcal{D}\\right)$, together with the mean of the beta distribution, we obtain \n",
    "$$p\\left(x=1|\\mathcal{D}\\right)=\\frac{m+a}{m+a+l+b}$$\n",
    "which has a simple interpretation as the total fraction of observations that correspond to $x=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the limit of an infinitely large data set $m,l\\to\\infty$ the result reduces to the maximum likelihood result. As we shall see, it is a very general property that the Bayesian and maximum likelihood results will agree in the limit of an infinitely large data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe more and more data, the uncertainty represented by the posterior distribution will steadily decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 Multinomial Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encounter discrete variables that can take on one of $K$ possible mutually exclusive states. A particularly convenient representation is the 1-of-K scheme in which the variable is represented by a k-dimensional vector $\\mathbf{x}$ in which one of the elements $x_k$ equals $1$, and all remaining elements equal $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector $\\mathbf{x}$ will be represented by \n",
    "$$\\mathbf{x}=\\left(x_1,\\dots,x_K\\right)^\\top.$$\n",
    "Note that such vectors satisfy $\\sum_{k=1}^K x_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote the probability of $x_k=1$ by the parameter $\\mu_k$, then the distribution of $\\mathbf{x}$ is given \n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol{\\mu}\\right)=\\prod_{k=1}^K \\mu_k^{x_k}$$\n",
    "where $\\boldsymbol{\\mu}=\\left(\\mu_1,\\dots,\\mu_K\\right)^\\top$, and the parameters $\\mu_k$ are constrained to satisfy $\\mu_k\\geqslant0$ and $\\sum_k \\mu_k=1$, because they represent probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is normalized \n",
    "$$\\sum_\\mathbf{x} p\\left(\\mathbf{x}|\\boldsymbol{\\mu}\\right)=\\sum_{k=1}^K \\mu_k=1$$\n",
    "and that \n",
    "$$\\mathbb{E}\\left[\\mathbf{x}|\\boldsymbol{\\mu}\\right]=\\sum_\\mathbf{x} p\\left(\\mathbf{x}|\\boldsymbol{\\mu}\\right)\\mathbf{x}=\\left(\\mu_1,\\dots,\\mu_K\\right)^\\top=\\boldsymbol{\\mu}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a data set $\\mathcal{D}$ of $N$ independent observations $\\mathbf{x}_1,\\dots,\\mathbf{x}_N$. The corresponding likelihood function takes the form\n",
    "$$p\\left(\\mathcal{D}|\\boldsymbol{\\mu}\\right)=\\prod_{n=1}^N \\prod_{k=1}^K \\mu_k^{x_{nk}}=\\prod_{k=1}^K \\mu_k^{\\left(\\sum_n x_{nk}\\right)}=\\prod_{k=1}^K \\mu_k^{m_k}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the likelihood function depends on the $N$ data points only through the $K$ quantities\n",
    "$$m_k=\\sum_n x_{nk}$$\n",
    "which represent the number of observations of $x_k=1$. These are called the sufficient statistics for this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the maximum likelihood solution for $\\boldsymbol{\\mu}$, we need to maximize $\\ln p\\left(\\mathcal{D}|\\boldsymbol{\\mu}\\right)$ with respect to $\\mu_k$ taking account of the constraint that the $\\mu_k$ must sum to one. This can be achieved using a Lagrange multiplier $\\lambda$ and maximizing \n",
    "$$\\sum_{k=1}^K m_k\\ln\\mu_k+\\lambda\\left(\\sum_{k=1}^K\\mu_k-1\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $\\mu_k$ to zero, we obtain\n",
    "$$\\mu_k=-m_k/\\lambda. \\quad\\quad\\quad\\left(2.32\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve for the Lagrange multiplier $\\lambda$ by substituting (2.32) into the constraint $\\sum_k \\mu_k=1$ to give $\\lambda=-N$. Thus we obtain the maximum likelihood solution in the from\n",
    "$$\\mu_{k ML}=\\frac{m_k}{N}$$\n",
    "which is the fraction of the $N$ observations for which $x_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider the joint distribution of the quantities $m_1,\\dots,m_K$, conditioned on the parameters $\\boldsymbol{\\mu}$ and on the total number $N$ of observations.The from \n",
    "$$Mult\\left(m_1,m_2,\\dots,m_K|\\boldsymbol{\\mu},N\\right)=\n",
    "\\left(\\begin{array} {cccc} \n",
    "    N \\\\   \n",
    "    m_1 m_2 \\dots m_k\\\\    \n",
    "\\end{array}\\right)\\prod_{k=1}^K \\mu_k^{m_k}$$\n",
    "which is known as the multinomial distribtution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization coefficient is the numnber of ways of partitioning $N$ object into $K$ groups of size $m_1,\\dots,m_K$ and is given by\n",
    "$$\\left(\\begin{array} {cccc} \n",
    "    N \\\\   \n",
    "    m_1m_2\\dots m_K\\\\    \n",
    "\\end{array}\\right)=\\frac{N!}{m_1!m_2!\\dots m_K!}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the variables $m_k$ are subject to the constraint\n",
    "$$\\sum_{k=1}^K m_k = N.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 The Dirichlet distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspection of the form the multinomial distribution, we see that the conjugate prior is given by\n",
    "$$p\\left(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}\\right)\\propto\\prod_{k=1}^K \\mu_k^{\\alpha_k-1}$$\n",
    "where $0\\leqslant\\mu_k\\leqslant 1$ and $\\sum_k \\mu_k=1$. Here $\\alpha_1,\\dots,\\alpha_K$ are the parameters of the distribution, and $\\boldsymbol{\\alpha}$ denotes $\\left(\\alpha_1,\\dots,\\alpha_K\\right)^\\top$. Note that, because of the summation constraint, the distribution over the space of the $\\{\\mu_k\\}$ is confined to simplex of dimensionality $K-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized form for this distribution is by\n",
    "$$Dir\\left(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}\\right)=\\frac{\\Gamma\\left(\\alpha_0\\right)}{\\Gamma\\left(\\alpha_1\\right)\\dots\\Gamma\\left(\\alpha_K\\right)}\\prod_{k=1}^K \\mu_k^{\\alpha_k-1}$$\n",
    "which is called the Dirichlet distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\Gamma\\left(x\\right)$ is the gamma function while \n",
    "$$\\alpha_0=\\sum_{k=1}^K a_k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying the prior by the likelihood function, we obtain the posterior distribution for the parameters $\\{\\mu_k\\}$ in the form\n",
    "$$p\\left(\\boldsymbol{\\mu}|\\mathcal{D},\\boldsymbol{\\alpha}\\right)\\propto p\\left(\\mathcal{D}|\\boldsymbol{\\mu}\\right)p\\left(\\boldsymbol{\\mu}|\\boldsymbol{\\alpha}\\right)\\propto\\prod_{k=1}^K \\mu_k^{\\alpha_k+m_k-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the posterior distribution again takes the form of a Dirichleet distribution, confirming that the Dirichlet is indeed a conjugate prior for the multinomial.\n",
    "$$p\\left(\\boldsymbol{\\mu}|\\mathcal{D},\\boldsymbol{\\alpha}\\right)=Dir\\left(\\boldsymbol{\\mu}|\\boldsymbol{\\mu}+\\mathbf{m}\\right)=\\frac{\\Gamma\\left(\\alpha_0ï¼‹N\\right)}{\\Gamma\\left(\\alpha_1+m_1\\right)\\dots\\Gamma\\left(\\alpha_K+m_K\\right)}\\prod_{k=1}^K \\mu_k^{\\alpha_k+m_k-1}$$\n",
    "where we have denoted $\\mathbf{m}=\\left(m_1,\\dots,m_K\\right)^\\top$. As for the case of the binomial distribution with its beta prior, we can interpret the parameters $\\alpha_k$ of the Dirichlet prior as an effective number of observations of $x_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 The Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a D-dimensional vector $\\mathbf{x}$, the multivariate Gaussian distribution takes the form\n",
    "$$\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)\\right\\}$$\n",
    "where $\\boldsymbol{\\mu}$ is a D-dimensional mean vector, $\\boldsymbol{\\Sigma}$ is a $D\\times D$ covariance matrix, and $|\\boldsymbol{\\Sigma}|$ denotes the determinant of $\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single real variable, the distribution that maximizes the entropy is the Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The central limit theorem tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomees increasingly Gaussian as the number of terms in the sum increases (Walker, 1969)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by considering the geometrical form of the Gaussian distribution. The functional dependence of the Gaussian on $\\mathbf{x}$ is through the quadratic form\n",
    "$$\\Delta^2=\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)$$\n",
    "which appears in the exponent. The quantity $\\Delta$ is called the Mahalanobis distance from $\\boldsymbol{\\mu}$ to $\\mathbf{x}$ and reduces to the Euclidean distance when $\\boldsymbol{\\Sigma}$ is the identity matrix. The Gaussian distribution will be constant on surfaces in $\\mathbf{x}$-space for which this quadratic form is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the matrix $\\boldsymbol{\\Sigma}$ can be taken to be symmetric, without loss of generality, because any antisymmetric component would disappear from the exponenent. Now consider the eignvecor equation for the covariance matrix\n",
    "$$ \\boldsymbol{\\Sigma}\\mathbf{u}_i=\\lambda_i\\mathbf{u}_i$$\n",
    "where $i=1,\\dots,D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because $\\boldsymbol{\\Sigma}$ is a real, symmetric matrix its eigenvalues will be real, and its eigenmectors can be chosen to form an orthonormal set, so that\n",
    "$$\\mathbf{u}_i^\\top\\mathbf{u}_j=\\mathbf{I}_{ij}$$\n",
    "where $\\mathbf{I}_{ij}$ is the $i,j$ element of the identity matrix and satisfies\n",
    "$$\\mathbf{I}_{ij}=\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "1,\\quad if\\;i=j\\\\\n",
    "0,\\quad if\\;i\\neq j.\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix $\\boldsymbol{\\Sigma}$ can be expressed as an expansion in terms of its eigenvectors in the form\n",
    "$$\\boldsymbol{\\Sigma}=\\sum_{i=1}^D \\lambda_i\\mathbf{u}_i\\mathbf{u}_i^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse convariance matrix $\\boldsymbol{\\Sigma}^{-1}$ can be expressed as \n",
    "$$\\boldsymbol{\\Sigma}^{-1}=\\sum_{i=1}^D \\frac{1}{\\lambda_i}\\mathbf{u}_i\\mathbf{u}_i^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic form becomes \n",
    "$$\\Delta^2=\\sum_{i=1}^D\\frac{y_i^2}{\\lambda_i}$$\n",
    "where we have defined \n",
    "$$y_i=\\mathbf{u}_i^\\top\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right).\\quad\\quad\\quad \\left(2.51\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret $\\{y_i\\}$ as a new coordinate system defined by the orthonormal vectors $\\mathbf{u}_i$ that are shifted and rotated with respect to the original $x_i$ coordinates. Forming the vector $\\mathbf{y}=\\left(y_1,\\dots,y_D\\right)^\\top$, we have\n",
    "$$\\mathbf{y}=\\mathbf{U}\\left(\\mathbf{x}-\\boldsymbol{\\Sigma}\\right)$$\n",
    "where $\\mathbf{U}$ is a matrix whose rows are given by $\\mathbf{u}_i^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{U}$ is an orthogonal matrix, i.e., it satisfies $\\mathbf{U}\\mathbf{U}^\\top=\\mathbf{I}$, and hence also $\\mathbf{U}^\\top\\mathbf{U}=\\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic form, and hence the Gaussian density, will be constant on surfaces for which (2.51) is constant. If all of the eigenvalues $\\lambda_i$ are positive, then these surfaces represent ellipsoids, with their centres at $\\boldsymbol{\\mu}$ and their axes oriented along $\\mathbf{u}_i$, and with scaling factors in the directions of the axes given by $\\lambda_i^{1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Gaussian distribution to be well defined, it is necessary for all of the eigenvalues $\\lambda_i$ of the covariance matrix to be strictly positive, otherwise the distribution cannot be properly normalized. A matrix whose eigenvalues are strictly positive is said to be positive definite. We will encounter Gaussian distributions for which one or more of the eigenvalues are zero, in which case the distribution is singular and is confined to a subspace of lower dimensionality. If all of the eigenvalues are nonnegative, then the covariance matrix is said to be positive semidefinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the form of the Gaussian distribution in the new coordinate system defined by the $y_i$. In going from the $\\mathbf{x}$ to the $\\mathbf{y}$ coordinate system, we have a Jacobian matrix $\\mathbf{J}$ with elements given by \n",
    "$$J_{ij}=\\frac{\\partial x_i}{\\partial y_j}=U_{ij}$$\n",
    "where $U_{ji}$ are the elements of the matrix $\\mathbf{U}^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the orthonormality property of the matrix $\\mathbf{U}$, we see that the square of the determinant of the Jacobian matrix is \n",
    "$$|\\mathbf{J}|^2=|\\mathbf{U}^\\top|^2=|\\mathbf{U}^\\top||\\mathbf{U}|=|\\mathbf{U}^\\top\\mathbf{U}|=|\\mathbf{I}|=1$$\n",
    "and hence $|\\mathbf{J}|=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant $|\\boldsymbol{\\Sigma}|$ of the covariance matrix can be writen as the product of its eigenvalues, and hence\n",
    "$$|\\boldsymbol{\\Sigma}|^{1/2}=\\prod_{j=1}^D \\lambda_j^{1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus in the $y_j$ coordinate system, the Gaussian distribution takes the form\n",
    "$$p\\left(\\mathbf{y}\\right)=p\\left(\\mathbf{x}\\right)|\\mathbf{J}|=\\prod_{j=1}^D \\frac{1}{\\left(2\\pi\\lambda_j\\right)^{1/2}}\\exp\\left\\{-\\frac{y_j^2}{2\\lambda_j}\\right\\}$$\n",
    "which is the product of D independent univariate Gaussian distribution. The eigenvectors therefore define a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral of the distribution in the $\\mathbf{y}$ coordinate system is then\n",
    "$$\\int p\\left(\\mathbf{y}\\right)\\mathrm{d}\\mathbf{y}=\\prod_{j=1}^D \\int_{-\\infty}^\\infty \\frac{1}{\\left(2\\pi\\lambda_j\\right)^{1/2}}\\exp\\left\\{-\\frac{y_j^2}{2\\lambda_j}\\right\\}\\mathrm{d}y_j=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the moments of the Gaussian distribution and thereby provide an interpretation of the parameters $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. The expectation of $\\mathbf{x}$ under the Gaussian distribution is given by\n",
    "$$\\mathbb{E}\\left[\\mathbf{x}\\right]=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\int\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)\\right\\}\\mathbf{x}\\mathrm{d}\\mathbf{x} \\\\\n",
    "=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\int\\exp\\left\\{-\\frac{1}{2}\\mathbf{z}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}\\right\\}\\left(\\mathbf{z}+\\boldsymbol{\\mu}\\right)\\mathrm{d}\\mathbf{z}$$\n",
    "where we have changed variables using $\\mathbf{z}=\\mathbf{x}-\\boldsymbol{\\mu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now note that the exponent is an even function of the components of $\\mathbf{z}$ and , because the integrals over these are taken over the range $\\left(-\\infty,\\infty\\right)$, the term in $\\mathbf{z}$ in the factor $\\left(\\mathbf{z}+\\boldsymbol{\\mu}\\right)$ will vanish by symmetry. Thus \n",
    "$$\\mathbb{E}\\left[\\mathbf{x}\\right]=\\boldsymbol{\\mu}$$\n",
    "and so we refer to $\\boldsymbol{\\mu}$ as the mean of the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider second order moments of the Gaussian. In the univariate case, we considered the second order moment given by $\\mathbb{E}\\left[x^2\\right]$. For the multivariate Gaussian, there are $D^2$ second ordermoments given by $\\mathbb{E}\\left[x_ix_j\\right]$, which we can group together to form the matrix $\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right]$. This matrix can be written as \n",
    "$$\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right]=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\int\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)\\right\\}\\mathbf{x}\\mathbf{x}^\\top\\mathrm{d}\\mathbf{x} \\\\\n",
    "=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\int\\exp\\left\\{-\\frac{1}{2}\\mathbf{z}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}\\right\\}\\left(\\mathbf{z}+\\boldsymbol{\\mu}\\right)\\left(\\mathbf{z}+\\boldsymbol{\\mu}\\right)^\\top\\mathrm{d}\\mathbf{z}$$\n",
    "where again we have changed variables using $\\mathbf{z}=\\mathbf{x}-\\boldsymbol{\\mu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the cross-terms invlving $\\boldsymbol{\\mu}\\mathbf{z}^\\top$ and $\\boldsymbol{\\mu}\\mathbf{z}^\\top$ will again vanish by symmetry. The term $\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top$ is constant and can be taken outside the integral, which itself is unity because the Gaussian distribution is normalized. Consider the term involving $\\mathbf{z}\\mathbf{z}^\\top$. Again, we can make use of the eigenvector expansion of the covariance matrix, together with the completeness of the set of eigenvectors, to write \n",
    "$$\\mathbf{z}=\\sum_{j=1}^D y_j\\mathbf{u}_j$$\n",
    "where $y_j=\\mathbf{u}_j^\\top\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that \n",
    "$$\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\int\\exp\\left\\{-\\frac{1}{2}\\mathbf{z}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}\\right\\}\\mathbf{z}\\mathbf{z}^\\top\\mathrm{d}\\mathbf{z} \\\\\n",
    "=\\frac{1}{\\left(2\\pi\\right)^{\\frac{D}{2}}}\\frac{1}{|\\boldsymbol{\\Sigma}|^{\\frac{1}{2}}}\\sum_{i=1}^D\\sum_{j=1}^D \\mathbf{u}_i\\mathbf{u}_j^\\top\\int\\exp\\left\\{-\\frac{1}{2}\\mathbf{z}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{z}\\right\\}y_iy_j\\mathrm{d}\\mathbf{z} \\\\\n",
    "=\\sum_{i=1}^D \\mathbf{u}_i\\mathbf{u}_i^\\top\\boldsymbol{\\lambda}_i=\\boldsymbol{\\Sigma}$$\n",
    "where we have made use of the eigenvector equation, together with the fact that the integral on the right-hand side of the middle line vanishes by symmetry unless $i=j$. Thus we hava\n",
    "$$\\mathbb{E}\\left[\\mathbf{x}\\mathbf{x}^\\top\\right]=\\boldsymbol{\\mu}\\boldsymbol{\\mu}^\\top+\\boldsymbol{\\Sigma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the multivariate case it is again convenient to subtract off the mean, giving rise to the covariance of a random vector $\\mathbf{x}$ defined by \n",
    "$$cov\\left[\\mathbf{x}\\right]=\\mathbb{E}\\left[\\left(\\mathbf{x}-\\mathbb{E}\\left[\\mathbf{x}\\right]\\right)\\left(\\mathbf{x}-\\mathbb{E}\\left[\\mathbf{x}\\right]\\right)^\\top\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the specific case of a Gaussian distribution, we can make use of $\\mathbb{E}\\left[\\mathbf{x}\\right]=\\boldsymbol{\\mu}$,to give \n",
    "$$cov\\left[\\mathbf{x}\\right]=\\boldsymbol{\\Sigma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Gaussian distrtibution is widely used as a density model, it suffers from some significant limitations. For large $D$, the total number of parameters therefore grows quadratically with $D$, and the computational task of manipulating and inverting large matrices can become prohibitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further limitation of the Gaussian distribution is that it is intrinsically unimodal (i.e., has a single maximum) and so is unable to provide a good approximation to multimodal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see later that the introduction fo latent variables, also called hidden variables or unobserved variables, allows both of these problems to be addressed. In particular, a rich family of multimodal distributions is obtained by introducing discrete latent variables leading to mixtures of Gaussians. Similarly, the introduction of continuous latent variables leads to models in which the number of free parameters can be controlled independently of the dimensionality $D$ of the data space while still allowing the model to capture the dominant correlations in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.1 Conditional Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important property of the multivariate Gaussian distribution is that if two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the other is again Gaussian. Similarly, the marginal distribution of either set is also Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consider first the case of conditional distributions. Suppose $\\mathbf{x}$ is a D-dimensional vector with Gaussian distribution $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)$ and that we partition $\\mathbf{x}$ into two disjoint subsets $\\mathbf{x}_a$ and $\\mathbf{x}_b$. Without loss of generality, we can take $\\mathbf{x}_a$ to form the first $M$ components of $\\mathbf{x}$, with $\\mathbf{x}_b$ comprising the remaining $D-M$ components, so that\n",
    "$$\\mathbf{x}=\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x}_a \\\\   \n",
    "     \\mathbf{x}_b \\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define corresponding partitions of the mean vector $\\boldsymbol{\\mu}$ given by\n",
    "$$\\boldsymbol{\\mu}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\mu}_a \\\\   \n",
    "    \\boldsymbol{\\mu}_b \\\\    \n",
    "\\end{array}\\right)$$\n",
    "and of the covariance matrix $\\boldsymbol{\\Sigma}$ given by\n",
    "$$\\boldsymbol{\\Sigma}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Sigma}_{aa}, \\boldsymbol{\\Sigma}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Sigma}_{ba},  \\boldsymbol{\\Sigma}_{bb}\\\\    \n",
    "\\end{array}\\right).$$\n",
    "Note that the symmetry $\\boldsymbol{\\Sigma}^\\top =\\boldsymbol{\\Sigma}$ of the covariance matrix implies that $\\boldsymbol{\\Sigma}_{aa}$ and $\\boldsymbol{\\Sigma}_{bb}$ are symmetric, while $\\boldsymbol{\\Sigma}_{ba} = \\boldsymbol{\\Sigma}_{ab}^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many situations, it will be convenient to work with the inverse of the covariance matrix \n",
    "$$\\boldsymbol{\\Lambda}\\equiv\\boldsymbol{\\Sigma}^{-1}$$\n",
    "which is known as the precision matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore also introuduce the partitioned form of the precision matrix\n",
    "$$\\boldsymbol{\\Lambda}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}_{aa}, \\boldsymbol{\\Lambda}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Lambda}_{ba},  \\boldsymbol{\\Lambda}_{bb}\\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin by finding an expression for the conditional distribution $p\\left(\\mathbf{x}_a|\\mathbf{b}\\right)$. We obtain\n",
    "$$-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)= \\\\\n",
    "-\\frac{1}{2}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_a\\right)^\\top\\boldsymbol{\\Lambda}_{aa}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_a\\right)-\\frac{1}{2}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_a\\right)^\\top\\boldsymbol{\\Lambda}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right) \\\\\n",
    "-\\frac{1}{2}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right)^\\top\\boldsymbol{\\Lambda}_{ba}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_a\\right)-\\frac{1}{2}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right)^\\top\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that as a function of $\\mathbf{x}_a$, this is again a quadratic form, and hence the correspoinding conditional distribution $p\\left(\\mathbf{a}|\\mathbf{b}\\right)$ will be Gaussian. Because this distribution is completely characterized by its mean and its covariance, our goal will be to identify expressions for the mean and covariance of $p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of a rather common operation associated with Gaussian distribution, sometimes called 'completing the square', in which we are given a quadratic form defining the expoinent terms in a Gaussian distribution, and we need to determine the crooespoinding mean and covariance. Such problems can be solved straightforwardly by noting that the exponent in a general Gaussian distribution $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}\\right)$ can be written \n",
    "$$ -\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)=-\\frac{1}{2}\\mathbf{x}^\\top\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\mathbf{x}^\\top\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}+const$$\n",
    "where 'const' denotes terms which are independent of $\\mathbf{x}$, and we have made use of the symmetry of $\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus if we take our general quadratic form and express it in the from gien by the right-hand, then we can immediately equate the matrix of coefficients entering the second order term in $\\mathbf{x}$ to the inverse covariance $\\boldsymbol{\\Sigma}^{-1}$ and the coefficient of the linear term in $\\mathbf{x}$ to $\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}$, from which we can obtain $\\boldsymbol{\\mu}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will denote the mean and covariance of the conditional Gaussian distribution $p\\left(\\mathbf{x}_a|\\mathbf{b}_b\\right)$ by $\\boldsymbol{\\mu}_{a|b}$ and $\\boldsymbol{\\Sigma}_{a|b}$, respectively. If we pick out all terms that are second order in $\\mathbf{x}_a$, we have \n",
    "$$-\\frac{1}{2}\\mathbf{x}_a^\\top\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a$$\n",
    "from which we can immediately conclude that the covariance (inverse precision) of $p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)$ is given by \n",
    "$$\\boldsymbol{\\Sigma}_{a|b}=\\boldsymbol{\\Lambda}_{a|a}^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now consider all of the terms that are linear in $\\mathbf{x}_a$\n",
    "$$\\mathbf{x}_a^\\top\\left\\{\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right)\\right\\}$$\n",
    "where we have used $\\boldsymbol{\\Lambda}_{ba}^\\top=\\boldsymbol{\\Lambda}_{ab}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient of $\\mathbf{x}_a$ in this expression must equal $\\boldsymbol{\\Sigma}_{a|b}^{-1}\\boldsymbol{\\mu}_{a|b}$ and hence \n",
    "$$\\boldsymbol{\\mu}_{a|b}=\\boldsymbol{\\Sigma}_{a|b}\\left\\{\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right)\\right\\} \\\\\n",
    "=\\boldsymbol{\\mu}_{a}-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express these results in terms of the corresponding partitioned covariance matrix. To do this, we make use of the following identity for the inverse of a partitioned matrix\n",
    "$$\\left(\\begin{array} {cccc} \n",
    "    A & B \\\\   \n",
    "    C & D\\\\    \n",
    "\\end{array}\\right)^{-1}=\\left(\\begin{array} {cccc} \n",
    "    M & -MBD^{-1} \\\\   \n",
    "    D^{-1}CM & D^{-1}+D^{-1}CMBD^{-1}\\\\    \n",
    "\\end{array}\\right)$$\n",
    "where we have defined \n",
    "$$M=\\left(A-BD^{-1}C\\right)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity $M^{-1}$ is known as the Schur complement of the matrix on the left-hand side with respect to the submatrix D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the definition\n",
    "$$\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Sigma}_{aa}, \\boldsymbol{\\Sigma}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Sigma}_{ba},  \\boldsymbol{\\Sigma}_{bb}\\\\    \n",
    "\\end{array}\\right)^{-1}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}_{aa}, \\boldsymbol{\\Lambda}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Lambda}_{ba},  \\boldsymbol{\\Lambda}_{bb}\\\\    \n",
    "\\end{array}\\right)$$\n",
    "we have \n",
    "$$\\boldsymbol{\\Lambda}_{aa}= \\left(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\right)^{-1} \\\\\n",
    "\\boldsymbol{\\Lambda}_{ab}=-\\left(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\right)^{-1}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form these we obtain the following expressions for the mean and covariance of the conditional distribution $p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)$\n",
    "$$\\boldsymbol{\\mu}_{a|b}=\\boldsymbol{\\mu}_{a}+\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_{b}\\right) \\\\\n",
    "\\boldsymbol{\\Sigma}_{a|b}=\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the mean of the conditional distribution $p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)$ is a linear function of $\\mathbf{x}_b$ and that the covariance is independent of $\\mathbf{x}_a$. This represents an example of a linear-Gaussian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Marginal Gaussian distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal distribution given by \n",
    "$$p\\left(\\mathbf{x}_a\\right)=\\int p\\left(\\mathbf{x}_a,\\mathbf{x}_b\\right)\\mathrm{d}\\mathbf{x}_b$$\n",
    "which is also Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic from for the joint distribution canbe expressed, using the partitioned precision matrix. Because our goal is to integrate out $\\mathbf{x}_b$, this is most easily achieved by first considering the terms involving $\\mathbf{x}_b$ and then completing the square in order to facilitate integration. Picking out just those terms that involve $\\mathbf{x}_b$, we have \n",
    "$$-\\frac{1}{2}\\mathbf{x}_b^\\top\\boldsymbol{\\Lambda}_{bb}\\mathbf{x}_b^\\top\\mathbf{m}=-\\frac{1}{2}\\left(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}\\right)^\\top\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}\\right)+\\frac{1}{2}\\mathbf{m}^\\top\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}$$\n",
    "where we have defined \n",
    "$$\\mathbf{m}=\\boldsymbol{\\Lambda}_{bb}\\boldsymbol{\\mu}_{b}-\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_{a}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dependence on $\\mathbf{x}_b$ has been cast into the standard quadratic form of a Gaussian distribution corresponding to the first term on the right-hand side, plus a term that does not depend on $\\mathbf{x}_b$ (but that does depend on $\\mathbf{x}_a$). Thus, when we take the exponential of this quadratic form, we see that the integration over $\\mathbf{x}_b$ will take the form\n",
    "$$\\int \\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}\\right)^\\top\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}\\right)\\right\\}\\mathrm{d}\\mathbf{x}_b.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This integration is easily performed by noting that it is the integral ove an unnormalized Gaussian, and so the result will be the reciprocal of the normalization coeffiient. We know from the form of the normalized Gaussian, that this coefficient is independent of the mean and depends only on the determinant of the covariance matrix. Thus, by completing the square with respect to $\\mathbf{x}_b$, we can integrate out $\\mathbf{x}_b$ and the only term remaining from the contributions on the left-hand side that depends on $\\mathbf{x}_a$ si the last term on the right-hand side. Combining this term with the remaining terms that depend on $\\mathbf{x}_a$, we obtain \n",
    "$$\\frac{1}{2}\\left[\\boldsymbol{\\Lambda}_{bb}\\boldsymbol{\\mu}_{b}-\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_{a}\\right)\\right]^\\top\\boldsymbol{\\Lambda}_{bb}^{-1}\\left[\\boldsymbol{\\Lambda}_{bb}\\boldsymbol{\\mu}_{b}-\\boldsymbol{\\Lambda}_{bb}\\left(\\mathbf{x}_a-\\boldsymbol{\\mu}_{a}\\right)\\right]-\\frac{1}{2}\\mathbf{x}_a^\\top\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\mathbf{x}_a^\\top\\left(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_{a}+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_{b}\\right)+const \\\\\n",
    "=-\\frac{1}{2}\\mathbf{x}_a^\\top\\left(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\right)\\mathbf{x}_a+\\mathbf{x}^\\top\\left(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba}\\right)^{-1}\\boldsymbol{\\mu}_{a}+const$$\n",
    "where 'const' denotes quantities independent of $\\mathbf{x}_a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the covariance of the marginal distribution of $p\\left(\\mathbf{x}_a\\right)$ is given by \n",
    "$$\\boldsymbol{\\Sigma}_{a}=\\left(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba}\\right)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean is given by\n",
    "$$\\boldsymbol{\\Sigma}_{a}\\left(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba}\\right)\\boldsymbol{\\mu}_{a}=\\boldsymbol{\\mu}_{a}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite this in terms of the correspoinding partitioning of the covariance matrix, as we did for the conditional distribution. These partitioned matrices are related by\n",
    "$$\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}_{aa}, \\boldsymbol{\\Lambda}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Lambda}_{ba},  \\boldsymbol{\\Lambda}_{bb}\\\\    \n",
    "\\end{array}\\right)^{-1}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Sigma}_{aa}, \\boldsymbol{\\Sigma}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Sigma}_{ba},  \\boldsymbol{\\Sigma}_{bb}\\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have \n",
    "$$\\left(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba}\\right)^{-1}=\\boldsymbol{\\Sigma}_{aa}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we obtain the intuitively satisfying result that the marginal distribution $p\\left(\\mathbf{x}_a\\right)$ has mean and covariance given by\n",
    "$$\\mathbb{E}\\left[\\mathbf{x}_a\\right]=\\boldsymbol{\\mu}_{a} \\\\\n",
    "cov\\left[\\mathbf{x}_a\\right]=\\boldsymbol{\\Sigma}_{aa}.$$\n",
    "In contrast to the conditional distribution for which the partitioned precision matrix gives rise to simpler expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results for the marginal and conditional distributions of a partitioned Gaussian are summarized below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a joint Gaussian distribution $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)$ with $\\boldsymbol{\\Lambda}\\equiv\\boldsymbol{\\Sigma}^{-1}$ and \n",
    "$$\\mathbf{x}=\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x}_a \\\\   \n",
    "     \\mathbf{x}_b \\\\    \n",
    "\\end{array}\\right),\\quad\n",
    "\\boldsymbol{\\mu}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\mu}_a \\\\   \n",
    "    \\boldsymbol{\\mu}_b \\\\    \n",
    "\\end{array}\\right)  \\\\\n",
    "\\boldsymbol{\\Sigma}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Sigma}_{aa}, \\boldsymbol{\\Sigma}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Sigma}_{ba},  \\boldsymbol{\\Sigma}_{bb}\\\\    \n",
    "\\end{array}\\right),\\quad\n",
    "\\boldsymbol{\\Lambda}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}_{aa}, \\boldsymbol{\\Lambda}_{ab} \\\\   \n",
    "    \\boldsymbol{\\Lambda}_{ba},  \\boldsymbol{\\Lambda}_{bb}\\\\    \n",
    "\\end{array}\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional distribution:\n",
    "$$p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)=\\mathcal{N}\\left(\\mathbf{x}_a|\\boldsymbol{\\mu}_{a|b},\\boldsymbol{\\Lambda}_{aa}^{-1}\\right) \\\\\n",
    "\\boldsymbol{\\mu}_{a|b}=\\boldsymbol{\\mu}_{a}-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_{b}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal distribution:\n",
    "$$p\\left(\\mathbf{x}_a\\right)=\\mathcal{N}\\left(\\mathbf{x}_a|\\boldsymbol{\\mu}_{a},\\boldsymbol{\\Sigma}_{aa}\\right) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.3 Bayes' theorem for Gaussian variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noted that the mean of the conditional distribution $p\\left(\\mathbf{x}_a|\\mathbf{x}_b\\right)$ was a linear function of $\\mathbf{x}_b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we shall suppose that we are given a Gaussian marginal distribution $p\\left(\\mathbf{x}\\right)$ and a Gaussian conditional distribution $p\\left(\\mathbf{y}|\\mathbf{x}\\right)$ in which $p\\left(\\mathbf{y}|\\mathbf{x}\\right)$ has a mean that is a linear function of $\\mathbf{x}$, and a covariance which is independent of $\\mathbf{x}$. This is an example of a linear Gaussian model. We wish to find the marginal distribution $p\\left(\\mathbf{y}\\right)$ and the conditional distribution $p\\left(\\mathbf{x}|\\mathbf{y}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall take the marginal and conditional disttributions to be\n",
    "$$p\\left(\\mathbf{x}\\right)=\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}\\right) \\\\\n",
    "p\\left(\\mathbf{y}|\\mathbf{x}\\right)=\\mathcal{N}\\left(\\mathbf{y}|\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}\\right)$$\n",
    "where $\\boldsymbol{\\mu},\\mathbf{A}$, and $\\mathbf{b}$ are parameters governing the means, and $\\boldsymbol{\\Lambda}$ and $\\mathbf{L}$ are precision matrices. If $\\mathbf{x}$ has dimensionality $M$ and $\\mathbf{y}$ has dimensiionality $D$, then the matrix $\\mathbf{A}$ has size $D\\times M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we find an expression for the joint distribution over $\\mathbf{x}$ and $\\mathbf{y}$. To do this, we define\n",
    "$$\\mathbf{z}=\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x} \\\\   \n",
    "     \\mathbf{y} \\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then consider the log of the joint distribution\n",
    "$$\\ln p\\left(\\mathbf{x}\\right)=\\ln p\\left(\\mathbf{x}\\right)+\\ln p\\left(\\mathbf{y}|\\mathbf{x}\\right) \\\\\n",
    "=-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Lambda}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right) \\\\ \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad-\\frac{1}{2}\\left(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right)^\\top\\mathbf{L}\\left(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b}\\right)+const$$\n",
    "where 'const' denotes terms independent of $\\mathbf{x}$ and $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we see that this is a quadratic fuction of the components of $\\mathbf{z}$, and hence $p\\left(\\mathbf{z}\\right)$ is Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the precision of this Gaussian, we consider the second order terms, which can be written as \n",
    "$$-\\frac{1}{2}\\mathbf{x}^\\top\\left(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{A}\\right)\\mathbf{x}-\\frac{1}{2}\\mathbf{y}^\\top\\mathbf{L}\\mathbf{y}+\\frac{1}{2}\\mathbf{y}^\\top\\mathbf{L}\\mathbf{A}\\mathbf{x}+\\frac{1}{2}\\mathbf{x}^\\top\\mathbf{A}^\\top\\mathbf{L}\\mathbf{y} \\\\\n",
    "=-\\frac{1}{2}\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x} \\\\   \n",
    "     \\mathbf{y} \\\\    \n",
    "\\end{array}\\right)^\\top\n",
    "\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{y} & -\\mathbf{A}^\\top\\mathbf{L}\\\\   \n",
    "     -\\mathbf{L}\\mathbf{A} & \\mathbf{L}  \n",
    "\\end{array}\\right)\n",
    "\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x} \\\\   \n",
    "     \\mathbf{y} \\\\    \n",
    "\\end{array}\\right)=-\\frac{1}{2}\\mathbf{z}^\\top\\mathbf{R}\\mathbf{z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Gausian distribution over $\\mathbf{z}$ has precision (inverse covariance) matrix given by \n",
    "$$\\mathbf{R}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{y} & -\\mathbf{A}^\\top\\mathbf{L}\\\\   \n",
    "     -\\mathbf{L}\\mathbf{A} & \\mathbf{L}  \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix is found by taking the inverse of the precision,\n",
    "$$cov\\left[\\mathbf{z}\\right]=\\mathbf{R}^{-1}=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}^{-1} & \\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\top\\\\   \n",
    "     \\mathbf{A}\\boldsymbol{\\Lambda}^{-1} & \\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\top  \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can find the mean of the Gaussian distribution over $\\mathbf{z}$ by identifying the linear terms, which are given by\n",
    "$$\\mathbf{x}^\\top\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{x}^\\top\\mathbf{A}^\\top\\mathbf{L}\\mathbf{b}+\\mathbf{y}^\\top\\mathbf{L}\\mathbf{b}=\\left(\\begin{array} {cccc} \n",
    "    \\mathbf{x} \\\\   \n",
    "     \\mathbf{y} \\\\    \n",
    "\\end{array}\\right)^\\top\n",
    "\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\top\\mathbf{L}\\mathbf{b} \\\\   \n",
    "     \\mathbf{L}\\mathbf{b} \\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the mean of $\\mathbf{z}$ is given by\n",
    "$$\\mathbb{E}\\left[\\mathbf{z}\\right]=\\mathbf{R}^{-1}\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\top\\mathbf{L}\\mathbf{b} \\\\   \n",
    "     \\mathbf{L}\\mathbf{b} \\\\    \n",
    "\\end{array}\\right)=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol{\\mu} \\\\   \n",
    "     \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b} \\\\    \n",
    "\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we find an expression for the marginal distrbution $p\\left(\\mathbf{y}\\right)$ in which we have marginalized over $\\mathbf{x}$. Recall that the marginal distribution over a subset of the components of a Gaussian random vector takes a particularly simple form when expressed in terms of the partitioned covariance matrix.The mean and covariance of the marginal distribution $p\\left(\\mathbf{y}\\right)$ are giben by\n",
    "$$\\mathbb{E}\\left[\\mathbf{y}\\right]=\\mathbb{E}\\left[\\mathbf{x}_b\\right]=\\boldsymbol{\\mu}_b=\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b} \\\\\n",
    "cov\\left[\\mathbf{y}\\right]=cov\\left[\\mathbf{x}_b\\right]=\\boldsymbol{\\Sigma}_{bb}=\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A special case of this resutl is when $\\mathbf{A}=\\mathbf{I}$, in which case it reduces to the convolution of two Gaussians, for which we see that the mean of the convolution is the sum of the mean of the two Gaussians, and the covariance of the convolution is the sum of their covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we seek an expression for the conditional $p\\left(\\mathbf{x}|\\mathbf{y}\\right)$. Recall that the results for the conditional distribution are most easily expressed in terms of the partitioned precision matrix. We see that the conditional distribution $p\\left(\\mathbf{x}|\\mathbf{y}\\right)$ has mean and covariance given by\n",
    "$$cov\\left[\\mathbf{x}|\\mathbf{y}\\right]=\\boldsymbol{\\Sigma}_{a|b}=\\boldsymbol{\\Lambda}_{a|b}^{-1}=\\left(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{A}\\right)^{-1}\\\\\n",
    "\\mathbb{E}\\left[\\mathbf{x}|\\mathbf{y}\\right]=\\boldsymbol{\\Sigma}_{a|b}\\left\\{\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Sigma}_{ab}\\left(\\mathbf{x}_b-\\boldsymbol{\\mu}_b\\right)\\right\\}=\\left(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{A}\\right)^{-1}\\left\\{\\mathbf{A}^\\top\\mathbf{L}\\left(\\mathbf{y}-\\mathbf{b}\\right)+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a marginal Gaussian distribution for $\\mathbf{x}$ and a conditional Gaussian distribution for $\\mathbf{y}$ given $\\mathbf{x}$ in the form\n",
    "$$p\\left(\\mathbf{x}\\right)=\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}\\right) \\\\\n",
    "p\\left(\\mathbf{y}|\\mathbf{x}\\right)=\\mathcal{N}\\left(\\mathbf{y}|\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}\\right)$$\n",
    "the marginal distribution of $\\mathbf{y}$ and conditional distribution of $\\mathbf{x}$ given $\\mathbf{y}$ are given by\n",
    "$$p\\left(\\mathbf{y}\\right)=\\mathcal{N}\\left(\\mathbf{y}|\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b},\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^{-1}\\right) \\\\\n",
    "p\\left(\\mathbf{x}|\\mathbf{y}\\right)=\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol{\\Sigma}\\left\\{\\mathbf{A}^\\top\\mathbf{L}\\left(\\mathbf{y}-\\mathbf{b}\\right)+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}\\right\\},\\boldsymbol{\\Sigma}\\right)$$\n",
    "where\n",
    "$$\\boldsymbol{\\Sigma}=\\left(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\top\\mathbf{L}\\mathbf{A}\\right)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Maximum likelihood for the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data set $\\mathbf{X}=\\left(\\mathbf{x}_1,\\dots,\\mathbf{x}_N\\right)^\\top$ inã€€which the observations $\\{\\mathbf{x}_n$ are assumed to be drawn independently from a multivariate Gaussian distribution, we can estimate the parameters of the distribution by maximum likelihood. The log likelihood function is given by\n",
    "$$\\ln p\\left(\\mathbf{X}|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)=-\\frac{ND}{2}\\ln\\left(2\\pi\\right)-\\frac{N}{2}\\ln|\\boldsymbol{\\Sigma}|-\\frac{1}{2}\\sum_{n=1}^N\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}\\right)^\\top\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simple rearrangement, we see that the likelihood function depends on the data set only through the two quantities\n",
    "$$\\sum_{n=1}^N\\mathbf{x}_n,\\quad\\quad\\sum_{n=1}^N\\mathbf{x}_n\\mathbf{x}^\\top.$$\n",
    "These are known as the sufficient statistics for the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the log likelihood with respect to $\\boldsymbol{\\mu}$ is given by\n",
    "$$\\frac{\\partial}{\\partial\\boldsymbol{\\mu}}\\ln p\\left(\\mathbf{X}|\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}\\right)=\\sum_{n=1}^N\\boldsymbol{\\Sigma}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting this derivative to zero, we obtain the solution for the maximum likelihood estimate of the mean given by\n",
    "$$\\boldsymbol{\\mu}_{ML}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$$\n",
    "which is the mean of the observed set of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximization of log likelihood function with respect to $\\boldsymbol{\\Sigma}$ is as expected and takes the form\n",
    "$$\\boldsymbol{\\Sigma}_{ML}=\\frac{1}{N}\\sum_{n=1}^N\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_{ML}\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_{ML}\\right)^\\top$$\n",
    "which involves $\\boldsymbol{\\mu}$ because this is the result of a joint maximization with respect to $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the solution for $\\boldsymbol{\\mu}_{ML}$ does not depend on $\\boldsymbol{\\Sigma}_{ML}$, and so we can first evaluate $\\boldsymbol{\\mu}_{ML}$ and then use this to evaluate $\\boldsymbol{\\Sigma}_{ML}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evalutate the expectations of the maximum likelihood solutions under the true distribution, we obtain the following results\n",
    "$$\\mathbb{E}\\left[\\boldsymbol{\\mu}_{ML}\\right]=\\boldsymbol{\\mu} \\\\\n",
    "\\mathbb{E}\\left[\\boldsymbol{\\Sigma}_{ML}\\right]=\\frac{N-1}{N}\\boldsymbol{\\Sigma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the expectation of the maximum likelihood estimate for the mean is equal to the true mean. However, the maximum likelihood estimate for the covariance has an expectation that is less than the true value, and hence it is biased. We cna correct this bias by defining a different estimator $\\tilde{\\boldsymbol{\\Sigma}}$ given by\n",
    "$$\\tilde{\\boldsymbol{\\Sigma}}=\\frac{1}{N-1}\\sum_{n=1}^N\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_{ML}\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_{ML}\\right)^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expectation of $\\tilde{\\boldsymbol{\\Sigma}}$ is equal to $\\boldsymbol{\\Sigma}$,\n",
    "$$\\mathbb{E}\\left[\\tilde{\\boldsymbol{\\Sigma}}\\right]=\\boldsymbol{\\Sigma}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Sequential estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sequential estimation for maximum likelihood methods allow data pints to be processed one at a time and then discarded and are important for on-line applications, and also where large data sets are involved so that batch processing of all data points at once is infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the maximum likelihood estimator of the mean $\\boldsymbol{\\mu}_{ML}$, which we will denote by $ \\boldsymbol{\\mu}_{ML}^{\\left(N\\right)}$ when it is based on $N$ observations. If we dissect out the contribution from the final data point $\\mathbf{x}_N$, we obtain\n",
    "$$\\boldsymbol{\\mu}_{ML}^{\\left(N\\right)}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n \\\\\n",
    "=\\frac{1}{N}\\mathbf{x}_N+\\frac{1}{N}\\sum_{n=1}^{N-1}\\mathbf{x}_n \\\\\n",
    "=\\frac{1}{N}\\mathbf{x}_N+\\frac{N-1}{N}\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)} \\\\\n",
    "=\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}+\\frac{1}{N}\\left(\\mathbf{x}_N-\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result has a nice interpretation, as follows. After observing $N-1$ data points we have estimated $ \\boldsymbol{\\mu}$ by $\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}$. We now observe data point $\\mathbf{x}_N$, and we obtain our revised estimate $\\boldsymbol{\\mu}_{ML}^{\\left(N\\right)}$ by moving the old estimate a small amount, proportional to $1/N$, in the direction of the 'error signal' ($\\mathbf{x}_N-\\boldsymbol{\\mu}_{ML}^{\\left(N-\\right)}$). Note that, as $N$ increases, so the contribution from successive data points gets samller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seek a more general formulation of sequential learning, which leads us to the Robbins-Monro algorithm. Consider a pair of random variables $\\theta$ and $z$ governed by a joint distribution $p\\left(z,\\theta\\right)$. The conditional expectation of $z$ given $\\theta$ defines a deterministic function $f\\left(\\theta\\right)$ that is given by\n",
    "$$f\\left(\\theta\\right)\\equiv\\mathbb{E}\\left[z|\\theta\\right]=\\int z\\;p\\left(z|\\theta\\right)\\mathrm{d}z$$\n",
    "Functions defined in this way are called regression functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find the root $\\theta^*$ at which $f\\left(\\theta^*\\right)=0$. If we had a large data set of observations of $z$ and $\\theta$, then we could model the regression function directly and then obtain an estimate of its root. Suppose, however, that we observe values of $z$ one at a time and we wish to find a correspoiding sequential estimation scheme for $\\theta^*$. The following general procedure for solving such problems was gien by Robbins and Monro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall assume that the conditional variance of $z$ is finite so that \n",
    "$$\\mathbb{E}\\left[\\left(z-f\\right)^2|\\theta\\right]<\\infty$$\n",
    "and we shall also, without loss of generality, consider the case where $f\\left(\\theta\\right)>0$ for $\\theta>\\theta^*$ and\n",
    "$f\\left(\\theta\\right)<0$ for $\\theta<\\theta^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Robbins-Monro procedure then defines a sequence of successive estimates of the root $\\theta^*$ given by\n",
    "$$\\theta^{\\left(N\\right)}=\\theta^{\\left(N-1\\right)}+a_{N-1}z\\left(\\theta^{\\left(N-1\\right)}\\right)$$\n",
    "where $z\\left(\\theta^{\\left(N\\right)}\\right)$ is an observed value of $z$ when $\\theta$ takes the value $\\theta^{\\left(N\\right)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients $\\{a_N\\}$ represent a sequence of positive numbers that satisfy the conditions\n",
    "$$\\lim_{N\\to\\infty}a_N=0 \\\\\n",
    "\\sum_{N=1}^\\infty a_N=\\infty \\\\\n",
    "\\sum_{N=1}^\\infty a_N^2<\\infty.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can then be shown that the sequence of estimates does indeed converge to the root with probability one. Note that the first condition ensures that the successive corrections decrease in magnitude so that the process can converge to a limiting value. The second condition is required to ensure that the algorithm does not converge short of the root, and the third condition is needed to ensure that the accumulated noise has finite variance and hence does not spoil convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us consider how a general maximum likelihood problem can be solved sequentially using the Robbins-Monro algorithm. By definition, the maximum likelihood solumtion $\\theta_{ML}$ is a stationary point of the log likelihood function and hence satisfies\n",
    "$$\\frac{\\partial}{\\partial\\theta}\\left\\{\\frac{1}{N}\\sum_{n=1}^N-\\ln p\\left(\\mathbf{x}_n|\\theta\\right)\\right\\}\\Bigg|_{\\theta_{ML}}=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exchanging the derivative and the summation, and taking the limit $N\\to\\infty$ we have\n",
    "$$\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^N\\frac{\\partial}{\\partial\\theta}\\ln p\\left(x_n|\\theta\\right)=\\mathbb{E}_x\\left[-\\frac{\\partial}{\\partial\\theta}\\ln p\\left(x_n|\\theta\\right)\\right]$$\n",
    "and so we see that finding the maximum likelihood solution corresponds to finding the root of a regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can therefore apply the Robbins-Monro procedure, which now takes the form\n",
    "$$\\theta^{\\left(N\\right)}=\\theta^{\\left(N-1\\right)}+a_{N-1}\\frac{\\partial}{\\partial\\theta^{\\left(N-1\\right)}}\\ln p\\left(x_N|\\theta^{\\left(N-1\\right)}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a specific example, we consider once again the sequential estimation of the mean of a Gaussian distribution, in which case the parameter $\\theta^{\\left(N\\right)}$ is the estimate $\\mu_{ML}^{\\left(N\\right)}$ of the mean of the Gaussian, and the random variable $z$ is given by\n",
    "$$z=-\\frac{\\partial}{\\partial\\mu_{ML}}\\ln p\\left(x_n|\\mu_{ML},\\sigma^2\\right)=-\\frac{1}{\\sigma^2}\\left(x-\\mu_{ML}\\right).$$\n",
    "Thus the distribution of $z$ is Gaussian with mean $\\mu-\\mu_{ML}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain\n",
    "$$\\boldsymbol{\\mu}_{ML}^{\\left(N\\right)}=\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}-a_{\\left(N-1\\right)}\\left[-\\frac{\\partial}{\\partial\\mu_{ML}^{\\left(N-1\\right)}}\\ln p\\left(x_n|\\mu_{ML}^{\\left(N-1\\right)},\\sigma^2\\right)\\right] \\\\\n",
    "=\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}+a_{N-1}\\frac{1}{\\sigma^2}\\left(x-\\mu_{ML}^{\\left(N-1\\right)}\\right) \\\\\n",
    "=\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}+\\frac{1}{N}\\left(\\mathbf{x}_N-\\boldsymbol{\\mu}_{ML}^{\\left(N-1\\right)}\\right)$$\n",
    "where $a_{N}=\\frac{\\sigma^2}{N+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Bayesian inference for the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The maximum likelihood framework gave point estimates for the parameters $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we develop a Bayesian treatment by introducing prior distributions over these parameters. Let us begin with a simple example in which we consider a single Gaussian random variable $x$. We shall suppose that the variance $\\sigma^2$ is known, and we consider the task of inferring the mean $\\mu$ given a set of $N$ observations $\\mathbf{X}=\\{x_1,\\dots,x_N\\}$. The likelihood function, that is the probability of the observed data given $\\mu$, viewed as a functioin of $\\mu$, is given by\n",
    "$$p\\left(\\mathbf{X}|\\mu\\right)=\\prod_{n=1}^Np\\left(x_n|\\mu\\right)=\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{N/2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\sum_{n=1}^N\\left(x_n-\\mu\\right)^2\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the likelihood function takes the form of the exponential of a quadratic form in $\\mu$. Thus if we choose a prior $p\\left(\\mu\\right)$ given by a Gaussian, it will be a conjugate distribution for this likelihood function because the correspoinding posterior will be a product of two exponentials of quadratic functions of $\\mu$ and hence will also be Gaussian. We therefore take our prior distribution to be \n",
    "$$p\\left(\\mu\\right)=\\mathcal{N}\\left(\\mu|\\mu_0,\\sigma_0^2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution is given by\n",
    "$$p\\left(\\mu|\\mathbf{X}\\right)\\propto p\\left(\\mathbf{X}|\\mu\\right)p\\left(\\mu\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple manipulation involving completing the square in the exponent shows that the posterior distribution is given by\n",
    "$$p\\left(\\mu|\\mathbf{X}\\right)=\\mathcal{N}\\left(\\mu|\\mu_N,\\sigma_N^2\\right)$$\n",
    "where\n",
    "$$\\mu_N=\\frac{\\sigma^2}{N\\sigma_0^2+\\sigma^2}\\mu_0+\\frac{N\\sigma_0^2}{N\\sigma_0^2+\\sigma^2}\\mu_{ML} \\\\\n",
    "\\frac{1}{\\sigma_N^2}=\\frac{1}{\\sigma_0^2}+\\frac{N}{\\sigma^2}$$\n",
    "in which $\\mu_{ML}$ is the maximum likilihood solution for $\\mu$ given by the sample mean\n",
    "$$\\mu_{ML}=\\frac{1}{N}\\sum_{n=1}^Nx_n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we note that the mean of the posterior distribution is a compromise between the prior mean $\\mu_0$ and the maximum likelihood solution $\\mu_{ML}$. If the number of observed data points $N=0$, then the posteriro mean reduces to the prior mean as expected. For $N\\to\\infty$, the posterior mean is given by the maximum likelihood solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, consider the variance of the posterior distribution. We see that this is most naturally expressed in terms of the inverse variance, which is called the precision. Furthermore, the precisions are additive, so that the precision of the posterior is giben by the precision of the prior plus one contribution of the data precision from each of the observed data points. As we increase the number of observed data points, the precision steadily increases, correspoinding to a posterior distribution with steadily decreasing variance. With no observed data points, we have the prior variance, whereas if the number of data points $N\\to\\infty$, the variance $\\sigma_N^2$ oes to zero and posterior distribution becomes infinitely peaked around the maximum likelihood solution. We therefore see that the maximum likelihood result of a point estimate for $\\mu$ is recovered precisely from the Bayesian formalism in the limit of an infinite number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that for finite $N$, if we take the limit $\\sigma_0^2\\to\\infty$ in which the prior has infinite variance then the posterior mean reduces to the maximum likelihood result, while the posterior variance is given by $\\sigma_N^2=\\sigma^2/N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of this result to the case of a D-dimensional Gaussian random variable $\\mathbf{x}$ with known covariance and unknown mean is straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen how the maximum likelihood expression for the mean of a Gaussian can be re-cast as a sequential update formula in which the mean after observing $N$ data points was expressed in terms of the mean after observing $N-1$ data points together with the contribution from data point $\\mathbf{x}_N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the Bayesian paradigm leads very naturally to a sequential view of a Gaussian, we write the posterior distribution  with the contribution from the final data point $\\mathbf{x}_N$ separated out so that\n",
    "$$p\\left(\\boldsymbol\\mu|D\\right)\\propto\\left[p\\left(\\boldsymbol{\\mu}\\right)\\sum_{n=1}^{N-1}p\\left(\\mathbf{x}_n|\\mu\\right)\\right]p\\left(\\mathbf{x}_N|\\mu\\right).$$\n",
    "The term in square brackets is (up to a normalization coefficient) just the posterior distribution after observing $N-1$ data points. We see that this can be viewed as a prior distribution, which is combined using Bayes' theorem with the likelihood function asociated with data point $\\mathbf{x}_N$ to arrive at the posterior distribution after observing $N$ data points. This sequential view of Bayesian inference is very general and applies to any problem in which the observed data are assumend to be independent and identically distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let us suppose that the mean is known and we wish to infer the variance. Again, our calculations will be greatly simplified if we choose a conjugate form for the prior distribution. It turns out to be most convenient to work with the precision $\\lambda\\equiv1/\\sigma^2$. The likelihood function for $\\lambda$ takes the form \n",
    "$$p\\left(\\mathbf{X}|\\lambda\\right)=\\prod_{n=1}^N\\mathcal{N}\\propto\\lambda^{N/2}\\exp\\left\\{-\\frac{\\lambda}{2}\\sum_{n=1}^N\\left(x_n-\\mu\\right)^2\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correspoinding conjugate prior should therefore be proportional to the product of a power of $\\lambda$ and the exponential of a linear function of $\\lambda$. This coresponds to the gamma distribution which is defined by\n",
    "$$Gam\\left(\\lambda|a,b\\right)=\\frac{1}{\\Gamma\\left(a\\right)}b^a\\lambda^{a-1}\\exp\\left(-b\\lambda\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean and variance of the gamma distribution are given by\n",
    "$$\\mathbb{E}\\left[\\lambda\\right]=\\frac{a}{b} \\\\\n",
    "var\\left[\\lambda\\right]=\\frac{a}{b^2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a prior distribution $p\\left(\\lambda\\right)=Gam\\left(\\lambda|a_0,b_0\\right)$. If we multiply by the likelihood function, then we obtain a posterior distribution\n",
    "$$p\\left(\\lambda|\\mathbf{X}\\right)\\propto\\lambda^{a_0-1}\\lambda^{N/2}\\exp\\left\\{-b_0\\lambda-\\frac{\\lambda}{2}\\sum_{n=1}^N\\left(x_n-\\mu\\right)^2\\right\\}$$\n",
    "which we recognize as a gamma distribution of the form $Gam\\left(\\lambda|a_N,b_N\\right)$ where \n",
    "$$a_N=a_0+\\frac{N}{2} \\\\\n",
    "b_N=b_0+\\frac{1}{2}\\sum_{n=1}^N\\left(x_n-\\mu\\right)^2=b_0+\\frac{N}{2}\\sigma_{ML}^2$$\n",
    "where $\\sigma_{ML}^2$ is the maximum likelihood estimator of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the effect of observing $N$ data points is to increase the value of the coefficient $a$ by $N/2$. Thus we can interpret the parameter $a_0$ in the prior in terms of $2a_0$ 'effective' prior obserbations. Similarly, we see that the $N$ data points contribute $N\\sigma_{ML}^2/2$ to the parameter $b$, where $\\sigma_{ML}^2$ is the variance, and so we can interpret the parameter $b_0$ in the prior as arising  from the $2a_0$ 'effective' prior observations having variance $2b_0/\\left(2a_0\\right)=b_0/a_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These distributions are example of the exponential family, and we hall see that the interpretation of a conjugate prior in terms of effective fictitious data points is a general one for the exponential family of distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that both the mean and the precision are unknown. To find a conjugate prior, we consider the dependence of the likelihood function on $\\mu$ and $\\lambda$\n",
    "$$p\\left(\\mathbf{X}|\\mu,\\lambda\\right)=\\prod_{n=1}^N\\left(\\frac{\\lambda}{2\\pi}\\right)^{1/2}\\exp\\left\\{-\\frac{\\lambda}{2}\\left(x_n-\\mu\\right)^2\\right\\} \\\\\n",
    "\\propto\\left[\\lambda^{1/2}\\exp\\left(-\\frac{\\lambda\\mu^2}{2}\\right)\\right]^2\\exp\\left\\{\\lambda\\mu\\sum_{n=1}^Nx_n-\\frac{\\lambda}{2}\\sum_{n=1}^Nx_n^2\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to identify a prior distribution $p\\left(\\mu,\\lambda\\right)$ that has the same functional dependence on $\\mu$ and $\\lambda$ as the likelihood function and that should therefore take the form \n",
    "$$p\\left(\\mu,\\lambda\\right)\\propto\\left[\\lambda^{1/2}\\exp\\left(-\\frac{\\lambda\\mu^2}{2}\\right)\\right]^\\beta\\exp\\left\\{c\\lambda\\mu-d\\lambda\\right\\} \\\\\n",
    "=\\exp\\left\\{-\\frac{\\beta\\lambda}{2}\\left(\\mu-c/\\beta\\right)^2\\right\\}\\lambda^{\\beta/2}\\exp\\left\\{-\\left(d-\\frac{c^2}{2\\beta}\\right)\\lambda\\right\\}$$\n",
    "where $c,d$, and $\\beta$ are constants. Since we can always write $p\\left(\\mu,\\lambda\\right)=p\\left(\\mu|\\lambda\\right)p\\left(\\lambda\\right)$, we can find $p\\left(\\mu|\\lambda\\right)$ and $p\\left(\\lambda\\right)$ by inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, we see that $p\\left(\\mu|\\lambda\\right)$ is a Gaussian whose precision is a linear function fo $\\lambda$ and that $p\\left(\\lambda\\right)$ is a gamma distribution, so that the normalized prior takes the form\n",
    "$$p\\left(\\mu,\\lambda\\right)=\\mathcal{N}\\left(\\mu|\\mu_0,\\left(\\beta\\lambda\\right)^{-1}\\right)Gam\\left(\\lambda|a,b\\right)$$\n",
    "where we have defined new constants given by $\\mu_0=c/\\beta, a=1+\\beta/2, b=d-c^2/2\\beta$. This distribution is called the normal-gamma or Gaussian-gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is not simply the product of an independent Gaussian prior over $\\mu$ and a gamma prior over $\\lambda$, because the precision of $\\mu$ is a linear function of $\\lambda$. Even if we chose a prior in which $\\mu$ and $\\lambda$ were independent, the posterior distribution would exhibit a coupling between the precision of $\\mu$ and the value of $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the multivariate Gaussian distriution $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu,\\boldsymbol\\Lambda\\right)$ for a D-dimensional variable $\\mathbf{x}$, the conjugate prior distribution for the mean $\\boldsymbol\\mu$, assuming the precision is known, is again a Gaussian. For known mean and unknown precision matrix $\\boldsymbol\\Lambda$, the conjugate prior is the Wishart distribution given by\n",
    "$$\\mathcal{W}\\left(\\boldsymbol\\Lambda|\\mathbf{W},\\nu\\right)=B|\\boldsymbol\\Lambda|^{\\left(\\nu-D-1\\right)/2}\\exp\\left(-\\frac{1}{2}Tr\\left(\\mathbf{W}^{-1}\\boldsymbol\\Lambda\\right)\\right)$$\n",
    "where $\\nu$ is called the number of degrees offreedom of the distribution, $\\mathbf{W}$ is a $D\\times D$ scale matrix, and $Tr\\left(\\cdot\\right)$ denotes the trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalization constant $B$ is given by\n",
    "$$B\\left(\\mathbf{W},\\nu\\right)=|\\mathbf{W}|^{-\\nu/2}\\left(2^{\\mu D/2}\\pi^{D\\left(D-1\\right)/4}\\prod_{i=1}^D\\Gamma\\left(\\frac{\\nu+1-i}{2}\\right)\\right)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If both the mean and the precision are unknown, then, following a similar line of reasoning to the univariate case, the conjugate prior is given by\n",
    "$$p\\left(\\boldsymbol\\mu,\\boldsymbol\\Lambda|\\boldsymbol\\mu_0,\\beta,\\mathbf{W},\\mu\\right)=\\mathcal{N}\\left(\\boldsymbol\\mu|\\boldsymbol\\mu_0,\\left(\\beta,\\boldsymbol\\Lambda\\right)^{-1}\\right)\\mathcal{W}\\left(\\boldsymbol\\Lambda|\\mathbf{W},\\nu\\right)$$\n",
    "which is known as the normal-Wishart or Gaussian-Wishart distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7 Student's t-distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the conjugate prior for the precision of a Gaussian is given by a gamma distribution. If we have a univariate Gaussian $\\mathcal{N}\\left(x|\\mu,\\tau^{-1}\\right)$ together with a Gamma prior $Gam\\left(\\tau|a,b\\right)$ and we integrate out the precision, we obtain the marginal distribution of $x$ in the form\n",
    "$$p\\left(x|\\mu,a,b\\right)=\\int_0^\\infty\\mathcal{N}\\left(x|\\mu,\\tau^{-1}\\right)Gam\\left(\\tau|a,b\\right)\\mathrm{d}\\tau \\\\\n",
    "ï¼\\int_0^\\infty\\frac{b^ae^{\\left(-b\\tau\\right)}\\tau^{a-1}}{\\Gamma\\left(a\\right)}\\left(\\frac{\\tau}{2\\pi}\\right)^{1/2}\\exp\\left\\{-\\frac{\\tau}{2}\\left(x-\\mu\\right)^2\\right\\}\\mathrm{d}\\tau \\\\\n",
    "=\\frac{b^a}{\\Gamma\\left(a\\right)}\\left(\\frac{1}{2\\pi}\\right)^{1/2}\\left[b+\\frac{\\left(x+\\mu\\right)^2}{2}\\right]^{-a-1/2}\\Gamma\\left(a+1/2\\right)$$\n",
    "where we have made the change of variable $z=\\tau\\left[b+\\left(x-\\mu\\right)^2/2\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention we define new parameters given by $\\tau=2a$ and $\\lambda=a/b$, in terms of which the distribution $p\\left(x|\\mu,a,b\\right)$ takes the form\n",
    "$$St\\left(x|\\mu,\\lambda,\\tau\\right)=\\frac{\\Gamma\\left(\\tau/2+1/2\\right)}{\\Gamma\\left(\\tau/2\\right)}\\left(\\frac{\\lambda}{\\pi\\tau}\\right)^{1/2}\\left[1+\\frac{\\lambda\\left(x-\\mu\\right)^2}{\\tau}\\right]^{\\tau/2-1/2}$$\n",
    "which is known as Studnet's t-distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter $\\lambda$ is sometimes called the precision of the t-distribution, even though it is not in general equal to the inverse of the variance. The parameter $\\tau$ is called the degrees offreedom. For the particular case of $\\tau=1$, the t-distribution reduces to the Cauchy distribution, while in the limit $\\tau\\to\\infty$ the t-distribution $St\\left(x|\\mu,\\lambda,\\tau\\right)$ becomes a Gaussian $\\mathcal{N}\\left(x|\\mu,\\lambda^{-1}\\right)$ with mean $\\mu$ and precision $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Student's t-distrbuition is obtained by adding up an infinite number of Gaussian distributions having the same mean but different precisions. This can be interpreted as an infinite mixture of Gaussians. The result is a distribution that in general has longer 'tails' than a Gaussian. This gives the t-distribution an important property called robustness, which means that it is much less sensitive than the Gaussian to the presence of a few data points which are outliers. Note that the maximum likelihood solution for the t-distribution can be found using the expectation-maximization (EM) algorithm. Here we see that the effect of a small number of outliers is much less significant for the t-distribution than for the Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute the alternative parameters $\\tau=2a, \\lambda=a/b$ and $\\eta=\\tau b/a$, we see that the t-distribution can be written in form\n",
    "$$St\\left(x|\\mu,\\lambda\\tau\\right)=\\int_0^\\infty\\mathcal{N}\\left(x|\\mu,\\left(\\eta\\lambda\\right)^{-1}\\right)Gamm\\left(\\eta|\\nu/2,\\nu/2\\right)\\mathrm{d}\\eta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then generalize this to a multivariate Gaussian $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu,\\left(\\eta\\boldsymbol\\Lambda\\right)^{-1}\\right)$ to obtain the corresponding multivariate Student's t-distribution in the form\n",
    "$$St\\left(\\mathbf{x}|\\boldsymbol\\mu,\\boldsymbol\\Lambda,\\nu\\right)=\\int_0^\\infty\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu,\\left(\\eta\\boldsymbol\\Lambda\\right)^{-1}\\right)Gamm\\left(\\eta|\\nu/2,\\nu/2\\right)\\mathrm{d}\\eta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same technique as for the univariate case, we can evaluate this integral to give\n",
    "$$St\\left(\\mathbf{x}|\\boldsymbol\\mu,\\boldsymbol\\Lambda,\\nu\\right)=\\frac{\\Gamma\\left(D/2+\\nu/2\\right)}{\\Gamma\\left(\\nu/2\\right)}\\frac{|\\boldsymbol\\Lambda|^{1/2}}{\\left(\\pi\\nu\\right)^{D/2}}\\left[1+\\frac{\\Delta^2}{\\nu}\\right]^{-D/2-\\nu/2}$$\n",
    "where $D$ is the dimensionality of $\\mathbf{x}$, and $\\Delta^2$ is the squared Mahalanobis distance defined by\n",
    "$$\\Delta^2=\\left(\\mathbf{x}-\\boldsymbol\\mu\\right)^\\top\\boldsymbol\\Lambda\\left(\\mathbf{x}-\\boldsymbol\\mu\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the multivariate form of Student's t-distribution and satisfies the following properties\n",
    "$$\\mathbb{E}\\left[\\mathbf{x}\\right]=\\boldsymbol\\mu,\\quad if\\;\\nu>1 \\\\\n",
    "cov\\left[\\mathbf{x}\\right]=\\frac{\\nu}{\\left(\\nu-2\\right)}\\boldsymbol\\Lambda^{-1},\\quad if\\;\\nu>2 \\\\\n",
    "mode\\left[\\mathbf{x}\\right]=\\boldsymbol\\mu$$\n",
    "with corresponding results for the univariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3.8 Periodic variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The periodic variables can conveniently be represented using an angular (polar) coordinate $0\\leqslant\\theta<2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider the problem of evaluation the mean of a set of observations $\\mathcal{D}=\\{\\theta_1,\\dots,\\theta_N\\}$ of a periodic variable. Form now on, we shall assume that $\\theta$ is measured in radians. We have already seen that that the simple average $\\left(\\theta_1+\\dots+\\theta_N\\right)/N$ will be strongly cooredinate dependent. To find an invariant measure of the mean, we note that the observations can be viewed as points on the unit circle and can therefore be described instead by two-dimensional unit vectors $\\mathbf{x}_1,\\dots,\\mathbf{x}_N$ wher $\\|\\mathbf{x}_n\\|=1$ for $n=1,\\dots,N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can average the vectors $\\{\\mathbf{x}_n\\}$ instead to give \n",
    "$$\\bar{\\mathbf{x}}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n$$\n",
    "and then find the corresponding angle $\\bar{\\theta}$ of this average. Clearly, this definition will ensure that the location of the mean is independent of the origin of the angular coordinate. Note that $\\bar{\\mathbf{x}}$ will typilcally lie inside the unit circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cartesian coordinates of the observations are given by $\\mathbf{x}_n=\\left(\\cos\\theta,\\sin\\theta\\right)$, and we can write the Carteian coordinates of the sample mean in the form $\\bar{\\mathbf{x}}=\\left(\\bar{r}\\cos\\bar{\\theta},\\bar{r}\\sin\\bar{\\theta}\\right)$. Equating the $x_1$ and $x_2$ components then giens\n",
    "$$\\bar{x}_1=\\bar{r}\\cos\\theta=\\frac{1}{N}\\sum_{n=1}^N\\cos\\bar{\\theta_n},\\quad \\bar{x}_2=\\bar{r}\\sin\\bar{\\theta}=\\frac{1}{N}\\sum_{n=1}^N\\sin\\theta_n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the ratio, and using the identity $\\tan\\theta=\\sin\\theta/\\cos\\theta$, we can slove for $\\bar{\\theta}$ to gie \n",
    "$$\\bar{\\theta}=\\tan^{-1}\\left\\{\\frac{\\sum_n\\sin\\theta_n}{\\sum_n\\cos\\theta_n}\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortly, we shall see how this result arises naturally as the maximum likelihood estimator for an appropriately defined distribution over a periodic variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention, we will consider distributions $p\\left(\\theta\\right)$ that have period $2\\pi$. Any probability density $p\\left(\\theta\\right)$ defined over $\\theta$ must not only be nonnegative and integrage to one, but it must also be periodic. Thus $p\\left(\\theta\\right)$ must satisfy the three condistions\n",
    "$$p\\left(\\theta\\right)\\geqslant0 \\\\\n",
    "\\int_0^{2\\pi} p\\left(\\theta\\right)\\mathrm{d}\\theta=1 \\\\\n",
    "p\\left(\\theta+2\\pi\\right)=p\\left(\\theta\\right).$$\n",
    "It follows that $p\\left(\\theta+M2\\pi\\right)=p\\left(\\theta\\right)$ for any integer $M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain a Gaussian-like distribution that satisfies these three properties as follows. Consider a Gaussian distribution over two variables $\\mathbf{x}=\\left(x_1,x_2\\right)$ having mean $ \\boldsymbol\\mu=\\left(\\mu_1,\\mu_2\\right)$ and a covariance matrix $\\boldsymbol\\Sigma=\\sigma^2\\mathbf{I}$ where $\\mathbf{I}$ is the $2\\times2$ identity matrix, so that\n",
    "$$p\\left(x_1,x_2\\right)=\\frac{1}{2\\pi\\sigma^2}\\exp\\left\\{-\\frac{\\left(x_1-\\mu_1\\right)^2+\\left(x_2-\\mu\\right)^2}{2\\sigma^2}\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contours of constant $p\\left(\\mathbf{x}\\right)$ are circles. Now suppose we consider the value of this distribution along a circle of fixed radius. Then by construction this distribution will be periodic, althought it will not be normalized. We can determine the form of this distriubiton by transforming from Cartesian coordinates $\\left(x_1,x_2\\right)$ to polar coordinates $\\left(r,\\theta\\right)$ so that\n",
    "$$x_1=r\\cos\\theta,\\quad x_2=r\\sin\\theta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also map the mean $\\boldsymbol\\mu$ into polar coordinates by writing \n",
    "$$\\mu_1=r_0\\cos\\theta_0,\\quad \\mu_2=r_0\\sin\\theta_0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we substitute these transformations into the two-dimaensional Gaussian distribution, and then condition on the unit circle $r=1$, noting that we are interested only in the dependence on $\\theta$. Focussing on the exponent in the Gaussian distribution we have \n",
    "$$-\\frac{1}{2\\sigma^2}\\left\\{\\left(r\\cos\\theta-r_0\\cos\\theta_0\\right)^2+\\left(r\\sin\\theta-r_0\\sin\\theta_0\\right)^2\\right\\} \\\\\n",
    "=-\\frac{1}{2\\sigma^2}\\left\\{1+r_0^2-2r_0\\cos\\theta\\cos\\theta_0-2r_0\\sin\\theta\\sin\\theta_0\\right\\} \\\\\n",
    "=\\frac{r_0}{\\sigma^2}\\cos\\left(\\theta-\\theta_0\\right)+const$$\n",
    "where 'const' denotes terms independent of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now define $m=r_0/\\sigma^2$, we obtain our final expression for the distribution of $p\\left(\\theta\\right)$ along the unit circle $r=1$ in the form \n",
    "$$p\\left(\\theta|\\theta_0,m\\right)=\\frac{1}{2\\pi I_0\\left(m\\right)}\\exp\\left\\{m\\cos\\left(\\theta-\\theta_0\\right)\\right\\}$$\n",
    "which is called the von Mises distribution, or the cricular normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the parameter $\\theta_0$ corresponds to the mean of the distribution, while $m$, which is known as the concentration parameter, is analogous to the inverse variance (precision) for the Gaussian. The normalization coefficient is expressed in terms of $I_0\\left(m\\right)$, which is the zeroth-order Bessel funciton of the first kind and is defined by\n",
    "$$I_0\\left(m\\right)=\\frac{1}{2\\pi}\\int_0^{2\\pi}\\exp\\left\\{m\\cos\\theta\\right\\}\\mathrm{d}\\theta.$$\n",
    "For large $m$, the distribution becomes approximately Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the maximum likelihood estimators for the parameters $\\theta_0$ and $m$ for the von Mises distribution. The log likelihood function is gien by \n",
    "$$\\ln p\\left(\\mathcal{D}|\\theta_0,m\\right)=-N\\ln\\left(2\\pi\\right)-N\\ln I_0\\left(m\\right)+m\\sum_{n=1}^N\\cos\\left(\\theta_n-\\theta_0\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $\\theta_0$ equal to zero gies\n",
    "$$\\sum_{n=1}^N\\sin\\left(\\theta_n-\\theta_0\\right)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain \n",
    "$$\\theta_0^{ML}=\\tan^{-1}\\left\\{\\frac{\\sum_n\\sin\\theta_n}{\\sum_n\\cos\\theta_n}\\right\\}$$\n",
    "which we recognize obtained earlier for the mean of the observations viewed in a two-dimensional Cartesian space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, maximizing the log likelihood with respect to $m$, and making use of $I^{'}_0\\left(m\\right)=I_1\\left(m\\right)$, we have \n",
    "$$A\\left(m\\right)=\\frac{1}{N}\\sum_{n=1}^N\\cos\\left(\\theta_n-\\theta_0^{ML}\\right)$$\n",
    "where we have substituted for the maximum likelihood solution for $\\theta_0^{ML}$ (recalling that we are performing a joint optimization over $\\theta$ and $m$), and we have defined\n",
    "$$A\\left(m\\right)=\\frac{I_1\\left(m\\right)}{I_0\\left(m\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write \n",
    "$$A\\left(m_{ML}\\right)=\\left(\\frac{1}{N}\\sum_{n=1}^N\\cos\\theta_n\\right)\\cos\\theta_0^{ML}-\\left(\\frac{1}{N}\\sum_{n=1}^N\\sin\\theta_n\\right)\\sin\\theta_0^{ML}.$$\n",
    "The right-hand side is easily evaluated, and the funcation $A\\left(m\\right)$ can be inverted numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of the von Mises distribution is that it is unimodal. By forming mixtures of von Miese distribuiton, we obtain a flexible framework for modelling periodic variables that can handle multimodality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9 Mixtures of Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions. We see that a linear combination of Gaussians can give rise to very complex densitiees. By using a sufficient number of Gaussians, and by adjusting their means and comariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore consider a superoposition of $K$ Gaussian densities of the form\n",
    "$$p\\left(\\mathbf{x}\\right)=\\sum_{k=1}^K\\pi_k\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)$$\n",
    "which is called a mixture of Gausian. Each Gaussian density $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)$ is called a component of the mixture and has its own mean $\\boldsymbol\\mu_k$ and covariance $\\boldsymbol\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we shall consider Gaussian components to illustrate the framework of mixture models. More generally, mixture models can comprise linear combinations of other distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters $\\pi_k$ are called mixing coefficients. If we integrate both sids with respect to $\\mathbf{x}$, and note that both $p\\left(\\mathbf{x}\\right)$ and the individual Gaussian components are normalized, we obtain\n",
    "$$\\int_\\mathbf{x} p\\left(\\mathbf{x}\\right)=\\int_\\mathbf{x}\\sum_{k=1}^K\\pi_k\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)=\\sum_{k=1}^K\\pi_k\\int_\\mathbf{x}\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)=\\sum_{k=1}^K\\pi_k=1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the requirement that $p\\left(\\mathbf{x}\\right)\\geqslant0$, together with $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)\\geqslant0$, implies $\\pi_k\\geqslant0$ for all $k$. we obtain \n",
    "$$0\\leqslant\\pi_k\\leqslant1.$$\n",
    "We therefore see that the mixing coefficients satisfy the requirements to be probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sum and product rules, the marginal density is given by \n",
    "$$p\\left(\\mathbf{x}\\right)=\\sum_{k=1}^K p\\left(k\\right)p\\left(\\mathbf{x}|k\\right)$$\n",
    "which is equivalent to the superoposition of $K$ Gaussian densities in which we can view $\\pi_k=p\\left(k\\right)$ as the prior probability of picking the $k^{th}$ component, and the density $\\mathcal{N}\\left(\\mathbf{x}|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)=p\\left(\\mathbf{x}|k\\right)$ as the probability of $\\mathbf{x}$ conditioned on $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The form of the Gaussian mixture distribution is governed by the parameters $\\boldsymbol\\pi$,$\\boldsymbol\\mu$, and $\\boldsymbol\\Sigma$, we have used the notation $\\boldsymbol\\pi\\equiv\\{\\pi_1,\\dots,\\pi_K\\}$,$\\boldsymbol\\mu_\\equiv\\{\\boldsymbol\\mu_1,\\dots,\\boldsymbol\\mu_K\\}$,$\\boldsymbol\\Sigma\\equiv\\{\\boldsymbol\\Sigma_1,\\dots,\\boldsymbol\\Sigma_K\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to set the values of these parameters is to use maximum likelihood. The log of the likelihood funciton is given by \n",
    "$$\\ln p\\left(\\mathbf{X}|\\boldsymbol\\pi,\\boldsymbol\\mu,\\boldsymbol\\Sigma\\right)=\\sum_{n=1}^N\\ln\\left\\{\\sum_{k=1}^K\\pi_k\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol\\mu_k,\\boldsymbol\\Sigma_k\\right)\\right\\}$$\n",
    "where $\\mathbf{X}=\\{\\mathbf{x}_1,\\dots\\mathbf{x}_N\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immediatrely see that the situation is now much more complex than with a sinle Gaussian, due to the presence of the summation over $k$ inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. We can employ a powerful framework called expectation maximization, which will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 The Exponential Family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability distributions that we have studied so far in this chapter (with the exception of the Gaussian mixture) are specific example of a broad class of distributions called the expoinential family."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential family of distributions over $\\mathbf{x}$, given parameters $\\boldsymbol\\eta$, is defined to be the set of distributions of the form\n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol\\eta\\right)=h\\left(\\mathbf{x}\\right)g\\left(\\boldsymbol\\eta\\right)\\exp\\left\\{\\boldsymbol{\\eta}^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\right\\}$$\n",
    "where $\\mathbf{x}$ may be scalar or vector, and may be discrete or continuous. Here $\\boldsymbol\\eta$ are called the natural parameters of the distribution, and $\\mathbf{u}\\left(\\mathbf{x}\\right)$ is some function of $\\mathbf{x}$. The function $g\\left(\\boldsymbol\\eta\\right)$ can be interpreted as the coefficient that ensures that the distribution is normalized and therefore statisfies\n",
    "$$g\\left(\\boldsymbol\\eta\\right)\\int h\\left(\\mathbf{x}\\right)\\exp\\{\\boldsymbol{\\eta}^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\}\\mathrm{d}\\mathbf{x}=1$$\n",
    "where the integration is replaced by summation if $\\mathbf{x}$ is a discrete variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the Bernoulli distributin\n",
    "$$p\\left(x|\\mu\\right)=Bern\\left(x|\\mu\\right)=\\mu^x\\left(1-\\mu\\right)^{1-x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expressing the right-hand side as the exponential of the logarithm, we have \n",
    "$$p\\left(x|mu\\right)=\\exp\\{x\\ln\\mu+\\left(1-x\\right)\\ln\\left(1-mu\\right)\\} \\\\\n",
    "=\\left(1-\\mu\\right)\\exp\\left\\{\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)x\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with the exponential family of distributions allows us to identify\n",
    "$$\\eta=\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)$$\n",
    "which we can solve for $\\mu$ to given $\\mu=\\sigma\\left(\\eta\\right)$, where\n",
    "$$\\sigma\\left(\\eta\\right)=\\frac{1}{1+\\exp\\left(-\\eta\\right)}$$\n",
    "is called the logistic sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can write the Bernoulli distribuiton using the standard representation in the form \n",
    "$$p\\left(x|\\eta\\right)=\\sigma\\left(-\\eta\\right)\\exp\\left(\\eta x\\right)$$\n",
    "where we have used $1-\\sigma\\left(\\eta\\right)=\\sigma\\left(-\\eta\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with the exponential family of distributions shows that \n",
    "$$u\\left(x\\right)=x \\\\\n",
    "h\\left(x\\right)=1 \\\\\n",
    "g\\left(\\eta\\right) = \\sigma\\left(-\\eta\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next consider the multinomial distribution that, for a single observation $\\mathbf{x}$, takes the form\n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol\\mu\\right)=\\prod_{k=1}^M\\mu_k^{x_k}=\\exp\\left\\{\\sum_{k=1}^M x_k\\ln\\mu_k\\right\\}$$\n",
    "where $\\mathbf{x}=\\left(x_1,\\dots,x_N\\right)^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can write this in the standard representation so that \n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol\\eta\\right)=\\exp\\left(\\boldsymbol\\eta^\\top\\mathbf{x}\\right)$$\n",
    "where $\\eta_k=\\ln\\mu_k$, and we have defined $\\boldsymbol\\eta=\\left(\\eta_1,\\dots,\\eta_M\\right)^M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have \n",
    "$$\\mathbf{u}\\left(\\mathbf{x}\\right)=\\mathbf{x} \\\\\n",
    "h\\left(\\mathbf{x}\\right)=1 \\\\\n",
    "g\\left(\\boldsymbol\\eta\\right)=1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameters $\\eta_k$ are not independent because the parameters $\\mu_k$ are subject to the constraint\n",
    "$$\\sum_{k=1}^M\\mu_k=1$$\n",
    "so that, given any $M-1$ of the parameters $\\mu_k$, the value of the remaining parameter is fixed. In some circumstances, it will be convenient ot remove this constraint by expressing the distribution in term of only $M-1$ parameters. This can be achieved by using the relationship to eliminate $\\mu_M$ by expressing it in terms of the remaining $\\{\\mu_k\\}$ where $k=1,\\dots,M-1$, thereby leaving $M-1$ parameters. Note that these remaining parameters are still subject to the constraints\n",
    "$$0\\leqslant\\mu_k\\leqslant1,\\quad \\sum_{k=1}^{M-1}\\mu_k\\leqslant1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial distribution in this representation then becomes\n",
    "$$\\exp\\left\\{\\sum_{k=1}^M x_k\\ln\\mu_k\\right\\} \\\\\n",
    "=\\exp\\left\\{\\sum_{k=1}^{M-1}x_k\\ln\\mu_k+\\left(1-\\sum_{k=1}^{M-1}x_k\\right)\\ln\\left(1-\\sum_{k=1}^{M-1}\\mu_k\\right)\\right\\} \\\\\n",
    "=\\exp\\left\\{\\sum_{k=1}^{M-1}x_k\\ln\\left(\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\right)+\\ln\\left(1-\\sum_{k=1}^{M-1}\\mu_k\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now identify \n",
    "$$\\ln\\left(\\frac{\\mu_k}{1-\\sum_j\\mu_j}\\right)=\\eta_k$$\n",
    "which we can solve for $mu_k$ by first summing both sides over $k$ and then rearranging and back-substituting to give\n",
    "$$\\mu_k=\\frac{\\exp\\left(\\eta_k\\right)}{1+\\sum_j\\exp\\left(\\eta_j\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the softmax function, or the normalized exponential. In this representation, the multinomial distrbution therefore takes the form\n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol\\eta\\right)=\\left(1+\\sum_{k=1}^{M-1}\\exp\\left(\\eta_k\\right)\\right)^{-1}\\exp\\left(\\boldsymbol\\eta^\\top\\mathbf{x}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the standard form of the exponential family, with parameter vector $\\boldsymbol\\eta=\\left(\\eta_1,\\dots,\\eta_{M-1}\\right)^\\top$ in which \n",
    "$$\\mathbf{u}\\left(\\mathbf{x}\\right)=\\mathbf{x} \\\\\n",
    "h\\left(\\mathbf{x}\\right)=1 \\\\\n",
    "g\\left(\\boldsymbol\\eta\\right)=\\left(1+\\sum_{k=1}^{M-1}\\exp\\left(\\eta_k\\right)\\right)^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let us consider the Gaussian distribution. For the univariate Gaussian, we have \n",
    "$$p\\left(x|\\mu,\\sigma^2\\right)=\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{1/2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}\\left(x-\\mu\\right)^2\\right\\}=\\frac{1}{\\left(2\\pi\\sigma^2\\right)^{1/2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}\\mu^2\\right\\}$$\n",
    "which, after some simple rearrangement, can be cast in the standard exponential family with \n",
    "$$\\boldsymbol\\eta=\\left(\\begin{array} {cccc} \n",
    "    \\boldsymbol\\eta_1 \\\\   \n",
    "    \\boldsymbol\\eta_2 \\\\    \n",
    "\\end{array}\\right)=\\left(\\begin{array} {cccc} \n",
    "    \\mu/\\sigma^2 \\\\   \n",
    "    -1/2\\sigma^2 \\\\    \n",
    "\\end{array}\\right) \\\\\n",
    "\\mathbf{u}\\left(x\\right)=\\left(\\begin{array} {cccc} \n",
    "    x \\\\   \n",
    "    x^2 \\\\    \n",
    "\\end{array}\\right)  \\\\\n",
    "h\\left(\\mathbf{x}\\right)=\\left(2\\pi\\right)^{-1/2} \\\\\n",
    "g\\left(\\boldsymbol\\eta\\right)=\\left(-2\\eta_2\\right)^{1/2}\\exp\\left(\\frac{\\eta_1^2}{4\\eta_2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.4.1 Maximum likelihood and sfuuicient statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the problem of estimating the parameter vector $\\boldsymbol\\eta$ in the general exponential family distribution using the technique of maximum likelihood. Taking the gradient with respect to $\\boldsymbol\\eta$, we have \n",
    "$$\\nabla g\\left(\\boldsymbol\\eta\\right)\\int h\\left(\\mathbf{x}\\right)\\exp\\left\\{\\boldsymbol\\eta^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\right\\}\\mathrm{d}\\mathbf{x}+g\\left(\\boldsymbol\\eta\\right)\\int h\\left(\\mathbf{x}\\right)\\exp\\left\\{\\boldsymbol\\eta^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\right\\}\\mathbf{u}\\left(\\mathbf{x}\\right)\\mathrm{d}\\mathbf{x}=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging, gives\n",
    "$$-\\frac{1}{g\\left(\\boldsymbol\\eta\\right)}\\nabla g\\left(\\boldsymbol\\eta\\right)=g\\left(\\boldsymbol\\eta\\right)\\int h\\left(\\mathbf{x}\\right)\\exp\\left\\{\\boldsymbol\\eta^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\right\\}\\mathbf{u}\\left(\\mathbf{x}\\right)\\mathrm{d}\\mathbf{x}=\\mathbb{E}\\left[\\mathbf{u}\\left(\\mathbf{x}\\right)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore obtain the result\n",
    "$$-\\nabla\\ln g\\left(\\boldsymbol\\eta\\right)=\\mathbb{E}\\left[\\mathbf{u}\\left(\\mathbf{x}\\right)\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the covariance of $\\mathbf{u}\\left(\\mathbf{x}\\right)$ can be expressed in terms of the second derivatives of $g\\left(\\boldsymbol\\eta\\right)$, and similarly for higher order moments. Thus, provided we can normalize a distribution from the exponential family, we can always find its moments by simple differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a set of independent identically distributed data denoted by $\\mathbf{X}=\\left\\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\right\\}$, for which the likelihood function is given by\n",
    "$$p\\left(\\mathbf{X}|\\boldsymbol\\eta\\right)=\\left(\\prod_{n=1}^N h\\left(\\mathbf{x}_n\\right)\\right)g\\left(\\boldsymbol\\eta\\right)^N\\exp\\left\\{\\boldsymbol\\eta^\\top\\sum_{n=1}^N\\mathbf{u}\\left(\\mathbf{x}\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the gradient of $\\ln p\\left(\\mathbf{X}|\\boldsymbol\\eta\\right)$ with respect to $\\boldsymbol\\eta$ to zero, we get the following condition to be satisfied by the maximum likelihood estimator $\\boldsymbol\\eta_{ML}$\n",
    "$$-\\nabla\\ln g\\left(\\boldsymbol\\eta_{ML}\\right)=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{u}\\left(\\mathbf{x}_n\\right)$$\n",
    "which can in principle be solved to obtain $\\boldsymbol\\eta_{ML}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the solution for the maximum likelihood estimator depends on the data only through $\\sum_n\\mathbf{u}\\left(\\mathbf{x}\\right)$, which is therefore called the sufficient statistic of the distribution. We do not need to store the entire data set itself but only the value of the sufficient statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the limit $N\\to\\infty$, then the right-hand side becomes $\\mathbb{E}\\left[\\mathbf{u}\\left(\\mathbf{x}\\right)\\right]$, and we see that in this limit $\\boldsymbol\\eta_{ML}$ will equal the true value $\\boldsymbol\\eta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Conjugate priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, for a given probability distribution $p\\left(\\mathbf{x}|\\boldsymbol\\eta\\right)$, we can seek a prior $p\\left(\\boldsymbol\\eta\\right)$ that is conjugate to the likelihood function, so that the posterior distribution has the same functional form as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any mumber of the exponential family, there exists a conjugate prior that can be written in the form\n",
    "$$p\\left(\\boldsymbol\\eta|\\boldsymbol\\chi,\\nu\\right)=f\\left(\\boldsymbol\\chi,\\nu\\right)g\\left(\\boldsymbol\\eta\\right)^\\nu\\exp\\left\\{\\nu\\boldsymbol\\eta^\\top\\boldsymbol\\chi\\right\\}$$\n",
    "where $f\\left(\\boldsymbol\\chi,\\nu\\right)$ is a normalization coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see that this is indeed conjugate, let us multiply the prior by the likelihood function to obtain the posterior distribution, up to a normalizatin coefficient, in the form \n",
    "$$p\\left(\\boldsymbol\\eta,\\mathbf{X},\\boldsymbol\\chi,\\nu\\right)\\propto g\\left(\\boldsymbol\\eta\\right)^{\\nu+N}\\exp\\left\\{\\boldsymbol\\eta^\\top\\left(\\sum_{n=1}^N\\mathbf{u}\\left(\\mathbf{x}_n\\right)+\\nu\\boldsymbol\\chi\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again takes the ssame functional form as the prior, confirming conjugacy. Furthermore, we see that the parameter $\\nu$ can be interpreted as a effective number of pseudo-obserations in the prior, each of which has a value for the sufficient statistic $\\mathbf{u}\\left(\\mathbf{x}\\right)$ given by $\\boldsymbol\\chi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Noninformative priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, however, we may hae little idea of what form the distribuiton should take. We may then seek a form of prior distribution, called a noninformative prior, which is intended to have as little influence on the posterior distribution as possible. This is sometimes referred to as 'letting the data speak for themselves'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a distribution $p\\left(x|\\lambda\\right)$ governed by a parameter $\\lambda$, we might be tempted to propose a prior distribution $p\\left(\\lambda\\right)=const$ as a suitable prior. If $\\lambda$ is a discrete variable with $K$ states, this simply amounts to setting the prior probability of each state ot $1/K$. In the case of continuous parameters, however, there are two potential difficulties with this approach. The first is that, if the domain of $\\lambda$ is unbounded, this prior distribution cannot be correctly normalized because the integral over $\\lambda$ diverges. Such priors are called improper. In practice, improper priors can often be used provided the corresponding posterior distribution is proper, i.e., that it can be correctly normalized. For instance, if we put a uniform prior distribution over the mean of a Gaussian, then the osterior distribution for the mean, once we have observed at least one data point, will the proper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second difficulty arises from the transformation behaviour of a probability density under a nonlinear change of variables. If a function $h\\left(\\lambda\\right)$ is constant, and we change variables to $\\lambda=\\eta^2$, then $\\hat{h}\\left(\\eta^2\\right)$ will also be constant. However, if we choose the density $p_\\lambda\\left(\\lambda\\right)$ to be constant, then the density of $\\eta$ will be given, by\n",
    "$$p_\\eta\\left(\\eta\\right)=p_\\lambda\\left(\\lambda\\right)\\bigg|\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}\\eta}\\bigg|=p_\\lambda\\left(\\eta^2\\right)2\\eta\\propto\\eta$$\n",
    "and so the density over $\\eta$ will not be constant. This issue does not arise when we use maximum likelihood, because the likelihood function $p\\left(x|\\lambda\\right)$ is a simple function of $\\lambda$ and so we are free to use any convenient parameterization. If, however, we are to choose a prior distribution that is constant, we must take care to use an appropriate representation for the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider two simple examples of noninformative priors.First of all, if a density takes the from\n",
    "$$p\\left(x|\\mu\\right)=f\\left(x-\\mu\\right)$$\n",
    "then the parameter $\\mu$ is known as a location parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This family of densities exhibits translation invariance because if we shift $x$ by a constant to given $\\hat{x}=x+c$, then\n",
    "$$p\\left(\\hat{x}|\\hat{\\mu}\\right)=f\\left(\\hat{x}-\\hat{\\mu}\\right)=f\\left(\\left(x+c\\right)-\\left(\\mu+c\\right)\\right)=f\\left(x-\\mu\\right)$$\n",
    "where we have defined $\\hat{\\mu}=\\mu+c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the density takes the same form in the new variable as in the original one, and so the density is independent of the choice of origin. We would like to choose a prior distribution that reflects this translation invariance property, and so we choose a prior that assigns equal probability mass to an interval $A\\leqslant\\mu\\leqslant B$ as to the shifted interval $A-c\\leqslant\\mu\\leqslant B-c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implies \n",
    "$$\\int_A^B p\\left(\\mu\\right)\\mathrm{d}\\mu=\\int_{A-c}^{B-c}p\\left(\\mu\\right)\\mathrm{d}\\mu=\\int_A^B p\\left(\\mu-c\\right)\\mathrm{d}\\mu$$\n",
    "and because this must hold for all choices of $A$ and $B$, we have \n",
    "$$p\\left(\\mu-c\\right)=p\\left(\\mu\\right)$$\n",
    "which implies thatã€€$p\\left(\\mu\\right)$ is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a location parameter would be the mean $\\mu$ of a Gaussian distribuion. As we have seen, the conjugate prior distribution for $\\mu$ in this case is a Gaussian $p\\left(\\mu|\\mu_0,\\sigma_0^2\\right)=\\mathcal{N}\\left(\\mu|\\mu_0,\\sigma_0^2\\right)$, and we obtain a noninformative prior by taking the limit $\\sigma^2\\to\\infty$. We see that this gives a posterior distribution over $\\mu$ in which the contributions from the prior vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second example, consider a density of the form\n",
    "$$p\\left(x|\\sigma\\right)=\\frac{1}{\\sigma}f\\left(\\frac{x}{\\sigma}\\right)$$\n",
    "where $\\sigma>0$. Note that this will be a normalized density provided $f\\left(x\\right)$ is correctly normalized. The parameter $\\sigma$ is known as a scale parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density exhibits scale invariance because if we scale $x$ by a constant to give $\\hat{x}=cx$, then\n",
    "$$p\\left(\\hat{x}|\\hat{\\sigma}\\right)=\\frac{1}{\\hat{\\sigma}}f\\left(\\frac{\\hat{x}}{\\hat{\\sigma}}\\right)=\\frac{1}{c\\sigma}f\\left(\\frac{x}{\\sigma}\\right)$$\n",
    "where we have defined $\\hat{\\sigma}=c\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation corresponds to a change of scale. We would like to choose a prior distribution that reflects this scale invariance. If we consider an interval $A\\leqslant\\sigma\\leqslant B$, and a scaled interval $A/c\\leqslant\\sigma\\leqslant B/c$, then the prior should assign equal probability mass to these two intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have \n",
    "$$\\int_A^B p\\left(\\sigma\\right)\\mathrm{d}\\sigma=\\int_{A/c}^{B/c} p\\left(\\sigma\\right)\\mathrm{d}\\sigma=\\int_A^B p\\left(\\frac{1}{c}\\sigma\\right)\\frac{1}{c}\\mathrm{d}\\sigma$$\n",
    "and because this must hold for choices of $A$ and $B$, we have \n",
    "$$p\\left(\\sigma\\right)=p\\left(\\frac{1}{c}\\sigma\\right)\\frac{1}{c}$$\n",
    "and hence $p\\left(\\sigma\\right)\\propto 1/\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that again this is an improper prior because the integral of the distribution over $0\\leqslant\\sigma\\leqslant\\infty$ is divergent. It is sometimes also convenient to think of the prior distribution for a scale parameter in terms of the density of the log of the parameter. Using the trasformation rule for densities we see that $p\\left(\\ln\\sigma\\right)=const$. Thus, for this prior these is the same probability mass in the range $1\\leqslant\\sigma\\leqslant 10$ as in the range $10\\leqslant\\sigma\\leqslant 100$ and in $100\\leqslant\\sigma\\leqslant 1000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a scale parameter would be the standard deviantion $\\sigma$ of a Gaussian distribution, after we have taken account of the location parameter $\\mu$, because \n",
    "$$\\mathcal{N}\\left(x|\\mu,\\sigma^2\\right)\\propto\\sigma^{-1}\\exp\\left\\{-\\left(\\tilde{x}/\\sigma\\right)^2\\right\\}$$\n",
    "where $\\tilde{x}=x-\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, it is often more convenient ot work in terms of the precision $\\lambda=1/\\sigma^2$ rather than $\\sigma$ itself. Using the transformation rule for densities, we see that a distribution $p\\left(\\sigma\\right)\\propto1/\\sigma$ corresponds to a distribution over $\\lambda$ of the form $p\\left(\\lambda\\right)\\propto1/\\lambda$. We have seen that the conjugate prior for $\\lambda$ was the gamma distribuiton $Gam\\left(\\lambda|a_0,b_0\\right)$. The noninformative prior is obtained as the special case $a_0=b_0=0$. Again, if we examine the results for the posterior distribution of $\\lambda$, we see that for $a_0=b_0=0$, the osterior depends only on terms arising from the data and not from the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.5 Norparametric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this chapter, we have focussed on the use of probability distributions having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. This is called the parametric approach to density modelling. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal, then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we consider some nonparametric approaches to density estimation that make few assumptions about the form of the disturibution. Here we shall focus mainly on simple frequentist methods. The reader should be aware, however, that nonparametric Bayesian methods are attracting increasing interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with a discussion of histogram methods for density estimation. Here we explore the properties of histogram density models in more detail, focussing on the case of a single continuous variable $x$. Standard histograms simply partition $x$ falling in bin $i$. In order to turn this count into a normalized probability density, we simply divide by the total number $N$ of observations and by the width $\\varDelta_i$ of the bins to obtain probability values for each bin given by\n",
    "$$p_i=\\frac{n_i}{N\\varDelta_i}$$\n",
    "for which it is easily seen that $\\int p\\left(x\\right)=1$. This gives a model for the ensity $p\\left(x\\right)$ that is constant over the width of each bin, and often the bins are chosen to have the same width $\\varDelta_i=\\varDelta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram density model is dependent on the choice of the value of $\\varDelta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the histogram method has property (unlike the methods to be discussed shortly) that, once the histogram has been computed, the data set itself can be discarded, which can be advantageous if the data set is large. Also, the histogram approach is easily applied if the data points are arriving sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the histogram technique can be useful for obtaining a quick visualization of data in one or two dimensions but is unsuited to most density estimation applications. One obvious problem is that the estimated density has discontinuities that are due to the bin edges rather thatn any property of the underlying distribution that generated the data. Another major limitation of the histogram approach is its scaling with dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram approach to density estimation does, however, teach us two important lessons. First, to estimate the probability density at a particular location, we should consider the data points that lie within some local neighbourhood of that point. Note that the concept of locality requires that we assume some form of distance measure, and here we have been assuming Euclidean distance. For histograms, this neighbourhood property was defined by the bins, and there is a natural 'smoothing' parameter describing the spatial extent of the local region, in this case the bin width. Second, the value of the smoothing parameter should be neither too large nor too small in order to obtain good results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel estimators and nearest neighbours have better scaling with dimensionality than the simple histogram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Kernel density estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us suppose that observations are being drawn from some unknown probability density $p\\left(\\mathbf{x}\\right)$ in some D-dimensional space, which we shall take to be Euclidean, and we wish to estimate the value of $p\\left(\\mathbf{x}\\right)$. From our earlier discussion of locality, let us consider some small region $\\mathcal{R}$ containing $\\mathbf{x}$. The probability mass associated with this region is given by\n",
    "$$P=\\int_{\\mathcal{R}}p\\left(\\mathbf{x}\\right)\\mathrm{d}\\mathbf{x}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we have collected a data set comprising $N$ observations drawn from $p\\left(\\mathbf{x}\\right)$. Because each data point has a probability $P$ of falling within $\\mathcal{R}$, the total number $K$ of points that lie inside $\\mathcal{R}$ will be distributed according to the binomial distribution\n",
    "$$Bin\\left(K|N,P\\right)=\\frac{N!}{K!\\left(N-K\\right)!}P^K\\left(1-P\\right)^{1-K}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the mean fraction of points falling inside the region is $\\mathbb{E}\\left[K/N\\right]ï¼P$, and the variance around this mean is $var\\left[K/N\\right]=P\\left(1-P\\right)/N$. This distribuition will be sharply peaked around the mean and so \n",
    "$$K\\simeq NP.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, however, we also assume that the region $\\mathcal{R}$ is sufficiently small that the probability density $p\\left(\\mathbf{x}\\right)$ is roughly constant over the region, then we have\n",
    "$$P\\simeq p\\left(\\mathbf{x}\\right)V$$\n",
    "where $V$ is the volume of $\\mathcal{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain our density estimate in the form\n",
    "$$p\\left(\\mathbf{x}\\right)=\\frac{K}{NV}.\\quad\\quad\\quad\\left(2.246\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the validity of the formula depends on two contradictory assumptions, namely that the region $\\mathcal{R}$ be sufficiently small that the density is approximately constant over the region and yet sufficiently large (in relation to the value of that dnsity) that the number $K$ of points falling inside the reion is sufficient for the binomial distribution to be sharply peaked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can exploit the result in two different ways. Either we can fix $K$ and determine the value of $V$ from the data, which gives rise to the K-nearest-neighbour technique discussed shortly, or we can fix $V$ and determine $K$ from the data, giving rise to the kernel approach. It can be shown that both the K-nearest-neighbour density estimator and the kernel density estimator converge to the true probability density in the limit $N\\to\\infty$ provided $V$ shrinks suitably with $N$, and $K$ grows with $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by discussing the kernel method in detail, and to start with we take the region $\\mathcal{R}$ to be a small hypercube centred on the point $\\mathbf{x}$ at which we wish to determine the probability density. In order to count the number $K$ of points falling within this region, it is convenient to define the following function\n",
    "$$k\\left(\\mathbf{u}\\right)=\\begin{equation}\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "1,\\quad |u_i|\\leqslant 1/2,\\quad i=1,\\dots,D,\\\\\n",
    "0,\\quad otherwise\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "\\end{equation}$$\n",
    "which represents a unit cube centred on the origin. The function $k\\left(\\mathbf{u}\\right)$ is an example of a kernel function, and in this context is also called a Parzen window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantity $k\\left(\\left(\\mathbf{x}-\\mathbf{x}_n\\right)/h\\right)$ will be one if the data point $\\mathbf{x}_n$ lies inside a cube of side $h$ centred on $\\mathbf{x}$, and zero otherwise. The total number of data points lying inside this cube will therefore be \n",
    "$$K=\\sum_{n=1}^N k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_n}{h}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting this expression then gives the following result for the estimated density at $\\mathbf{x}$ \n",
    "$$p\\left(\\mathbf{x}\\right)=\\frac{1}{N}\\sum_{n=1}^N\\frac{1}{h^D}k\\left(\\frac{\\mathbf{x}-\\mathbf{x}_n}{h}\\right)$$\n",
    "where we have used $V=h^D$ for the volume fo a hypercube of side $h$ in $D$ dimensions. Using the symmetry of the function $k\\left(\\mathbf{u}\\right)$, we can now re-interpret this equation, not as a single cube centred on $\\mathbf{x}$ but as the sum over $N$ cubes sentred on the $N$ data points $\\mathbf{x}_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it stands, the kernel density estimator will suffer from one of the same problems that the histogram method suffered from, namely the presence of artificial discontinuities, in this case at the boundaries of the cubes We can obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian, which gives rise to the following kernel density model\n",
    "$$p\\left(\\mathbf{x}\\right)=\\frac{1}{N}\\sum_{n=1}^N\\frac{1}{\\left(2\\pi h\\right)^{1/2}}\\exp\\left\\{-\\frac{\\|\\mathbf{x}-\\mathbf{x}_n\\|^2}{2h^2}\\right\\}$$\n",
    "where $h$ represents the standard deviation of the Gaussian components. Thus our density model is obtained by placing a Gaussian over each data point and then adding up the contributions over the whole data set, and then dividing by $N$ so that the density is correctly normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, as expected, the parameter $h$ plays the role of a smoothing parameter, and there is a trade-off between sensitivity to noise at small $h$ and over-smoothing at large $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose any other kernel function $k\\left(\\mathbf{u}\\right)$ subject to the conditions \n",
    "$$k\\left(\\mathbf{u}\\right)\\geqslant0, \\\\\n",
    "\\int k\\left(\\mathbf{u}\\right)\\mathrm{d}\\mathbf{u}=1$$\n",
    "which ensure that the resulting probability distribution is nonnegative everywhere and integrates to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of density model is called a kernel density estimator, or Parzen estimator. It has a great merit that there is no computation involved in the 'training' phase because this simply requires storage of the training set. However, this is also one of its great weaknesses because the computational cost of evaluating the density grows linearly with the size of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Nearest-neighbour methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the difficulties with the kernel approach to density estimation is that the parameter $h$ governing the kernel width is fixed for all kernels. In regions of high data density, a large value of $h$ may lead to over-smoothing and a washing out of structure that might otherwise be extracted from the data. However, reducing $h$ may lead to noisy estimates elesewhere in data space where the density is smaller. Thus the optimal choice for $h$ may be dependent on location within the data space. This issue is addressed by nearest-neighbour methods for density estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We therefore return to our general result for local density estimation, and instead of fixing $V$ and determining the value of $K$ from the data, we consider a fixed value of $K$ and use the data to find an appropriate value for $V$. To do this, we consider a small sphere centred on the point $\\mathbf{x}$ at which we wish to estimate the density $p\\left(\\mathbf{x}\\right)$, and we allow the radius of the sphere to grow until it contains precisely $K$ data points. The estimate of the density $p\\left(\\mathbf{x}\\right)$ is then given by (2.246) with $V$ set to the volume of the resulting sphere. This technique is known as $K$ nearest neighbours. We see that the value of $K$ now governs the degree of smoothing and that again there is an optimum choice for $K$ that is neither too large nor too small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We close this chapter by showing how the K-nearest-neighbour techniquefor density estimation can be extended to the problem of classification. To do this, we apply the K-nearest-neighbour density estimation technique to each class separately and then make use of Bayes' theorem. Let us suppose that we have a data set comprising $N_k$ points in class $\\mathcal{C}_k$ with $N$ points in total, so that $\\sum_k N_k=N$. If we wish to classify a new point $\\mathbf{x}$ we draw a sphere centred on $\\mathbf{x}$ containing precisely $K$ point irrespective of their class. Suppose this sphere has volume $V$ and contains $K_k$ points from class $\\mathcal{C}_k$. Then (2.246) provides an estimate of the density associated with each class\n",
    "$$p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)=\\frac{K_k}{N_kV}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the unconditional density is given by\n",
    "$$p\\left(\\mathbf{x}\\right)=\\frac{K}{NV}$$\n",
    "while the class priors are giben by \n",
    "$$p\\left(\\mathcal{C}_k\\right)=\\frac{N_k}{N}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can using Bayes' theorem to obtain the posterior probability of classmembership\n",
    "$$p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)=\\frac{p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)p\\left(\\mathcal{C}_k\\right)}{p\\left(\\mathbf{x}\\right)}=\\frac{K_k}{K}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wish to minimize the probability of misclassification, this is done by assigning the test point $\\mathbf{x}$ to the class having the largest osterior probability, corresponding to the largest value of $K_k/K$. Thus to classify a new point, we identify the $K$ nearest points from the training data set and then assign the new point to the calss having the largest number of representatives amongst this set. Ties can be broken at random. The particular case of $K=1$ is called the nearest-neighbour rule, because a test point is simply assigned to the same calss as the nearest point from the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed so far, both the K-nearest-neighbour method, and the kernel density estimator, require the entire training data set to be stored, leading to expensive computation if the data set is large. This effect can be offset, at the expense of some additional one-off computation, by constructing tree-based search structures to allow (approximate) near neighbours to be found efficiently without doing an exhaustive search of the data set. Nevertheless, these nonparametric methods are still severely limited. On the other hand, we have seen that simple parametric models are very restricted in terms of the forms of distribution that they can represent. We therefore need to find density models that are very flexible and yet for which the complexity of the models can be controlled independently of the size of the training set, and we shall see in subsequent chapters how to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
