{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Linear Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in classification is to take an input vector $\\mathbf{x}$ and to assign it to on of $K$ discrete classes $\\mathcal{C}_k$ where $k=1,\\dots,K$. In the most common scenario, the calsses are taken to be disjoint, so that each input is assigned to one and only one class. The input space is thereby divided into decision regions whose boundaries are called decision boundaries or decision surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider linear models for classification, by which we mean that the decision surfaces are linear functions of the input vector $\\mathbf{x}$ and hence are defined by $\\left(D-1\\right)$-dimensional hyperplanes within the D-dimensional input space. Data sets whose classes can be separated exactly by linear decision surfaces are said to be linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression problems, the target variable $\\mathsf{t}$ was simply the vector of real numbers whose values we wish to predict. In the case of classification, there are various ways of using target values to represent class labels. For probabilistic models, the most convenient, in the case of two-class problems, is the binary representation in which there is a single target variable $t\\in\\{0,1\\}$ such that $t=1$ represents class $\\mathcal{C}_1$ and $t=0$ respresents class $\\mathcal{C}_2$. We can interpret the value of $t$ as the probability that the class is $\\mathcal{C}_1$, with the values of probability taking only the extreme values of $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $K>2$ classes, it is convenient to use a 1-of-K coding scheme in which $\\mathsf{t}$ is a vector of length $K$ such that if the $\\mathcal{C}_j$, then all elements $t_k$ of $\\mathsf{t}$ are zero except element $t_j$, which takes the value $1$. For instance, if we have $K=5$ classes, then a pattern from class $2$ would be given the target vector \n",
    "$$\\mathsf{t}=\\left(0,1,0,0,0\\right)^\\top.$$\n",
    "Again, we can interpret the value of $t_k$ as the probability that the class is $\\mathcal{C}_k$. For nonprobabilistic models, alternative choices of target variable representation will sometimes prove convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified three distinct approaches to the classification problem. The simplest involves constructing a discriminant function that directly assigns each vector $\\mathbf{x}$ to a specific class. A more powerful approach, however, models the conditional probability distribution $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)$ in an inference stage, and then subsequently uses this distribution to make optimal decisions. By separating inference and decision, we gain numerous benefits. There are two different approaches to determining the conditional probabilities $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)$. One technique is to model them directly, for example by representing them as parametric models and then optimizing the parameters using a training set. Alternatively, we can adopt a generative approach in which we model the class-conditional densities given by $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$, together with the prior probabilities $p\\left(\\mathcal{C}_k\\right)$ for the calsses, and then we compute the required posterior probabilities using Bayes' theorem\n",
    "$$p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)=\\frac{p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)p\\left(\\mathcal{C}_k\\right)}{p\\left(\\mathbf{x}\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear regression medels prediction $y\\left(\\mathbf{x},\\mathbf{w}\\right)$ was given by a linear function of the parameters $\\mathbf{w}$. In the simplest case, the model is also linear in the input variables and therefore takes the form $y\\left(\\mathbf{x}\\right)=\\mathbf{w}^\\top+w_0$, so that $y$ is a real number. For classification problems, however, we wish to predict discrete class labels, or more generally posterior probabilities that lie in the range $\\left(0,\\right)$. To achieve this, we consider a generalization of this model in which we transform the linear function of $\\mathbf{w}$ using a nonlinear function $f\\left(\\cdot\\right)$ so that \n",
    "$$y\\left(\\mathbf{x}\\right)=f\\left(\\mathbf{w}^\\top\\mathbf{x}+w_0\\right).$$\n",
    "In the machine learning literature $f\\left(\\cdot\\right)$ is known as an activation function, whereas its inverse is called a link function in the statistics literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision surfaces correspond to $y\\left(\\mathbf{x}\\right)=constant$, so that $\\mathbf{w}^\\top\\mathbf{x}+w_0=constant$ and hence the decision surfaces are linear functions of $\\mathbf{x}$, even if the function $f\\left(\\cdot\\right)$ is nonlinear. For this reason, the class of models called generalized linear models. Note, however, that in contrast to the models used for regression, they are no longer linear in the parameters due to the presence of the nonlinear function $f\\left(\\cdot\\right)$. This will lead to more complex analytical and computational properties than for linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Discriminant Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A discriminant is a function that takes an input vector $\\mathbf{x}$ and assigns it to one of $K$ classes, denoted $\\mathcal{C}_k$. We shall restrict attention to linear discriminants, namely those for which the decision surfaces are hyperplanes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest representation of a linear discriminant function is obtained by taking a linear function of the input vector so that\n",
    "$$y\\left(\\mathbf{x}\\right)=\\mathbf{w}^\\top\\mathbf{x}+w_0$$\n",
    "where $\\mathbf{w}$ is called a weight vector, and $w_0$ is a bias (not to be confused with bias in the statistical sense). The negative of the bias is sometimes called a threshold. An input vector $\\mathbf{x}$ is assigned to class $\\mathcal{C}_1$ if $y\\left(\\mathbf{x}\\right)\\geqslant0$ and to class $\\mathcal{C}_2$ otherwise. The corresponding decision boundary is therefore defined by the relation $y\\left(\\mathbf{x}\\right)=0$, which corresponds to a $\\left(D-1\\right)$-dimensional hyperplane within the D-dimensional input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$ both of which lie on the decision surface. Because $y\\left(\\mathbf{x}_A\\right)=y\\left(\\mathbf{x}_B\\right)=0$, we have $\\mathbf{w}^\\top\\left(\\mathbf{x}_A-\\mathbf{x}_B\\right)=0$ and hence the vector $\\mathbf{w}$ is orthogonal to every vector lying within the decision surface, and so $\\mathbf{w}$ determines the orientation of the decision surface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if $\\mathbf{x}$ is a point on the decision surface, then $y\\left(\\mathbf{x}\\right)=0$, and so the normal distance from the origin to the decision surface is given by \n",
    "$$\\frac{\\mathbf{w}^\\top\\mathbf{x}}{\\|\\mathbf{w}\\|}=-\\frac{w_0}{\\|\\mathbf{w}\\|}.$$\n",
    "We therefore see that the bias parameter $w_0$ determines the location of the decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the value of $y\\left(\\mathbf{x}\\right)$ gives a signed measure of the perpendicular distance $r$ of the point $\\mathbf{x}$ from the decision surface. To see this, consider an arbitrary point $\\mathbf{x}$ and let $\\mathbf{x}_\\bot$ be its orthogonal projection onto the decision surface, so that\n",
    "$$\\mathbf{x}=\\mathbf{x}_\\bot+r\\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|}.$$\n",
    "Multiplying both sides of this result by $\\mathbf{w}^\\top$ and adding $w_0$, and making use of $y\\left(\\mathbf{x}\\right)=\\mathbf{w}^\\top\\mathbf{x}+w_0$ and $y\\left(\\mathbf{x}_\\bot\\right)=\\mathbf{w}^\\top\\mathbf{x}_\\bot+w_0=0$, we have\n",
    "$$r=\\frac{y\\left(\\mathbf{x}\\right)}{\\|\\mathbf{w}\\|}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the linear regression models, it is sometimes convenient to use a more compact notation in which we introduce an additional dummy 'input' value $x_0=1$ and then define $\\tilde{\\mathbf{w}}=\\left(w_0,\\mathbf{w}\\right)$ and $\\tilde{\\mathbf{x}}=\\left(x_0, \\mathbf{x}\\right)$ so that\n",
    "$$y\\left(\\mathbf{x}\\right)=\\tilde{\\mathbf{w}}^\\top\\tilde{\\mathbf{x}}.$$\n",
    "In this case, the decision surfaces are D-dimensional hyperplanes passing through the origin of the $\\left(D+1\\right)$-dimensional expanded input space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "K-class discriminant comprising $K$ linear functins of the form\n",
    "$$y_k\\left(\\mathbf{x}\\right)=\\mathbf{w}_k^\\top\\mathbf{x}+w_{k0}$$\n",
    "and then assigning a point $\\mathbf{x}$ to class $\\mathcal{C}_k$ if $y_k\\left(\\mathbf{x}\\right)>y_j\\left(\\mathbf{x}\\right)$ for all $j\\neq k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision boundary between class $\\mathcal{C}_k$ and class $\\mathcal{C}_j$ is therefore given by $y_k\\left(\\mathbf{x}\\right)=y_j\\left(\\mathbf{x}\\right)$ and hence correspoinds to a $\\left(D-1\\right)$-dimensional hyperplane defined by \n",
    "$$\\left(\\mathbf{w}_k-\\mathbf{w}_j\\right)^\\top\\mathbf{x}+\\left(w_{k0}-w_{j0}\\right)=0.$$\n",
    "This has the same form as the decision boundary for the two-class case, and so analogous geometrical properties apply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision regions of such a discriminant are always singly connected and convex. Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$ both of which lie inside decision regiion $\\mathcal{R}_k$. Any point $\\hat{\\mathbf{x}}$ that lies on the line connecting $\\mathbf{x}_A$ and $\\mathbf{x}_B$ can be expressed in the form\n",
    "$$\\hat{\\mathbf{x}}=\\lambda\\mathbf{x}_A+\\left(1-\\lambda\\right)\\mathbf{x}_B$$\n",
    "where $0\\leqslant\\lambda\\leqslant 1$. Because both $\\mathbf{x}_A$ and $\\mathbf{x}_B$ lie inside $\\mathcal{R}_k$, it follows that $y_k\\left(\\mathbf{x}_A\\right)>y_j\\left(\\mathbf{x}_A\\right)$, and $y_k\\left(\\mathbf{x}_B\\right)>y_j\\left(\\mathbf{x}_B\\right)$, for all $j\\neq k$, and hence $y_k\\left(\\hat{\\mathbf{x}}\\right)>y_j\\left(\\hat{\\mathbf{x}}\\right)$, and so $\\hat{\\mathbf{x}}$ also lies inside $\\mathcal{R}_k$. $\\mathcal{R}_k$ is singly connected and convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Least squares for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a general classification problem with $K$ classes, with a 1-of-K binary coding scheme for the target vector $\\mathsf{t}$. One justification for using least squares in such a context is that it approximates the conditional expectation $\\mathbb{E}[\\mathsf{t}|\\mathbf{x}]$　of the target values given the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each class $\\mathcal{C}_k$ is described by its own linear model so that \n",
    "$$y_k\\left(\\mathbf{x}\\right)=\\mathbf{w}_k^\\top\\mathbf{x}+w_{k0}$$\n",
    "where $k=1,\\dots,K$. We can conveniently group these together using vector notation so that \n",
    "$$\\mathbf{y}\\left(\\mathbf{x}\\right)=\\widetilde{\\mathbf{W}}^\\top\\widetilde{\\mathbf{x}}$$\n",
    "where $\\widetilde{W}$ is a matrix whose $k^{th}$ column comprises the $\\left(D+1\\right)$-dimensional vector $\\widetilde{\\mathbf{w}}=\\left(w_{k0},\\mathbf{w}_k^\\top\\right)^\\top$ and $\\widetilde{\\mathbf{x}}$ is the corresponding augmented input vector $\\left(1,\\mathbf{x}^\\top\\right)^\\top$ with a dummy input $x_0=1$. A new input $\\mathbf{x}$ is then assigned to the class for which the output $y_k=\\widetilde{\\mathbf{w}}^\\top\\widetilde{\\mathbf{x}}$ is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now determine the parameter matrix $\\widetilde{\\mathbf{W}}$ by minimizing a sum-of-squares error function, as we did for regression. Consider a training data set $\\{\\mathbf{x}_n,\\mathsf{t}_n\\}$ where $n=1,\\dots,N$, and define a matrix $\\mathbf{T}$ whose $n^{th}$ row is the vector $\\mathsf{t}_n^\\top$, together with a matrix $\\widetilde{\\mathbf{X}}$ whose $n^{th}$ row is $\\widetilde{\\mathbf{x}}_n^\\top$. The sum-of-squares error function can then be written as \n",
    "$$E_D\\left(\\widetilde{\\mathbf{W}}\\right)=\\frac{1}{2}Tr\\left\\{\\left(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T}\\right)^\\top\\left(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T}\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $\\widetilde{\\mathbf{W}}$ to zero, and rearranging, we then obtain the solution for  $\\widetilde{\\mathbf{W}}$ in the form\n",
    "$$\\widetilde{\\mathbf{W}}=\\left(\\widetilde{\\mathbf{X}}^\\top\\widetilde{\\mathbf{X}}\\right)^{-1}\\widetilde{\\mathbf{X}}^\\top\\mathbf{T}=\\widetilde{\\mathbf{X}}^\\dagger\\mathbf{T}$$\n",
    "where $\\widetilde{\\mathbf{X}}^\\dagger$ is the pseudo-inverse of the matrix $\\widetilde{\\mathbf{X}}$. We then obtain the discriminant function in the form \n",
    "$$\\mathbf{y}\\left(\\mathbf{x}\\right)=\\widetilde{\\mathbf{W}}^\\top\\widetilde{\\mathbf{x}}=\\mathbf{T}^\\top\\left(\\widetilde{\\mathbf{X}}^\\dagger\\right)^\\top\\widetilde{\\mathbf{x}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least-squares approach gives an exact closed-from solution for the discriminant function parameters. However, we have already seen that least-squares solutions lack robustness to outliers, and this applies equally to the classification application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least-squares corresponds to maximum likelihood under the asumption of a Gaussian conditional distribution, whereas binary target vectors clearly have a distribution that is far from Gaussian. By adopting more appropriate probabilistic models, we shall obtain classification techniques with much better properties than least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Fisher's linear discriminant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "One way to view a linear classification model is in terms of dimensionality reduction. Consider first the case of two classes, and suppose we take the D-dimensional input vector $\\mathbf{x}$ and project it down to one dimension using\n",
    "$$y=\\mathbf{w}^\\top\\mathbf{x}.$$\n",
    "If we place a threshold on $y$ and classify $y\\geqslant -w_0$ as class $\\mathcal{C}_1$, otherwise class $\\mathcal{C}_2$, then we obtain our standard linear classifier discussed in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In general, the projection onto one dimension leads to a considerable loss of information, and classes that are well separated in the original D-dimensional space may become strongly overlapping in one dimension. However, by adjusting the components of the weight vector $\\mathbf{w}$, we can select a projection that maximizes the class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, consider a two-class problem in which there are $N_1$ points of class $\\mathcal{C}_1$ and $N_2$ points of class $\\mathcal{C}_2$, so that the mean vectors of the two classes are given by \n",
    "$$\\mathbf{m}_1=\\frac{1}{N_1}\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_1}\\mathbf{x}_n,\\quad\\mathbf{m}_2=\\frac{1}{N_2}\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_2}\\mathbf{x}_n.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest measure of the separation of the classes, when projected onto $\\mathbf{w}$, is the separation of the projected class means. This suggests that we might choose $\\mathbf{w}$ so as to maximize \n",
    "$$m_2-m_1=\\mathbf{w}^\\top\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)$$\n",
    "where \n",
    "$$m_k=\\mathbf{w}^\\top\\mathbf{m}_k$$\n",
    "is the mean of the projected data from class $\\mathcal{C}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this expression can be made arbitrarily large simply by increasing the magnitude of $\\mathbf{w}$. To solve this problem, we could constrain $\\mathbf{w}$ to have unit length, so that $\\sum_i w_i^2=1$. Using a Lagrange multiplier to perform the constrained maximization, we then find that $\\mathbf{w}\\propto \\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is considerable class overlap in the projected space. This difficulty arises from the strongly nondiagonal covariances of the class distributions. The idea proposed by Fisher is to maximize a function that will give a large separation between the projected class means while also giving a small variance within each class, thereby minimizing the class overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projectin formula transforms the set of labelled data points in $\\mathbf{x}$ into a labelled set in the one-dimensional space $y$. The within-class variance of the transformed data from class $\\mathcal{C}_k$ is therefore given by\n",
    "$$s_k^2=\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_k}\\left(y_n-m_k\\right)^2$$\n",
    "where $y_n=\\mathbf{w}^\\top\\mathbf{x}_n$.We can define the total within-class variance for the whole data set to be simply $s_1^2+s_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fisher criterion is defined to be the ratio of the between-class variance to the within-class variance and is given by\n",
    "$$J\\left(\\mathbf{w}\\right)=\\frac{\\left(m_2-m_1\\right)^2}{s_1^2+s_2^2}.$$\n",
    "We can rewrite the Fisher criterion in the form\n",
    "$$J\\left(\\mathbf{w}\\right)=\\frac{\\mathbf{w}^\\top\\mathbf{S}_B\\mathbf{w}}{\\mathbf{w}^\\top\\mathbf{S}_W\\mathbf{w}}$$\n",
    "where $\\mathbf{S}_B$ is the between-class covariance matrix and is given by \n",
    "$$\\mathbf{S}_B=\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)^\\top$$\n",
    "and $\\mathbf{S}_W$ is the total within-class covariance matrix, given by\n",
    "$$\\mathbf{S}_W=\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_1}\\left(\\mathbf{x}_n-\\mathbf{m}_1\\right)\\left(\\mathbf{x}_n-\\mathbf{m}_1\\right)^\\top+\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_2}\\left(\\mathbf{x}_n-\\mathbf{m}_2\\right)\\left(\\mathbf{x}_n-\\mathbf{m}_2\\right)^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating the formual with respect to $\\mathbf{w}$, we find that $J\\left(\\mathbf{w}\\right)$ is maximized when \n",
    "$$\\left(\\mathbf{w}^\\top\\mathbf{S}_B\\mathbf{w}\\right)\\mathbf{S}_\\mathbf{W}\\mathbf{w}=\\left(\\mathbf{w}^\\top\\mathbf{S}_W\\mathbf{w}\\right)\\mathbf{S}_B\\mathbf{w}.$$\n",
    "We see that $\\mathbf{S}_B\\mathbf{w}$ is always in the direction of $\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)$. Furthermore, we do not care about the magnitude of $\\mathbf{w}$, only its direction, and so we can drop the scalar factors $\\left(\\mathbf{w}^\\top\\mathbf{S}_B\\mathbf{w}\\right)$ and $\\left(\\mathbf{w}^\\top\\mathbf{S}_W\\mathbf{w}\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying both sides by $\\mathbf{S}_W^{-1}$ we then obtain\n",
    "$$\\mathbf{w}\\propto\\mathbf{S}_W^{-1}\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right).$$\n",
    "Note that if the within-class covariance is isotropic, so that $\\mathbf{S}_W$ is proportional to the unit matrix, we find that $\\mathbf{w}$ is proportional to the difference of the class means, as discussed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is known as Fisher's linear discriminant, although strictly it is not a discriminant but rather a specific choice of direction for projection of the data down to one dimension. However, the projected data can subsequently be sued to construct a discriminant, by choosing a threshold $y_0$ so that we classify a new point as belonging to $\\mathcal{C}_1$ if $y\\left(\\mathbf{x}\\right)\\geqslant y_0$ and classify it as belonging to $\\mathcal{C}_2$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1.5 Relation to least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least-squares approach to the determination of a linear discriminant was based on the goal of making the model predictions as close as possible to a set of target values. By contrast, the Fisher criterion was derived by requiring maximum class separation in the outpu space. In particular, we shall show that, for the two-class prolem, the Fisher criterion can be obtained as a special case of least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have considered 1-of-K coding for the target values. If, however, we adopt a slightly different target coding scheme, then the least-squares solution for the weights becomes equivalent to the Fisher solution. In paricular, we shall take the targets for class $\\mathcal{C}_1$ to be $N/N_1$, where $N_1$ is the number of patterns in class $\\mathcal{C}_1$, and $N$ is the total number of patterns. This target value approximates the reciprocal of the prior probability for calss $\\mathcal{C}_1$. For class $\\mathcal{C}_2$, we shall take the targets to be $-N/N_2$, where $N_2$ is the nuber of patterns in class $\\mathcal{C}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum-of-squares error function can be written\n",
    "$$E=\\frac{1}{2}\\sum_{n=1}^N\\left(\\mathbf{w}^\\top\\mathbf{x}_n+w_0-t_n\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivatives of $E$ with respect to $w_0$ and $\\mathbf{w}$ to zero, we obtain respectively \n",
    "$$\\sum_{n=1}^N\\left(\\mathbf{w}^\\top\\mathbf{x}_n+w_0-t_n\\right)=0 \\\\\n",
    "\\sum_{n=1}^N\\left(\\mathbf{w}^\\top\\mathbf{x}_n+w_0-t_n\\right)\\mathbf{x}_n=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making use of our choice of target coding scheme for the $t_n$, we obtain an expression for the bias in the form\n",
    "$$w_0=-\\mathbf{w}^\\top\\mathbf{m}$$\n",
    "where we have used \n",
    "$$\\sum_{n=1}^Nt_n=N_1\\frac{N}{N_1}-N_2\\frac{N}{N_2}=0$$\n",
    "and where $\\mathbf{m}$ is the mean of the total data set and is given by\n",
    "$$\\mathbf{m}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n=\\frac{1}{N}\\left(N_1\\mathbf{m}_1+N_2\\mathbf{m}_2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some straightforward algebra, and again making use of the choice of $t_n$, the second equatin becomes\n",
    "$$\\left(\\mathbf{S}_W+\\frac{N_1N_2}{N}\\mathbf{S}_B\\right)\\mathbf{w}=N\\left(\\mathbf{m}_1-\\mathbf{m}_2\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that $\\mathbf{S}_B\\mathbf{w}$ is always in the direction of $\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)$. Thus we can write \n",
    "$$\\mathbf{w}\\propto\\mathbf{S}_W^{-1}\\left(\\mathbf{m}_2-\\mathbf{m}_1\\right)$$\n",
    "where we have ignored irrelevant scale factors. Thus the weight vector coincides with that found from the Fisher criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we have also found an expression for the bias value $w_0$ gien by the formula. This tell us that a new vector $\\mathbf{x}$ should be classified as belonging to class $\\mathcal{C}_1$ if $y\\left(\\mathbf{x}\\right)=\\mathbf{w}^\\top\\left(\\mathbf{x}-\\mathbf{m}\\right)>0$ and class $\\mathcal{C}_2$ otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.1.6 Fisher's discriminant for multiple classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now consider the generalization of the Fisher discriminant to $K>2$ classes, and we shall assume that the dimensionality $D$ of the input space is greater than the number $K$ of classes. Next, we introduce $D'>1$ linear 'features' $y_k=\\mathbf{w}_k^\\top\\mathbf{x}$, where $k=1,\\dots,D'$. These feature values can conveniently be grouped together to form a vector $\\mathbf{y}$. Similarly, the weight vectors $\\{\\mathbf{w}_k\\}$ can be considered to be the columns of a matrix $\\mathbf{W}$, so that\n",
    "$$\\mathbf{y}=\\mathbf{W}^\\top\\mathbf{x}.$$\n",
    "Note that again we are not including any bias parameters in the definition of $\\mathbf{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalization of the within-class covariance matrix to the case of $K$ classes follows from the formula to give\n",
    "$$\\mathbf{S}_W=\\sum_{k=1}^K\\mathbf{S}_k$$\n",
    "where \n",
    "$$\\mathbf{S}_k=\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_k}\\left(\\mathbf{x}_n-\\mathbf{m}_k\\right)\\left(\\mathbf{x}_n-\\mathbf{m}_k\\right)^\\top \\\\\n",
    "\\mathbf{m}_k=\\frac{1}{N_k}\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_k}\\mathbf{x}_n$$\n",
    "and $N_k$ is the number of patterns in class $\\mathcal{C}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find a generalization of the between-class covariance matrix, we follow Duda and Hart and consider first the total covariance matrix\n",
    "$$\\mathbf{S}_T=\\sum_{n=1}^N\\left(\\mathbf{x}_n-\\mathbf{m}\\right)\\left(\\mathbf{x}_n-\\mathbf{m}\\right)^\\top$$\n",
    "where $\\mathbf{m}$ is the mean of the total data set\n",
    "$$\\mathbf{m}=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{x}_n=\\frac{1}{N}\\sum_{k=1}^KN_k\\mathbf{m}_k$$\n",
    "and $N=\\sum_kN_k$ is the total number of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total covariance matrix can be decomposed into the sum of the within-class covariance matrix, plus an additional matrix $\\mathbf{S}_B$, which we identify as a measure of the between-class covariance\n",
    "$$\\mathbf{S}_T=\\mathbf{S}_W+\\mathbf{S}_B$$\n",
    "where\n",
    "$$\\mathbf{S}_B=\\sum_{k=1}^KN_k\\left(\\mathbf{m}_k-\\mathbf{m}\\right)\\left(\\mathbf{x}_k-\\mathbf{m}\\right)^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These covariance matrices have been defined in the original $\\mathbf{x}$-space. We can now define similar matrices in the porjected $D'$-dimensional $\\mathbf{y}$-space\n",
    "$$\\mathbf{S}_W=\\sum_{k=1}^K\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_k}\\left(\\mathbf{y}_n-\\boldsymbol{\\mu}_k\\right)\\left(\\mathbf{y}_n-\\boldsymbol{\\mu}_k\\right)^\\top $$\n",
    "where\n",
    "$$\\boldsymbol{\\mu}_k=\\frac{1}{N_k}\\sum_{\\mathbf{x}_n\\in\\mathcal{C}_k}\\mathbf{y}_n,\\quad\\boldsymbol{\\mu}=\\frac{1}{N}\\sum_{k=1}^KN_k\\boldsymbol{\\mu}_k.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we wish to construct a scalar that is large when the between-class covariance is large and when then the within-class covariance is small. There are now many possible choices of criterion. One example is given by\n",
    "$$J\\left(\\mathbf{W}\\right)=Tr\\{\\mathbf{S}_W^{-1}\\mathbf{S}_B\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This criterion can then be rewritten as an explicit function of the projection matrix $\\mathbf{W}$ in the form\n",
    "$$J\\left(\\mathbf{W}\\right)=Tr\\left\\{\\left(\\mathbf{W}\\mathbf{S}_W\\mathbf{W}^\\top\\right)^{-1}\\left(\\mathbf{W}\\mathbf{S}_W\\mathbf{W}^\\top\\right)\\right\\}.$$\n",
    "Maximization of such criteria is straightforward, though somewhat involved, and is discussed at length in Fukunaga. The weight values are determined by those eigenvetors of $\\mathbf{S}_W^{-1}\\mathbf{S}_B$ that correspond to the $D'$ largest eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.7 The perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a linear discrimiant model is the perceptron of Rosenblatt, which occupies an important place in the history of pattern recognition algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It corresponds to a two-class model in which the input vector $\\mathbf{x}$ is first transformed using a fixed nonlinear transformantion to give a feature vector $\\phi\\left(\\mathbf{x}\\right)$, and this is then used to construct a generalized linear model of the form \n",
    "$$y\\left(\\mathbf{x}\\right)=f\\left(\\mathbf{w}^\\top\\phi\\left(\\mathbf{x}\\right)\\right)$$\n",
    "where the nonlinear activation function $f\\left(\\cdot\\right)$ is given by a step function of the form\n",
    "$$f\\left(a\\right)=\\left\\{\n",
    "\\begin{aligned}\n",
    "+1,\\quad a\\geqslant 0 \\\\\n",
    "-1,\\quad a < 0.\n",
    "\\end{aligned}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector $\\phi\\left(\\mathbf{x}\\right)$ will typically include a bias component $\\phi_0\\left(\\mathbf{x}\\right)=1$. In earlier discussions of two-class classification problems, we have focussed on a target coding scheme in which $t\\in\\{0,1\\}$, which is appropriate in the context of probabilistic models. For the perceptron, however, it is more convenient to use target values $t=+1$ for class $\\mathcal{C}_1$ and $t=-1$ for class $\\mathcal{C}_2$, which matches the choice of activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm used to determine the parameters $\\mathbf{w}$ of the perceptron can most easily be motivated by error function minimization. A natural choice of error function would be the total number of misclassified patterns. However, this does not lead to a simple learning algorithm because the error is piecewise constant function of $\\mathbf{w}$, with discontinuities wherever a change in $\\mathbf{w}$ causes the decision boundary to move across one of the data points. Methods based on changing $\\mathbf{w}$ using the gradient of the error function cannot then be applied, because the gradient is zero almost everywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore consider an alternative error function known as the perceptron criterion. To derive this, we not that we are seeking a werght vector $\\mathbf{w}$ such that patterns $\\mathbf{x}_n$ in class $\\mathcal{C}_1$ will have $\\mathbf{w}^\\top\\phi\\left(\\mathbf{x}_n\\right)>0$, whereas patterns $\\mathbf{x}_n$ in class $\\mathcal{C}_2$ have $\\mathbf{w}^\\top\\phi\\left(\\mathbf{x}_n\\right)<0$. Using the $t\\in\\{+1,-1\\}$ target coding scheme it follows that we would like all patterns to satisfy $\\mathbf{w}^\\top\\phi\\left(\\mathbf{x}_n\\right)t_n>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron criterion associates zero error with any pattern that is correctly classified, whereas for a misclassified pattern $\\mathbf{x}_n$ it tries to minimize the quantity $-\\mathbf{w}^\\top\\phi\\left(\\mathbf{x}_n\\right)t_n$. The perceptron criterion is therefore given by\n",
    "$$E_P\\left(\\mathbf{w}\\right)=-\\sum_{n\\in\\mathcal{M}}\\mathbf{w}^\\top\\phi_nt_n$$\n",
    "where $\\mathcal{M}$ denotes the set of all misclassified patterns. The contribution to the error associated with a particular misclassified pattern is a linear function of $\\mathbf{w}$ in regions of $\\mathbf{w}$ space where the pattern is misclassified and zero i nregions where it is correctly classified. The total error function is therefore piecewise linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the stochastic gradient descent algorithm to this error function. The change in the weight vector $\\mathbf{w}$ is then given by \n",
    "$$\\mathbf{w}^{\\left(\\tau+1\\right)}=\\mathbf{w}^{\\left(\\tau\\right)}-\\eta\\nabla E_P\\left(\\mathbf{w}\\right)=\\mathbf{w}^{\\left(\\tau\\right)}+\\eta\\phi_nt_n$$\n",
    "where $\\eta$ is the learning rate parameter and $\\tau$ is an integer that indexes the steps of the algorithm. Because the perceptron function $y\\left(\\mathbf{x},\\mathbf{w}\\right)$ is unchanged if we multiply $\\mathbf{w}$ by a constant, we can set the learning rate parameter $\\eta$ equal to $1$ without of generality. Note that, as the weight vector evolves during training, the set of patterns that are misclassified will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron learning algorithm has a simple interpretation, as follows. We cycle through the training patterns in turn, and for each pattern $\\mathbf{x}_n$ we evaluate the perceptron function. If the pattern is correctly classified, then the weight vector remains unchanged, whereas if it is incorrectly classified, then for class $\\mathcal{C}_1$ we add the vector $\\phi\\left(\\mathbf{x}_n\\right)$ onto the current estimate of weight vector $\\mathbf{w}$ while for calss $\\mathcal{C}_2$ we subtract the vector $\\phi\\left(\\mathbf{x}_n\\right)$ from $\\mathbf{w}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider the effect of a single update in the perceptron learining algorithm, we see that the contribution to the error from a misclassified pattern will be reduced because we have \n",
    "$$-\\mathbf{w}^{\\left(\\tau+1\\right)\\top}\\phi_nt_n=-\\mathbf{w}^{\\left(\\tau\\right)\\top}\\phi_nt_n-\\left(\\phi_nt_n\\right)^\\top\\phi_nt_n＜－\\mathbf{w}^{\\left(\\tau\\right)\\top}\\phi_nt_n$$\n",
    "where we have set $\\eta=1$, and made use of $\\|\\phi_nt_n\\|^2>0$. Of course, this does not imply that the contribution to the error function from the other misclassified patterns will have been reduced. Furthermore, the change in weight vector may have caused some previously correctly classified patterns to become misclassified. Thus the perceptron learning rule is not guaranteed to reduce the total error function at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the perceptron convergence theorem states that if there exists an exact solution (in other words, if the training data set is linearly separable), then the perceptron learning algorithm is guaranteed to find an exact solution in a finite number of steps. Note, owever, that the number of steps required to achieve convergence could still be substantial, and in practice, until convergence is achieved, we will not be able to distinguish between a nonseparable problem and one that is simply slow to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when the data set is linearly separable, there may be many solutions, and which one is found will depend on the initialization of the parameters and on the order of presentantion of the data points. Furthermore, for data sets that are not linearly separable, the perceptron learning algorithm will never converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside from difficulties with the learning algorithm, the perceptron does not provide probabilistic outputs, nor does it generalize readily to $K>2$ classes. The most important limitation, however, arises from the fact that (in common with all of the models discussed in this chapter and the previous one) it is based on linear combinations of fixed basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2 Probabilistic Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall adopt a generative approach in which we model the class-conditional densities $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$, as well as the class priors $p\\left(\\mathcal{C}_k\\right)$, and then use these to compute posterior probabilities $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)$ throught Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first of all the case of two calsses. The posterior probability for class $\\mathcal{C}_1$ can be written as \n",
    "$$p\\left(\\mathcal{C}_1|\\mathbf{x}\\right)=\\frac{p\\left(\\mathbf{x}|\\mathcal{C}_1\\right)p\\left(\\mathcal{C}_1\\right)}{p\\left(\\mathbf{x}|\\mathcal{C}_1\\right)p\\left(\\mathcal{C}_1\\right)+p\\left(\\mathbf{x}|\\mathcal{C}_2\\right)p\\left(\\mathcal{C}_2\\right)} \\\\\n",
    "=\\frac{1}{1+\\exp\\left(-a\\right)}=\\sigma\\left(a\\right)$$\n",
    "where we have defined \n",
    "$$a=\\ln\\frac{p\\left(\\mathbf{x}|\\mathcal{C}_1\\right)p\\left(\\mathcal{C}_1\\right)}{p\\left(\\mathbf{x}|\\mathcal{C}_2\\right)p\\left(\\mathcal{C}_2\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sigma\\left(a\\right)$ is the logistic sigmoid function defined by\n",
    "$$\\sigma\\left(a\\right)=\\frac{1}{1+\\exp\\left(-a\\right)}.$$\n",
    "The term 'sigmoid' means S-shaped. This type of function is sometimes also called a 'squashing function' because it maps the whole real axis into a finite interval. The logistic sigmoid plays an import role in many classification algorithms. It satisfies the following symmetry property\n",
    "$$\\sigma\\left(-a\\right)=1-\\sigma\\left(a\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of the logistic sigmoid is given by\n",
    "$$a=\\ln\\left(\\frac{\\sigma}{1-\\sigma}\\right)$$\n",
    "and is known as the logit function. It represents the log of the ratio of probabilities $\\ln\\left[p\\left(\\mathcal{C}_1|\\mathbf{x}\\right)/p\\left(\\mathcal{C}_2|\\mathbf{x}\\right)\\right]$ for the two classes, also known as the log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of $K>2$ classes, we have\n",
    "$$p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)=\\frac{p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)p\\left(\\mathcal{C}_k\\right)}{\\sum_jp\\left(\\mathbf{x}|\\mathcal{C}_j\\right)p\\left(\\mathcal{C}_j\\right)} \\\\\n",
    "=\\frac{\\exp\\left(a_k\\right)}{\\sum_j\\exp\\left(a_j\\right)}$$\n",
    "which is known as the normalized exponential and can be regarded as a multiclass generalization of the logistic sigmoid. Here the quantities $a_k$ are defined by\n",
    "$$a_k=\\ln p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)p\\left(\\mathcal{C}_k\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized exponential is alos known as the softmax function, as it represents a smoothed version of the 'max' function because, if $a_k\\gg a_j$ for all $j\\neq k$, then $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)\\simeq 1$, and $p\\left(\\mathcal{C}_j|\\mathbf{x}\\right)\\simeq 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Continuous inputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume that the calss-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. To start with, we shall assume that all classes share the same covariance matrix. Thus the density for class $\\mathbf{C}_k$ is given by \n",
    "$$p\\left(\\mathbf{x}|\\mathcal{C}\\right)=\\frac{1}{\\left(2\\pi\\right)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_k\\right)^\\top\\Sigma^{-1}\\left(\\mathbf{x}-\\boldsymbol{\\mu}_k\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the case of two classes. We have \n",
    "$$p\\left(\\mathcal{C}_1|\\mathbf{x}\\right)=\\sigma\\left(\\mathbf{w}^\\top\\mathbf{x}+w_0\\right)$$\n",
    "where we have defined \n",
    "$$\\mathbf{w}=\\Sigma^{-1}\\left(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2\\right) \\\\\n",
    "w_0=-\\frac{1}{2}\\boldsymbol{\\mu}_1^\\top\\Sigma^{-1}\\boldsymbol{\\mu}_1+\\frac{1}{2}\\boldsymbol{\\mu}_2^\\top\\Sigma^{-1}\\boldsymbol{\\mu}_2+\\ln\\frac{p\\left(\\mathcal{C}_1\\right)}{p\\left(\\mathcal{C}_2\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the quadratic terms in $\\mathbf{x}$ from the exponents of the Gaussian densities have cancelled (due to the assumption of common covariance matrices) leading to a linear function of $\\mathbf{x}$ in the argument of the logistic sigmoid. The resulting decision boundaries correspond to surfaces along which the posterior probabilities $p\\left(\\mathcal{C}|\\mathbf{x}\\right)$ are constant and so will be given by linear functions of $\\mathbf{x}$, and therefore the decision boundaries are linear in input space. The prior probabilities $p\\left(\\mathcal{C}_k\\right)$ enter only through the bias parameter $w_0$ so that changes in the priors have the effect of making parallel shifts of the decision bounary and more generally of the parallel contours of constant posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the general case of $K$ calsses we have\n",
    "$$a_k\\left(\\mathbf{x}\\right)=\\mathbf{w}_k^\\top\\mathbf{x}+w_{k0}$$\n",
    "where we have defined \n",
    "$$\\mathbf{w}_k=\\Sigma^{-1}\\boldsymbol{\\mu}_k \\\\\n",
    "w_{k0}=-\\frac{1}{2}\\boldsymbol{\\mu}_k^\\top\\Sigma^{-1}\\boldsymbol{\\mu}_k+\\ln p\\left(\\mathcal{C}_k\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $a_k\\left(\\mathbf{x}\\right)$ are again linear functions of $\\mathbf{x}$ as a consequence of the cancellation of the quadratic terms due to the shared covariances. The resulting decision boundaries, correspoonding to the minimum misclassification rate, will occur when two of the posterior probabilities (the two largest) are equal, and so will be defined by linear functions of $\\mathbf{x}$, and so again we have a generalized linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we relax the assumption of a shared covariance matrix and allow each classconditional density $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$ to have its own covariance matrix $\\Sigma_k$, then the earlier cancellations will no longer occur, and we will obtain quadratic functions of $\\mathbf{x}$, giving rise to a quadratic discriminant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Maximum likelihood solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have specified a parametric functional form for the class-conditional densities $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$, we can then determine the values of the parameters, together with the prior class probabilites $p\\left(\\mathcal{C}_k\\right)$, using maximum likelihood. This requires a data set comprising observations of $\\mathbf{x}$ along with their corresponding class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider first the case of two classes, each having a Gaussian class-conditional density with a shared covariancematrix, and suppose we have a data set $\\{\\mathbf{x}_n,t_n\\}$ where $n=1,\\dots,N$. Here $t_n=1$ denotes class $\\mathcal{C}_1$ and $t_n=0$ denotes $\\mathcal{C}_2$. We denote the prior class probability $p\\left(\\mathcal{C}_1\\right)=\\pi$, so that $p\\left(\\mathcal{C}_2\\right)=1-\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a data point $\\mathbf{x}_n$ from class $\\mathcal{C}_1$, we have $t_n=1$ and hence\n",
    "$$p\\left(\\mathbf{x}_n,\\mathcal{C}_1\\right)=p\\left(\\mathcal{C}_1\\right)p\\left(\\mathbf{x}_n|\\mathcal{C}_1\\right)=\\pi\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol{\\mu}_1,\\Sigma\\right).$$\n",
    "Similarly for class $\\mathcal{C}_2$, we have $t_n=0$ and hence\n",
    "$$p\\left(\\mathbf{x}_n,\\mathcal{C}_2\\right)=p\\left(\\mathcal{C}_2\\right)p\\left(\\mathbf{x}_n|\\mathcal{C}_2\\right)=\n",
    "\\left(1-\\pi\\right)\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol{\\mu}_2,\\Sigma\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the likelihood function is given by\n",
    "$$p\\left(\\mathsf{t}|\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\Sigma\\right)=\\prod_{n=1}^N\\left[\\pi\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol{\\mu}_1,\\Sigma\\right)\\right]^{t_n}\\left[\\left(1-\\pi\\right)\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol{\\mu}_1,\\Sigma\\right)\\right]^{1-t_n}$$\n",
    "where $\\mathsf{t}=\\left(t_1,\\dots,t_N\\right)^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, it is convenient to maximize the log of the likelihood function. Consider first the maximization with respect to $\\pi$. The terms in the log likelihood function that depend on $pi$ are\n",
    "$$\\sum_{n=1}^N\\{t_n\\ln\\pi+\\left(1-t_n\\right)\\ln\\left(1-\\pi\\right)\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $\\pi$ equal to zero and rearranging, we obtain \n",
    "$$\\pi=\\frac{1}{N}\\sum_{n=1}^Nt_n=\\frac{N_1}{N}=\\frac{N_1}{N_1+N_2}$$\n",
    "where $N_1$ denotes the total number of data points in class $\\mathcal{C}_1$, and $N_2$ denotes the total number of data points in class $\\mathcal{C}_2$. Thus the maximum likelihood estimate for $\\pi$ is simply the fraction of points in class $\\mathcal{C}_1$ as expected. This results is easily generalized to the multiclass case where again the maximum likelihood estimate of the prior probability associated with class $\\mathcal{C}_k$ is given by the fraction of the training set points assigned to that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the maximization with respect to $\\boldsymbol{\\mu}_1$. Again we can pick out of the log likelihood function those terms that depend on $\\boldsymbol{\\mu}_1$ giving \n",
    "$$\\sum_{n=1}^Nt_n\\ln\\mathcal{N}\\left(\\mathbf{x}_n|\\boldsymbol{\\mu}_1,\\Sigma\\right)=-\\frac{1}{2}\\sum_{n=1}^Nt_n\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)^\\top\\Sigma^{-1}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)+const.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the derivative with respect to $\\boldsymbol{\\mu}_1$ to zero and rearranging, we obtain\n",
    "$$\\boldsymbol{\\mu}_1=\\frac{1}{N}\\sum_{n=1}^Nt_n\\mathbf{x}_n$$\n",
    "which is simply the mean of all the input vectors $\\mathbf{x}_n$ assigned to class $\\mathbf{C}_1$. By a similar argument, the corresponding result for $\\boldsymbol{\\mu}_2$ is given by\n",
    "$$\\boldsymbol{\\mu}_2=\\frac{1}{N}\\sum_{n=1}^N\\left(1-t_n\\right)\\mathbf{x}_n$$\n",
    "which again is the mean of all the input vectors $\\mathbf{x}_n$ assigned to class $\\mathcal{C}_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, consider the maximum likelihood solution for the shared covariance matrix $\\Sigma$. Picking out the terms in the log likelihood function that depend on $\\Sigma$, we have \n",
    "$$-\\frac{1}{2}\\sum_{n=1}^Nt_n\\ln|\\Sigma|-\\frac{1}{2}\\sum_{n=1}^Nt_n\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)^\\top\\Sigma^{-1}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right) \\\\\n",
    "-\\frac{1}{2}\\sum_{n=1}^N\\left(1-t_n\\right)\\ln|\\Sigma|-\\frac{1}{2}\\sum_{n=1}^N\\left(1-t_n\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)^\\top\\Sigma^{-1}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right) \\\\\n",
    "=-\\frac{N}{2}\\ln|\\Sigma|-\\frac{N}{2}Tr\\{\\Sigma^{-1}\\mathbf{S}\\}$$\n",
    "where we have defined\n",
    "$$\\mathbf{S}=\\frac{N_1}{N}\\mathbf{S}_1+\\frac{N_2}{N}\\mathbf{S}_2 \\\\\n",
    "\\mathbf{S}_1=\\frac{1}{N_1}\\sum_{\\mathbf{x}\\in\\mathcal{C}_1}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_1\\right)^\\top \\\\\n",
    "\\mathbf{S}_2=\\frac{1}{N_2}\\sum_{\\mathbf{x}\\in\\mathcal{C}_2}\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_2\\right)\\left(\\mathbf{x}_n-\\boldsymbol{\\mu}_2\\right)^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the standard result for the maximu likelihood solution for a Gaussian distribution, we see that $\\Sigma=\\mathbf{S}$, which represents a weighted average of the covariance matrices associated with each of the two classes separately．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is easily extended to the $K$ class problem to obtain the corresponding maximum likelihood solutions for the parameters in which each class-conditional density is Gaussian with a shared covariance matrix. Note that the approach of fitting Gaussian distributions to the classes is not robust to outliers, because the maximum likelihood estimation of a Gaussian is not robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Discrete features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider the case of discrete feature values $x_i$. For simplicity, we begin by looking at binary feature values $x_i\\in\\{0,1\\}$ and discuss the extension to more general discrete features shortly. We will make the naive Bayes assumption in which the feature values are treated as independent, conditioned on the class $\\mathcal{C}_k$. Thus we have class-conditional distributions of the form\n",
    "$$p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)=\\prod_{i=1}^D\\mu_{ki}^{x_i}\\left(1-\\mu_{ki}\\right)^{1-x_i}$$\n",
    "which contain $D$ independent parameters for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have \n",
    "$$a_k\\left(\\mathbf{x}\\right)=\\sum_{i=1}^D\\{x_i\\ln\\mu_{ki}+\\left(1-x_i\\right)\\ln\\left(1-\\mu_{ki}\\right)\\}+\\ln p\\left(\\mathcal{C}_k\\right)$$\n",
    "which again are linear functions of the input values $x_i$. For the case of $K=2$ classes, we can alternatively consider the logistic sigmoid formulation. Analogous results are obtained for discrete variables each of which can take $M>2$ states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Exponential family"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, for boh Gaussian distributed and discrete inputs, the posterior class probabilities are given by generalized linear models with logistic sigmoid ($K=2$ classes) or softmax ($K\\geqslant2$ classes) activation functions. These are particular cases of a more general result obtained by assuming that the class-conditional densities $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$ are members of the exponential family of distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the form for members of the exponential family, we see that the distribution of $\\mathbf{x}$ can be written in the form \n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol{\\lambda}_k\\right)＝h\\left(\\mathbf{x}\\right)g\\left(\\boldsymbol{\\lambda}_k\\right)\\exp\\{\\boldsymbol{\\lambda}_k^\\top\\mathbf{u}\\left(\\mathbf{x}\\right)\\}.$$\n",
    "We now restrict attention to the subclass of such distributions for which $\\mathbf{u}\\left(\\mathbf{x}\\right)=\\mathbf{x}$. The we introduce a scaling parameter $s$, so that we obtain the restricted set of exponential family class-conditional densities of the form \n",
    "$$p\\left(\\mathbf{x}|\\boldsymbol{\\lambda}_k,s\\right)=\\frac{1}{s}h\\left(\\frac{1}{s}\\mathbf{x}\\right)g\\left(\\boldsymbol{\\lambda}_k\\right)\\exp\\left\\{\\frac{1}{s}\\boldsymbol{\\lambda}_k^\\top\\mathbf{x}\\right\\}.$$\n",
    "Note that we are allowing each class to have its own parameter vector $\\boldsymbol{\\lambda}_k$ but we are assuming that the classes share the same scale parameter $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the two-class problem, we substitute this expression for the class-conditional densities into the form and we see that the posterior class probability is again given by a logistic sigmoid acting on a linear function $a\\left(\\mathbf{x}\\right)$ which is given by\n",
    "$$a\\left(\\mathbf{x}\\right)=\\left(\\boldsymbol{\\lambda}_1-\\boldsymbol{\\lambda}_2\\right)^\\top\\mathbf{x}+\\ln g\\left(\\boldsymbol{\\lambda}_1\\right)-\\ln g\\left(\\boldsymbol{\\lambda}_2\\right)＋\\ln p\\left(\\mathcal{C}_1\\right)-\\ln p\\left(\\mathcal{C}_2\\right).$$\n",
    "Similarly, for the $K$-class problem, we substitute the class-conditional density expression into the form to give \n",
    "$$a_k\\left(\\mathbf{x}\\right)=\\boldsymbol{\\lambda}_k^\\top\\mathbf{x}+\\ln g\\left(\\boldsymbol{\\lambda}_k\\right)+\\ln p\\left(\\mathcal{C}_k\\right)$$\n",
    "and so again is a linear function of $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Probabilistic Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For the two-class classification problem, we have seen that the posterior probability of class $\\mathcal{C}_1$ can be writeen as a logistic sigmoid acting on a linear function of $\\mathbf{x}$, for a wide choice of class-conditional distribution $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$. Similarly, for the multiclass case, the posterior probability of class $\\mathcal{C}_k$ is given by a softmax transformation of a linear function of $\\mathbf{x}$. For specific choices of the class-conditionial densities $p\\left(\\mathbf{x}|\\mathcal{C}_1\\right)$, we have used maximum likelihood to determine the parameters of the densities as well as the class priors $p\\left(\\mathcal{C}_k\\right)$ and then used Bayes' theorem to find the posterior class probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, an alternative approach is to use the functional form of the generalized linear model expliitly and to determine its parameters directly by using maximum likelihood. We shall see that there is an efficient algorithm finding such solutions known as iterative reweighted least squares, or IRLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indirect approach to finding the parameters of a generalized linear model, by fitting class-conditional densities and class priors separately and then applying Bayes' theorem, represents an example of generative modelling, because we could take such a model and generate synthitic data by drawing values of $\\mathbf{x}$ from the marginal distribution $p\\left(\\mathbf{x}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the direct approach, we are maximizing a likelihood function defined through the conditional distribution $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)$, which represents a form of discriminative training. One advantage of the discriminative approach is that there will typically be fewer adaptive parameters to be determined, as we shall see shortly. It may also lead to improved predictive performance, particularly when the class-conditional density assumptions give a poor approximation to the true distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Fixed basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have considered classification models that work directly with the original input vector $\\mathbf{x}$. However, all of the algorithms are equally applicable if we first make a fixed nonlinear transformation of the inputs using a vector of basis functions $\\phi\\left(\\mathbf{x}\\right)$. The resulting decision boundaries will be linear in the feature space $\\phi$, and these correspond to nonlinear decision boundaries in the original $\\mathbf{x}$ space. Classes that are linearly separable in the feature space $\\phi\\left(\\mathbf{x}\\right)$ need not be linearly separable in the original observation space $\\mathbf{x}$. Note that as in our discussion of linear models for regression, one of the basis functions is typically set to a constant, say $\\phi_0\\left(\\mathbf{x}\\right)=1$, so that the corresponding parameter $w_0$ plays the role of a bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many problems of practical interest, there is significant overlap between the class-conditional densities $p\\left(\\mathbf{x}|\\mathcal{C}_k\\right)$. This corresponds to posterior probabilities $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)$, which, for at least some values of $\\mathbf{x}$, are not $0$ or $1$. In such cases, the optimal solution is obtained by modelling the posterior probabilities accurately and then applying standard decision theory. Note that nonlinear transformations $\\phi\\left(\\mathbf{x}\\right)$ cannot remove such class overlap. Indeed, they can increase the level of overlap, or create overlap where none existed in the original observation space. However, suitable choices of nonlinearity can make the process of modelling the posterior probabilities easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin our treatment of generalized linear models by considering the problem of two-class classification. In our discussion of generative approaches, we saw that under rather general assumptions, the posterior probability of class $\\mathcal{C}_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector $\\phi$ so that\n",
    "$$p\\left(\\mathcal{C}_1|\\phi\\right)=y\\left(\\phi\\right)=\\sigma\\left(\\mathbf{w}^\\top\\phi\\right)$$\n",
    "with $p\\left(\\mathcal{C}_2|\\phi\\right)=1-p\\left(\\mathcal{C}_1|\\phi\\right)$. Here $\\sigma\\left(\\cdot\\right)$ is the logistic sigmoid function. In the terminology of statistics, this model is known as logistic regression, although it should be emphasized that this is a model for classification rather than regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an M-dimensional feature space $\\phi$, this model has $M$ adjustable parameters. By contrast, if we had fitted Gaussian class conditional densities using maximum likelihood, we would have used $2M$ parameters for the means and $M\\left(M+1\\right)/2$ parameters for the (shared) covariance matrix. Together with the class prior $p\\left(\\mathcal{C}_1\\right)$, this gives a total of $M\\left(M+5\\right)/2+1$ parameters, which grows quadratically with $M$, in contrast to the linear dependence on $M$ of the number of parameters in logistic regression. For large values of $M$, there is a clear advantage in working with the logistic regression model directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use maximum likelihood to determine the parameters of the logistic regression model. To do this, we shall make use of the derivative of the logistic sigmoid function, which can conveniently be expressed in terms of the sigmoid function itself\n",
    "$$\\frac{\\mathrm{d}\\sigma}{\\mathrm{d}a}=\\sigma\\left(1-\\sigma\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a data set $\\left\\{\\phi_n,t_n\\right\\}$, where $t_n\\in\\left\\{0,1\\right)\\}$ and $\\phi_n=\\phi\\left(\\mathbf{x}_n\\right)$, with $n=1,\\dots,N$, the likelihood function can be written \n",
    "$$p\\left(\\mathsf{t}|\\mathbf{w}\\right)=\\prod_{n=1}^Ny_n^{t_n}\\left\\{1-y_n\\right\\}^{1-t_n}$$\n",
    "where $\\mathsf{t}=\\left(t_1\\dots,t_N\\right)^\\top$ and $y_n=p\\left(\\mathcal{C}_1|\\phi_n\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we can define an error function by taking the negative logarithm of the likelihood, which gives the cross-entropy error function in the form \n",
    "$$E\\left(\\mathbf{w}\\right)=-\\ln p\\left(\\mathsf{t}|\\mathbf{w}\\right)=-\\sum_{n=1}^N\\left\\{t_n\\ln y_n+\\left(1+t_n\\right)\\ln \\left(1-y_n\\right)\\right\\}$$\n",
    "where $y_n=\\sigma\\left(a_n\\right)$ and $a_n=\\mathbf{w}^\\top\\phi_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the gradient of the error function with respect to $\\mathbf{w}$, we obtain \n",
    "$$\\nabla E\\left(\\mathbf{w}\\right)=\\sum_{n=1}^N\\left(y_n-t_n\\right)\\phi_n.$$\n",
    "We see that the factor involving the derivative of the logistic sigmoid has cancelled, leading to a simplified form for the gradient of the log likelihood. In particular, the contribution to the gradient from data point $n$ is given by the 'error' $y_n-t_n$ between the target value and the prediction of the model, times the basis function vector $\\phi_n$. This takes precisely the same form as the gradient of the sum-of-squares error function for the linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If desired, we could make use of the result to give a sequential algorithm in which patterns are presented one at a time, in which each of the weight vectors is updated in which $\\nabla E_n$ is the $n^{th}$ term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that maximum likelihood can exhibit severe over-fitting for data sets that are linearly separable. This arises because the maximum likelihood solution occurs when the hyperplane corresponding to $\\sigma=0.5$, equivalent to $\\mathbf{w}^\\top\\phi=0$, separates the two classes and the magnitude of $\\mathbf{w}$ goes to infinity. In this case, the logistic sigmoid function becomes infinitely steep in feature space, correspoinding to a Aeaviside step function, so that every training point from each class $k$ is assigned a posterior probability $p\\left(\\mathcal{C}_k|\\mathbf{x}\\right)=1$. Furthermore, there is typically a continum of such solutions because any separating hyperplane will give rise to the same posterior probabilities at training data points. Maximum likelihood provides no way to favour one such solution over another, and which solution is found in proactice will depend on the choice of optimization algorithm and on the parameter initialization. Note that the problem will arise even if the number of data points is large compared with the number of parameters in the model, so long as the training data set is linearly separable. The singularity can be avoided by inclusion of a prior and finding a MAP solution for $\\mathbf{w}$, or equivalently by adding a regularization term to the error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Iterative reweighted least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the linear regression models, the maximum likelihood solution, on the assumption of a Gaussian noise model, leads to a closed-form solutin. This was a consequence of the quadratic dependence of the log likelihood function on the parameter vector $\\mathbf{w}$. For logistic regression, there is no longer a closed-form solution, due to the nonlinearityh of the logistic sigmoid function. However, the departure from a quadratic form is not substantial. To be precise, the error function is convave, as we shall see shortly, and hence has a unique minimum. Furthermore, the error function can be minimized by an efficient iterative technique based on the Newton-Raphson iterative optimization scheme, which uses a local quadratic approximation to the log likelihood function. The Newton-Raphson update, for minimizing a function $E\\left(\\mathbf{w}\\right)$, takes the form\n",
    "$$\\mathbf{w}^{\\left(new\\right)}=\\mathbf{w}^{\\left(old\\right)}-\\mathbf{H}^{-1}\\nabla E\\left(\\mathbf{w}\\right).$$\n",
    "where $\\mathbf{H}$ is the Hessian matrix whose elements comprise the second derivatives of $E\\left(\\mathbf{w}\\right)$ with respect to the components of $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first of all apply the Newton-Raphson method to the linear regression model with the sum-of-squares error function. The gradient and Hessian of this error function are given by\n",
    "$$\\nabla E\\left(\\mathbf{w}\\right)=\\sum_{n=1}^N\\left(\\mathbf{w}^\\top\\phi_n-t_n\\right)\\phi_n=\\Phi^\\top\\Phi\\mathbf{w}-\\Phi^\\top\\mathsf{t} \\\\\n",
    "\\mathbf{H}=\\nabla\\nabla E\\left(\\mathbf{w}\\right)=\\sum_{n=1}^N\\phi_n\\phi_n^\\top=\\Phi^\\top\\Phi$$\n",
    "where $\\Phi$ is the $N\\times M$ design matrix, whose $n^{th}$ row is given by $\\phi_n^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson update then takes the form\n",
    "$$\\mathbf{w}^{\\left(new\\right)}=\\mathbf{w}^{\\left(old\\right)}-\\left(\\Phi^\\top\\Phi\\right)^{-1}\\left\\{\\Phi^\\top\\Phi\\mathbf{w}^{\\left(old\\right)}-\\Phi^\\top\\mathsf{t}\\right\\} \\\\\n",
    "=\\left(\\Phi^\\top\\Phi\\right)^{-1}\\Phi^\\top\\mathsf{t}$$\n",
    "which we recognize as the standard least-squares solution. Note that the error function in this case is quadratic and hence the Newton-Raphson formula gives the exact solution in one step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us apply the Newton-Raphson update to the cross-entroopy error function for the logistic regression model. We see that the gradient and Hessian of this error function are given by\n",
    "$$\\nabla E\\left(\\mathbf{w}\\right)=\\sum_{n=1}^N\\left(y_n-t_n\\right)\\phi_n=\\Phi^\\top\\left(\\mathsf{y}-\\mathsf{t}\\right) \\\\\n",
    "\\mathbf{H}=\\nabla\\nabla E\\left(\\mathbf{w}\\right)=\\sum_{n=1}^Ny_n\\left(1-y_n\\right)\\phi_n\\phi_n^\\top=\\Phi^\\top\\mathbf{R}\\Phi$$\n",
    "Also, we have introduced the $N\\times M$ diagonal matrix $\\mathbf{R}$ with elements\n",
    "$$R_{nn}=y_n\\left(1-y_n\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Hessian is no longer constant but depends on $\\mathbf{w}$ through the weighting matrix $\\mathbf{R}$, corresponding to the fact that the error function is no longer quadratic. Using the property $0<y_n<1$, which follows from the form of the logistic sigmoid function, we see that $\\mathbf{u}^\\top\\mathbf{H}\\mathbf{u}>0$ for an arbitrary vector $\\mathbf{u}$, and so the Hessian matrix $\\mathbf{H}$ is positive definite. It follows that the error function is a concave function of $\\mathbf{w}$ and hence has a unique minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Newton-Raphson update formula for the logistic regression model then becomes \n",
    "$$\\mathbf{w}^{\\left(new\\right)}=\\mathbf{w}^{\\left(new\\right)}-\\left(\\Phi^\\top\\mathbf{R}\\Phi\\right)^{-1}\\Phi^\\top\\left(\\mathsf{y}-\\mathsf{t}\\right) \\\\\n",
    "=\\left(\\Phi^\\top\\mathbf{R}\\Phi\\right)^{-1}\\left\\{\\Phi^\\top\\mathbf{R}\\Phi\\mathbf{w}^{\\left(new\\right)}-\\Phi^\\top\\left(\\mathsf{y}-\\mathsf{t}\\right)\\right\\} \\\\\n",
    "=\\left(\\Phi^\\top\\mathbf{R}\\Phi\\right)^{-1}\\Phi^\\top\\mathbf{R}\\mathsf{z}$$\n",
    "where $\\mathsf{z}$ is an N-dimensional vector with elements \n",
    "$$\\mathsf{z}=\\Phi\\mathbf{w}^{\\left(old\\right)}-\\mathbf{R}^{-1}\\left(\\mathsf{y}-\\mathsf{t}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the update fomula takes the form of a set of normal equations for a weighted least-squares problem. Because the weighting matrix $\\mathbf{R}$ is not constant but depends on the parameter vector $\\mathbf{w}$, we must apply the normal equations iteratively, each time using the new weight vector $\\mathbf{w}$ to compute a revised weighing matrix $\\mathbf{R}$. For this reason, the algorithm is known as iterative reweighted least squares, or IRLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the weighted least-squares problem, the elements of the diagonal weighting matrix $\\mathbf{R}$ can be interpreted as variances because the mean and variance of $t$ in the logistic regression model are given by\n",
    "$$\\mathbb{E}\\left[t\\right]=\\sigma\\left(\\mathbf{x}\\right)=y \\\\\n",
    "var\\left[t\\right]=\\mathbb{E}\\left[t^2\\right]-\\mathbb{E}\\left[t\\right]^2=\\sigma\\left(\\mathbf{x}\\right)-\\sigma\\left(\\mathbf{x}\\right)^2=y\\left(1-y\\right)$$\n",
    "where we have used the property $t^2=t$ for $t\\in\\left\\{0,1\\right\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can interpret IRLS as the solution to a linearized problem in the space of the variable $a=\\mathbf{w}^\\top\\phi$. The quantity $z_n$, which corresponds to the $n^{th}$ element of $\\mathsf{z}$, can then be given a simple interpretation as an effective target value in this space obtained by making a local linear approximation to the logistic sigmoid function around the current operating point $\\mathbf{w}^{\\left(old\\right)}$\n",
    "$$a_n\\left(\\mathbf{w}\\right)\\simeq a_n\\left(\\mathbf{w}^{\\left(old\\right)}\\right)+\\frac{\\mathrm{d}a_n}{\\mathrm{d}y_n}\\bigg|_{\\mathbf{w}^{\\left(old\\right)}}\\left(t_n-y_n\\right) \\\\\n",
    "=\\phi_n^\\top\\mathbf{w}^{\\left(old\\right)}-\\frac{y_n-t_n}{y_n\\left(1-y_n\\right)}=z_n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 Multiclass logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our discussion of generative models for multiclass classification, we have seen that for a large class of distributions, the posterior probabilities are given by a softmax transformation of linear functions of the feature variables, so that\n",
    "$$p\\left(\\mathcal{C}_k|\\phi\\right)=y_k\\left(\\phi\\right)=\\frac{\\exp\\left(a_k\\right)}{\\sum_j\\exp\\left(a_j\\right)}$$\n",
    "where the 'activations' $a_k$ are given by\n",
    "$$a_k=\\mathbf{w}_k^\\top\\phi.$$\n",
    "These we used maximum likelihood to determine separately the class-conditional densities and the class priors and then found the corresponding posterior probabilities using Bayes' theorem, thereby implicitly determining the parameters $\\{\\mathbf{w}_k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider the use of maximum likelihood to determine the parameters $\\{\\mathbf{w}\\}$ of this model directly. To do this, we will require the derivatives of $y_k$ with respect to all of the activations $a_j$. These are given by\n",
    "$$\\frac{\\partial{y_k}}{\\partial{a_j}}=y_k\\left(I_{kj}-y_j\\right)$$\n",
    "where $I_{kj}$ are the elements of the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write down the likelihood function. This is most easily done using the 1-of-K coding scheme in which the target vector $\\mathsf{t}_n$ for a feature vector $\\phi_n$ belonging to class $\\mathcal{C}_k$ is abinary vector with all elements zero except for element $k$, which equals one. The likelihood function is then given by\n",
    "$$p\\left(\\mathbf{T}|\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)=\\prod_{n=1}^N\\prod_{k=1}^K p\\left(\\mathcal{C}_k|\\phi_n\\right)^{t_{nk}}=\\prod_{n=1}^N\\prod_{k=1}^Ky_{nk}^{t_{nk}}$$\n",
    "where $y_{nk}=y_k\\left(\\phi_n\\right)$, and $\\mathbf{T}$ is an $N\\times K$ matrix of target variables with elements $t_{nk}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the negative logarithm then gives\n",
    "$$E\\left(\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)=-\\ln p\\left(\\mathbf{T}|\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)=-\\sum_{n=1}^N\\sum_{k=1}^Kt_{nk}\\ln y_{nk}$$\n",
    "which is known as the cross-entropoy error function for the multiclass classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take the gradient of the error function with respect to one of the parameter vector $\\mathbf{w}_j$. Making use of the result for the derivatives of the softmax function, we obtain\n",
    "$$\\nabla_{\\mathbf{w}_j}E\\left(\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)=\\sum_{n=1}^N\\left(y_{nj}-t_{nj}\\right)\\phi_n$$\n",
    "where we have made use of $\\sum_kt_{nk}=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we see the same form arising for the gradient as was found for the sum-of-squares error function with the linear model and the cross-entropy error for the logistic regression model, namely the product of the error $\\left(y_{nj}-t_{nj}\\right)$ times the basis function $\\phi_n$. Again, we could use this to formulate a sequential algorithm in which patterns are presented one at a time, in which each of the weight vectors is updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the derivative of the log likelihood function for a linear regression model with respect to the parameter vector $\\mathbf{w}$ for a data point $n$ took the form of the 'error' $y_n-t_n$ times the feature vector $\\phi_n$. Similarly, for the combination of logistic sigmoid activation function and cross-entropy error function, and for the softmax activation function with the multiclass cross-entropy error function, we again obtain this same simple form. This is an example of a more general result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find a batch algorithm, we again appeal to the Newton-Raphson update to obtain the corresponding IRLS algorithm for the multiclass problem. This requires evaluation of the Hessian matrix that comprises blocks of size $M\\times M$ in which block $j,k$ is given by\n",
    "$$\\nabla_{\\mathbf{w}_k}\\nabla_{\\mathbf{w}_j}E\\left(\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)=-\\sum_{n=1}^Ny_{nk}\\left(I_{kj}-y_{nj}\\right)\\phi_n\\phi_n^\\top.$$\n",
    "As with the two-class problem, the Hessian matrix for the multiclass logistic regression model is positive definite and so the error function again has a unique minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 Probitregression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that, for a broad range of class-conditional distributions, described by the exponential family, the resulting posterior class probabilities are given by a logistic (or softmax) transformation acting on a linear function of the feature variables. However, not all choices of class-conditional density give rise to such a simple form for the posterior probabilities (for instance, if the class-conditional densities are modelled using Gaussian mixtures). This suggests that it might be worth exploring other types of discriminative probabilistic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the puposes, however, we shall return to the two-class case, and again remain within the framework of generalized linear models so that\n",
    "$$p\\left(t=1|a\\right)=f\\left(a\\right)$$\n",
    "where $a=\\mathbf{w}^\\top\\phi$, and $f\\left(\\cdot\\right)$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to motivate an alternative choice for the link function is to consider a noisy threshold model, as follows. For each input $\\phi_n$, we evaluate $a_n=\\mathbf{w}^\\top\\phi$ and then we set the target value according to\n",
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "+1,\\quad a_n\\geqslant \\theta \\\\\n",
    "0,\\quad a<\\theta.\n",
    "\\end{aligned}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the value of $\\theta$ is drawn from a probability density $p\\left(\\theta\\right)$, then the corresponding activation function will be given by the cumulative distribution function\n",
    "$$f\\left(a\\right)=\\int_{-\\infty}^a p\\left(\\theta\\right)\\mathrm{d}\\theta.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a specific example, suppose that the density $p\\left(\\theta\\right)$ is given by a zero mean, unit variance Gaussian. The corresponding cumulative distribution function is given by\n",
    "$$\\Phi\\left(a\\right)=\\int_{-\\infty}^a\\mathcal{N}\\left(\\theta|0,1\\right)\\mathrm{d}\\theta$$\n",
    "which is known as the probit function. It has a sigmoidal shape and is compared with the logistic sigmoid function. Note that the use of a more general Gaussian distribution does not change the model because this is equivalent to a re-scaling of the linear coefficients $\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many numerical packages provide for the evaluation of a closely related function defined by\n",
    "$$erf\\left(a\\right)=\\frac{2}{\\sqrt{\\pi}}\\int_0^a\\exp\\left(-\\theta^2/2\\right)\\mathrm{d}\\theta$$\n",
    "and known as the erf function or error function (not to be confused with the error function of a machine learning model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is related to the probit function by \n",
    "$$\\Phi\\left(a\\right)=\\frac{1}{2}\\left\\{1+\\frac{1}{\\sqrt{2}}eft\\left(a\\right)\\right\\}.$$\n",
    "The generalized linear model based on a probit activation function is known as probit regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can determine the parameters of this model using maximum likelihood, by a straightforward extension of the ideas discussed earlier. In practice, the results found using probit regression tend to be similar to those of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that can occur in practical applications is that of outliers, which can arise for instance through errors in measuring the input vector $\\mathbf{x}$ or through mislabelling of the target value $t$. Because such points can lie a long way to the wrong side of the ideal decision boundary, they can seriously distort the classifier. Note that the logistic and probit regression models behave differently in this respect because the tails of the logistic sigmoid decay asymptotically like $\\exp\\left(-x\\right)$ for $x\\to\\infty$, whereas for the probit activation function they decay like $\\exp\\left(-x^2\\right)$, and so the probit model can be significantly more sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, both the logistic and the probit models assume the data is correctly labelled. The effect of mislabelling is easily incorporated into a probabilistic model by introducing a probability $\\epsilon$ that the target value $t$ has been flipped to the wrong value, leading to a target value distribution for data point $\\mathbf{x}$ of the form\n",
    "$$p\\left(t|\\mathbf{x}\\right)=\\left(1-\\epsilon\\right)\\sigma\\left(\\mathbf{x}\\right)+\\epsilon\\left(1-\\sigma\\left(\\mathbf{x}\\right)\\right)$$\n",
    "where $\\sigma\\left(\\mathbf{x}\\right)$ is the activation function with input vector $\\mathbf{x}$. Here $\\epsilon$ may be set in advance, or it may be treated as a hyperparameter whose value is inferred from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.6 Canonical link functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear regression model with a Gaussian noise distribution, the error function, corresponding to hte negative log likelihood. If we take the derivative with respect to the parameter vector $\\mathbf{w}$ of the contribution to the error function from a data point $n$, this takes the form of the 'error' $y_n-t_n$ times the feature vector $\\phi_n$, where $y_n=\\mathbf{w}^\\top\\phi_n$. Similarly, for the combination of the logistic sigmoid activation function and the cross-entropy error function, and for the softmax activation function with the multiclass cross-entropy error function, we again obtain this same simle form. We now show that this is a general result of assuming a conditional distribution for the target variable from the exponential family, along with a corresponding choice for the activation function known as the canonical link function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again make use of the restricted form of exponential family distributions. Note that here we are applying the assumption of exponential family distribution to the target variable $t$. We therefore consider conditional distributions of the target variable fo the form\n",
    "$$p\\left(t|\\eta,s\\right)=\\frac{1}{s}h\\left(\\frac{t}{s}\\right)g\\left(\\eta\\right)\\exp\\left\\{\\frac{\\eta t}{s}\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same line of argument as led to the derivation fo the result, we see that the conditional mean of $t$, which we denote by $y$, is given by\n",
    "$$y\\equiv\\mathbb{E}\\left[t|\\eta\\right]=-s\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\ln g\\left(\\eta\\right).$$\n",
    "Thus $y$ and $\\eta$ must related, and we denote this relation through $\\eta=\\psi\\left(y\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a generalized linear model to be one for which $y$ is a nonlinear function of a linear combination fo the input (or feature) variables so that \n",
    "$$y=f\\left(\\mathbf{w}^\\top\\phi\\right)$$\n",
    "where $f\\left(\\cdot\\right)$ is known as the activation function in the machine learning literature, and $f^{-1}\\left(\\cdot\\right)$ is known as the link function in statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the log likelihood function for this model, which, as a function of $\\eta$, is given by\n",
    "$$\\ln p\\left(\\mathsf{t}|\\eta,s\\right)=\\sum_{n=1}^N\\ln p\\left(t_n|\\eta,s\\right)=\\sum_{n=1}^N\\left\\{\\ln g\\left(\\eta_n\\right)+\\frac{\\eta_nt_n}{s}\\right\\}+const$$\n",
    "where we are assuming that all observations share a common scale parameter (which corresponds to the noise variance for a Gaussian distribution for instance) and so $s$ is independent of $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the log likelihood with respect to the model parameters $\\mathbf{w}$ is then given by\n",
    "$$\\nabla_\\mathbf{w}\\ln p\\left(\\mathsf{t}|\\eta,s\\right)=\\sum_{n=1}^N\\left\\{\\frac{\\mathrm{d}}{\\mathrm{d}\\eta_n}\\ln g\\left(\\eta_n\\right)+\\frac{t_n}{s}\\right\\}\\frac{\\mathrm{d}\\eta_n}{\\mathrm{d}y_n}\\frac{\\mathrm{d}y_n}{\\mathrm{d}a_n}\\nabla a_n \\\\\n",
    "=\\sum_{n=1}^N\\frac{1}{s}\\{t_n-y_n\\}\\psi^{'}\\left(y_n\\right)f^{'}\\left(a_n\\right)\\phi_n$$\n",
    "where $a_n=\\mathbf{w}^\\top\\phi_n$, and we have used $y_n=f\\left(a_n\\right)$ together with the reslut for $\\mathbb{E}\\left[t|\\eta\\right]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that there is a considerable simplification if we choose a particular form for the link function $f^{-1}\\left(y\\right)$ given by \n",
    "$$f^{-1}\\left(y\\right)=\\psi\\left(y\\right)$$\n",
    "which gives $f\\left(\\psi\\left(y\\right)\\right)=y$ and hence $f^{'}\\left(\\psi\\right)\\psi^{'}\\left(y\\right)=1$. Also, because $a=f^{-1}\\left(y\\right)$, we have $a=\\psi$ and hence $f^{'}\\left(a\\right)\\psi^{'}\\left(y\\right)=1$. In this case, the gradient of the error function reduces to \n",
    "$$\\nabla\\ln E\\left(\\mathbf{w}\\right)=\\frac{1}{s}\\sum_{n=1}^N\\{y_n-t_n\\}\\phi_n.$$\n",
    "For the Gaussian $s=\\beta^{-1}$, whereas for the logistic model $s=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.4 The Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall discuss the Bayesian treatment of logistic regression. This is more complex than the Bayesian treatment of linear regression models. In particular, we cannot integrate exactly over the parameter vector $\\mathbf{w}$ since the posterior distribution is no longer Gaussian. It is therefore necessary to introduce some form of approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we introduce a simple, but widely used, framework called the Laplace apporximation, that aims to find a Gaussian approximation to a probability density defined over a set of continuous variables. Consider first the case of a single continuous variable $z$, and suppose the distribution $p\\left(z\\right)$ is defined by\n",
    "$$p\\left(z\\right)=\\frac{1}{Z}f\\left(z\\right)$$\n",
    "where $Z=\\int f\\left(z\\right)\\mathrm{d}z$ is the normalization coefficient. We shall suppose that the value of $Z$ is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Laplace method the goal is to find a Gausian approximation $q\\left(z\\right)$ which is centred on a mode of the distribution $p\\left(z\\right)$. The first step is to find a mode of $p\\left(z\\right)$, in other words a point $z_0$ such that $p^{'}\\left(z_0\\right)=0$, or equivalently\n",
    "$$\\frac{\\mathrm{d}f\\left(z\\right)}{\\mathrm{d}z}\\bigg|_{z=z_0}=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian distribution has the property that its logarithm is a quadratic function of the variables. We therefore consider a Taylor expansion of $\\ln f\\left(z\\right)$ centred on the mode $z_0$ so that\n",
    "$$\\ln f\\left(z\\right)\\simeq\\ln f\\left(z_0\\right)-\\frac{1}{2}A\\left(z-z_0\\right)^2$$\n",
    "where \n",
    "$$A=-\\frac{\\mathrm{d}^2}{\\mathrm{d}z^2}\\ln f\\left(z\\right)\\bigg|_{z=z_0}.$$\n",
    "Note that the first-order term in the Taylor expansion does not appear since $z_0$ is a local maximum of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the exponential we obtain\n",
    "$$f\\left(z\\right)\\simeq f\\left(z_0\\right)\\exp\\left\\{-\\frac{A}{2}\\left(z-z_0\\right)^2\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then obtain a normalized distribution $q\\left(z\\right)$ by making use of the standard result for the normalization of a Gaussian, so that\n",
    "$$q\\left(z\\right)=\\left(\\frac{A}{2\\pi}\\right)^{1/2}\\exp\\left\\{-\\frac{A}{2}\\left(z-z_0\\right)^2\\right\\}.$$\n",
    "Note that the Gaussian approximation will only be well defined if its precision $A>0$, in other words the stationary point $z_0$ must be a local maximum, so that the second derivative of $f\\left(z\\right)$ at the point $z_0$ is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the Laplace method to approximate a distribution $p\\left(\\mathbf{z}\\right)=f\\left(\\mathbf{z}\\right)/Z$ defined over an M-dimensional space $\\mathbf{z}$. At a stationary point $\\mathbf{z}_0$ the gradient $\\nabla f\\left(\\mathbf{z}\\right)$ will vanish. Expanding around this stationary point we have \n",
    "$$\\ln f\\left(\\mathbf{z}\\right)\\simeq\\ln f\\left(\\mathbf{z}_0\\right)-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)^\\top \\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)$$\n",
    "where the $M\\times M$ Hessian matrix $\\mathbf{A}$ is defined by\n",
    "$$\\mathbf{A}=\\nabla\\nabla\\ln f\\left(\\mathbf{z}\\right)\\big|_{\\mathbf{z}=\\mathbf{z}_0}$$\n",
    "and $\\nabla$ is the gradient operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the exponential of both sides we obtain\n",
    "$$f\\left(\\mathbf{z}\\right)\\simeq f\\left(\\mathbf{z}_0\\right)\\exp\\left\\{-frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)^\\top\\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution $q\\left(\\mathbf{z}\\right)$ is proportional to $f\\left(\\mathbf{z}\\right)$ and the appropriate normalization coefficient can be found by inspection, using the standard result for a normalized multivariate Gaussian, giving\n",
    "$$q\\left(\\mathbf{z}\\right)=\\frac{|\\mathbf{A}|^{1/2}}{\\left(2\\pi\\right)^{M/2}}\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)^\\top\\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)\\right\\}=\\mathcal{N}\\left(\\mathbf{z}|\\mathbf{z}_0,\\mathbf{A}^{-1}\\right)$$\n",
    "where $\\|\\mathbf{A}\\|$ denotes the determinant of $\\mathbf{A}$. This Gaussian distribution will be well defined provided its precision matrix, given by $\\mathbf{A}$, is positive definite, which imlies that the stationary point $\\mathbf{z}_0$ must be a local maximum, not a minimum or a saddle point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the Laplace approximation we first need to find the mode $\\mathbf{z}_0$, and then evaluate the Hessian matrix at that mode. In practice a model will typically be found by running some form of numerical optimaization algorithm. Many of the distributions encountered in practice will be multimodal and so there will be different Laplace approximations according to which mode is being considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the normalization constant $Z$ of the true distribution does not need to be known in order to apply the Laplace method. As a result of the central limit theorem, the posterior distribution for a model is expected to become increasingly better approximated by a Gaussian as the number of observed data points is increased, and so we would expect the Laplace approximation to be most useful in situations where the number of data points is relatively large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One major weakness of the Laplace approximation si that, since it is based on a Gaussian distribution, it is only directly applicable to real variables. The most serious limitation of the Laplae framework, however, is that it is based purely on the aspects of the true distribution at a specific value of the variable, and so can fail to capture important global properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Model comparison and BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as approximating the distribution $p\\left(\\mathbf{z}\\right)$ we can also obtain an approximation to the normalization constant $Z$. Using the approximation we have \n",
    "$$Z=\\int f\\left(\\mathbf{z}\\right)\\mathrm{d}\\mathbf{z} \\\\\n",
    "\\simeq f\\left(\\mathbf{z}_0\\right)\\int\\exp\\left\\{-\\frac{1}{2}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)^\\top\\mathbf{A}\\left(\\mathbf{z}-\\mathbf{z}_0\\right)\\right\\}\\mathrm{d}\\mathbf{z} \\\\\n",
    "=f\\left(\\mathbf{z}_0\\right)\\frac{\\left(2\\pi\\right)^{M/2}}{|\\mathbf{A}|^{1/2}}$$\n",
    "where we have noted that the integrand is Gaussian and made use of the standard result for a normalized Gaussian distribution. We can use the result to obtain an approximation to the model evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a data set $\\mathcal{D}$ and a set of models $\\{\\mathcal{M}_i\\}$ having parameters $\\{\\boldsymbol{\\theta}_i\\}$. For each model we define a likelihood function $p\\left(\\mathcal{D}|\\boldsymbol{\\theta}_i,\\mathcal{M}_i\\right)$. If we introduce a prior $p\\left(\\boldsymbol{\\theta}|\\mathcal{M}_i\\right)$ over the parameters, then we are interested in computing the model evidence $p\\left(\\mathcal{D}|\\mathcal{M}_i\\right)$ for the various models. From now on we omit the conditioning on $\\mathcal{M}_i$ to keep the notation uncluttered. From Bayes' theorem the model evidence is given by\n",
    "$$p\\left(\\mathcal{D}\\right)=\\int p\\left(\\mathcal{D}|\\boldsymbol{\\theta}\\right)p\\left(\\boldsymbol{\\theta}\\right)\\mathrm{d}\\boldsymbol{\\theta}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying $f\\left(\\boldsymbol{\\theta}\\right)=p\\left(\\mathcal{D}|\\boldsymbol{\\theta}\\right)p\\left(\\boldsymbol{\\theta}\\right)$ and $Z=p\\left(\\mathcal{D}\\right)$, and applying the result, we obtain\n",
    "$$\\ln p\\left(\\mathcal{D}\\right)\\simeq\\ln p\\left(\\mathcal{D}|\\boldsymbol{\\theta}_{MAP}\\right)+\\underbrace{\\ln p\\left(\\boldsymbol{\\theta}_{MAP}\\right)+\\frac{M}{2}\\ln\\left(2\\pi\\right)-\\frac{1}{2}\\ln|\\mathbf{A}|}_{Occam\\;factor}$$\n",
    "where $\\boldsymbol{\\theta}_{MAP}$ is the value of $\\boldsymbol{\\theta}$ at the mode of the posterior distribution, and $\\mathbf{A}$ is the Hessian matrix of second derivatives of the negative log posterior\n",
    "$$\\mathbf{A}=-\\nabla\\nabla\\ln p\\left(\\mathcal{D}|\\boldsymbol{\\theta}_{MAP}\\right)p\\left(\\boldsymbol{\\theta}\\right)=-\\nabla\\nabla\\ln p\\left(\\boldsymbol{\\theta}_{MAP}|\\mathcal{D}\\right)$$\n",
    "The first term on the right represents the log likelihood evaluated using the ooptimized parameters, while the remaining three terms comprise the 'Occam factor' which penalizes model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume that the Gaussian prior distribution over parameters is broad, and that the Hessian has full rank, then we can approximate the formula very roughly using \n",
    "$$\\ln p\\left(\\mathcal{D}\\right)\\simeq p\\left(\\mathcal{D}|\\boldsymbol{\\theta}_{MAP}\\right)-\\frac{1}{2}M\\ln N$$\n",
    "where $N$ is the number of data points, $M$ is the number of parameters in $\\boldsymbol{\\theta}$ and we have omitted additive constants. This is known as the Bayesian Information Criterion or the Schwarz criterion. Note that, compared to AIC, this penalizes model complexity more heavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complexity measures such as AIC and BIC have the virtue of being easy to evaluate, but can also gie misleading results. In particular, the assumption that the Hessian matrix has full rank is often not valid since many of the parameters are not 'well-determined'. We can use the result to obtain a more accurate estimate of the model evidence starting from the Laplace approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now turn to a Bayesian treatment of logistic regression. Exact Bayesian inference for logistic regression is intractable. In particular, evaluation of the posterior distribution would require normalization of the product of a proir distribution and a likelihood function that itself comprises a product of logistic sigmoid functions, one for every data point. Evaluation of the predictive distribution is similarly intractable. Here we consider the application of the Laplace approximation to the problem of Bayesian logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 Laplace approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Laplace approximation is obtained by finding the mode of the posterior distribution and then fitting a Gaussian centred at that mode. This requires evaluation of the second derivatives of the log posterior, which is equivalent to finding the Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we seek a Gaussian representation for the posterior distribution, it is natural to begin with a Gaussian prior, which we write in the general form\n",
    "$$p\\left(\\mathbf{w}\\right)=\\mathcal{N}\\left(\\mathbf{w}|\\mathbf{m}_0,\\mathbf{S}_0\\right)$$\n",
    "where $\\mathbf{m}_0$ and $\\mathbf{S}_0$ are fixed hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior distribution over $\\mathbf{w}$ is given by \n",
    "$$p\\left(\\mathbf{w}|\\mathsf{t}\\right)\\propto p\\left(\\mathbf{w}\\right)p\\left(\\mathsf{t}|\\mathbf{w}\\right)$$\n",
    "where $\\mathsf{t}=\\left(t_1,\\dots,t_N\\right)^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the log of both sides, and substituting for the prior distribution, and for the likelihood function, we obtain\n",
    "$$\\ln p\\left(\\mathbf{w}|\\mathsf{t}\\right)=-\\frac{1}{2}\\left(\\mathbf{w}-\\mathbf{m}_0\\right)^\\top\\mathbf{S}_0^{-1}\\left(\\mathbf{w}-\\mathbf{m}_0\\right)+\\sum_{n=1}^N\\{t_n\\ln y_n+\\left(1-t_n\\right)\\ln\\left(1-y_n\\right)\\}+const$$\n",
    "where $y_n=\\sigma\\left(\\mathbf{w}^\\top\\phi_n\\right).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a Gaussian approximation to the posterior distribution, we first maximize the posterior distribution to give the MAP (maximum posterior) solution $\\mathbf{w}_{MAP}$, which defines the mean of the Gaussian. The covariance is then given by the inverse of the matrix of second derivatives of the vegative log likelihood, which takes the form\n",
    "$$\\mathbf{S}_N=-\\nabla\\nabla\\ln p\\left(\\mathbf{w}|\\mathsf{t}\\right)=\\mathbf{S}_0^{-1}+\\sum_{n=1}^Ny_n\\left(1-y_n\\right)\\phi_n\\phi_n^\\top.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian approximation to the posterior distribution therefore takes the form\n",
    "$$q\\left(\\mathbf{w}\\right)=\\mathcal{N}\\left(\\mathbf{w}|\\mathbf{w}_{MAP},\\mathbf{S}_N\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained a Gaussian approximation to the posterior distribution, there remains the task of marginalizing with respect to this distribution in order to make prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 Predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictive distribution for class $\\mathcal{C}_1$, given a new feature vector $\\phi\\left(\\mathbf{x}\\right)$, is obtained by marginalizing with respect to the posterior distribution $p\\left(\\mathbf{w}|\\mathsf{t}\\right)$, which is itself approximated by a Gaussian distribution $q\\left(\\mathbf{w}\\right)$ so that\n",
    "$$p\\left(\\mathcal{C}_1|\\phi,\\mathsf{t}\\right)=\\int p\\left(\\mathcal{C}_1|\\phi,\\mathbf{w}\\right)p\\left(\\mathbf{w}|\\mathsf{t}\\right)\\mathrm{d}\\mathbf{w}\\simeq\\int\\sigma\\left(\\mathbf{w}^\\top\\phi\\right)q\\left(\\mathbf{w}\\right)\\mathrm{d}\\mathbf{w}$$\n",
    "with the corresponding probability for class $\\mathcal{C}_2$ given by $p\\left(\\mathcal{C}_2|\\phi,\\mathsf{t}\\right)=1-p\\left(\\mathcal{C}_2|\\phi,\\mathsf{t}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the predictive distribution, we first note that the function $\\sigma\\left(\\mathbf{w}^\\top\\phi\\right)$ depends on $\\mathbf{w}$ only throught its projection onto $\\phi$. Denoting $a=\\mathbf{w}^\\top\\phi$, we have \n",
    "$$\\sigma\\left(\\mathbf{w}^\\top\\phi\\right)=\\int\\delta\\left(a-\\mathbf{w}^\\top\\phi\\right)\\sigma\\left(a\\right)\\mathrm{d}a$$\n",
    "where $\\delta\\left(\\cdot\\right)$ is the Dirac delta function. From this we obtain\n",
    "$$\\int\\sigma\\left(\\mathbf{w}^\\top\\phi\\right)q\\left(\\mathbf{w}\\right)\\mathrm{d}\\mathbf{w}=\\int\\sigma\\left(a\\right)p\\left(a\\right)\\mathrm{d}a$$\n",
    "where\n",
    "$$p\\left(a\\right)=\\int\\delta\\left(a-\\mathbf{w}^\\top\\phi\\right)q\\left(\\mathbf{w}\\right)\\mathrm{d}\\mathbf{w}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate $p\\left(a\\right)$ by noting that the delta function imposes a linear constraint on $\\mathbf{w}$ and so forms a marginal distribution from the joint distribution $q\\left(\\mathbf{w}\\right)$ by integrating out all directions orthogonal to $\\phi$. Because $q\\left(\\mathbf{w}\\right)$ is Gaussian, we know that the marginal distribution will also be Gaussian. We can evaluate the mean and covariance of this distribution by taking moments, and interchanging the order of integration over $a$ and $\\mathbf{w}$, so that\n",
    "$$\\mu_a=\\mathbb{E}\\left[a\\right]=\\int p\\left(a\\right)a\\;\\mathrm{d}a=\\int q\\left(\\mathbf{w}\\right)\\mathbf{w}^\\top\\phi\\;\\mathrm{d}\\mathbf{w}=\\mathbf{w}_{MAP}^\\top\\phi$$\n",
    "where we have used the result for the variational posterior distribution $q\\left(\\mathbf{w}\\right)$. Similarly\n",
    "$$\\sigma_a^2=var\\left[a\\right]=\\int p\\left(a\\right)\\left\\{a^2-\\mathbb{E}\\left[a\\right]^2\\right\\}\\mathrm{d}a \\\\\n",
    "=\\int q\\left(\\mathbf{w}\\right)\\left\\{\\left(\\mathbf{w}^\\top\\phi\\right)^2-\\left(\\mathbf{m}_N^\\top\\phi\\right)^2\\right\\}\\mathrm{d}\\mathbf{w}=\\phi^\\top\\mathbf{S}_N\\phi.$$\n",
    "Note that the distribution of $a$ takes the same form as the predictive distribution for the linear regression model, with the noise variance set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus our variational approximation to the predictive distribution becomes \n",
    "$$p\\left(\\mathcal{C}_1|\\mathsf{t}\\right)=\\int\\sigma\\left(a\\right)p\\left(a\\right)\\mathrm{d}a=\\int\\sigma\\left(a\\right)\\mathcal{N}\\left(a|\\mu_a,\\sigma_a^2\\right)\\mathrm{d}a.$$\n",
    "This result can also be derived directly by making use of the results for the marginal of a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integral over $a$ represents the convolution of a Gaussian with a logistic sigmoid, and cannot be evaluated analytically. We can, however, obtain a good approximation by making use of the close similarity between the logistic sigmoid function $\\sigma\\left(a\\right)$ and the probit function $\\Phi\\left(a\\right)$. In order to obtain the best approximation to the logistic function we need to re-scale the horizontal axis, so that we approximate $\\sigma\\left(a\\right)$ by $\\Phi\\left(\\lambda a\\right)$. We can find a suitable value of $\\lambda$ by requiring that two functions have the same slope at the origin, which gives $\\lambda^2=\\pi/8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using a probit function is that its convolution with a Gaussian can be expressed analytically in terms of another probit funciton. Specifically we can show that \n",
    "$$\\int\\Phi\\left(\\lambda a\\right)\\mathcal{N}\\left(a|\\mu,\\sigma^2\\right)\\mathrm{d}a=\\Phi\\left(\\frac{\\mu}{\\left(\\lambda^{-2}+\\sigma^2\\right)^{1/2}}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the approximation $\\sigma\\left(a\\right)\\simeq\\Phi\\left(\\lambda a\\right)$ to the probit functions appearing on both sides of this equation, leading to the following approximation for the convolution of a logistic sigmoid with a Gaussian\n",
    "$$\\int\\sigma\\left(a\\right)\\mathcal{N}\\left(a|\\mu,\\sigma^2\\right)\\mathrm{d}a\\simeq\\sigma\\left(\\kappa\\left(\\sigma^2\\right)\\mu\\right)$$\n",
    "where we have defined \n",
    "$$\\kappa\\left(\\sigma^2\\right)=\\left(1+\\pi\\sigma^2/8\\right)^{-1/2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this result we obtain the approximate predictive distribution in the form \n",
    "$$p\\left(\\mathcal{C}_1|\\phi,\\mathsf{t}\\right)=\\sigma\\left(\\kappa\\left(\\sigma_a^2\\right)\\mu_a\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the decision boundary corresponding to $p\\left(\\mathcal{C}_1|\\phi,\\mathsf{t}\\right)=0.5$ is given by $\\mu_a=0$, which is the same as the decision boundary obtained by using the MAP value for $\\mathbf{w}$. Thus if the decison criterion is based on minimizing misclassification rate, with equal prior probabilities, then the marginalizaiton over $\\mathbf{w}$ has no effect. However, for more complex decision criteria it will play an important role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
